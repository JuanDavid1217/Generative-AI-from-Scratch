{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JuanDavid1217/Generative-AI-from-Scratch/blob/main/Transformers/Embedding_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAvXfaR8qZTX"
      },
      "source": [
        "**Primero se necesita entender como funcionan lo Embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BT8gperVqUhz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from pickle import load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXqWbx_BxQo1"
      },
      "outputs": [],
      "source": [
        "class Activation_ReLU:\n",
        "  def __init__(self):\n",
        "    self.input=[]\n",
        "    self.output=[]\n",
        "    self.dinputs=[]\n",
        "\n",
        "  def forward(self, input):\n",
        "    self.input = input\n",
        "    self.output=np.maximum(0, input)\n",
        "\n",
        "  def backward(self, dvalues):\n",
        "    dvalues = np.sum(dvalues, axis=0, keepdims=True)\n",
        "    self.dinputs=dvalues*np.where(self.output > 0, 1, 0)\n",
        "\n",
        "class Activation_Sigmoid:\n",
        "  def __init__(self):\n",
        "    self.input=[]\n",
        "    self.output=[]\n",
        "    self.dinputs=[]\n",
        "\n",
        "  def forward(self, input):\n",
        "    self.output = 1 / (1 + np.exp(-input))\n",
        "\n",
        "  def backward(self, dvalues):\n",
        "    self.dinputs = dvalues * (1 - self.output) * self.output\n",
        "\n",
        "class Activation_SoftMax:\n",
        "  def __init__(self):\n",
        "    self.input=[]\n",
        "    self.output=[]\n",
        "    self.dinputs=[]\n",
        "\n",
        "  def forward(self, input):\n",
        "    exp_z = np.exp(input - np.max(input))\n",
        "    self.output = exp_z / np.sum(exp_z, axis=0)\n",
        "\n",
        "  def backward(self, dvalues):\n",
        "    # Create uninitialized array\n",
        "    self.dinputs = np.empty_like(dvalues)\n",
        "    # Enumerate outputs and gradients\n",
        "    for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
        "      # Flatten output array\n",
        "      single_output = single_output.reshape(-1, 1)\n",
        "      # Calculate Jacobian matrix of the output and\n",
        "      jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
        "      # Calculate sample-wise gradient\n",
        "      # and add it to the array of sample gradients\n",
        "      self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkywBDDRxZhn"
      },
      "outputs": [],
      "source": [
        "class Dense:\n",
        "  def __init__(self, input_dim, neurons, bias=False):\n",
        "    self.weights=0.1*np.random.randn(input_dim, neurons)\n",
        "    self.bias=bias\n",
        "    if self.bias:\n",
        "      self.biases=np.zeros((1, neurons))\n",
        "      self.dbiases=0\n",
        "    self.input=[]\n",
        "    self.output=[]\n",
        "    self.dweights=0\n",
        "    self.dinputs=0\n",
        "\n",
        "  def  forward(self, input):\n",
        "    self.input=input\n",
        "    if self.bias:\n",
        "      self.output = np.dot(self.input, self.weights)+self.biases\n",
        "    else:\n",
        "      self.output=np.dot(self.input, self.weights)\n",
        "\n",
        "  def backward(self, dvalues):\n",
        "    if self.bias:\n",
        "      self.dbiases=dvalues\n",
        "    else:\n",
        "      if dvalues.shape[0]!=1:\n",
        "        dvalues=dvalues.reshape(1, -1)\n",
        "\n",
        "    if self.input.shape[0]!=1:\n",
        "        self.input=self.input.reshape(1, -1)\n",
        "    self.dweights=np.dot(self.input.T, dvalues)\n",
        "    self.dinputs=np.dot(dvalues, self.weights.T)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utieyHkpxvbo"
      },
      "outputs": [],
      "source": [
        "class Optimizer_Adam:\n",
        "  def __init__(self, learning_rate=0.01, decay=0., epsilon=1e-7,beta_1=0.9, beta_2=0.999):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.current_learning_rate = learning_rate\n",
        "    self.decay = decay\n",
        "    self.iterations = 0\n",
        "    self.epsilon = epsilon\n",
        "    self.beta_1 = beta_1\n",
        "    self.beta_2 = beta_2\n",
        "\n",
        "\n",
        "  def pre_update_params(self):\n",
        "    if self.decay:\n",
        "      self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "  def update_params(self, layer):\n",
        "    if not hasattr(layer, 'weight_cache'):\n",
        "      layer.weight_momentums = np.zeros_like(layer.weights)\n",
        "      layer.weight_cache = np.zeros_like(layer.weights)\n",
        "      if hasattr(layer, 'biases'):\n",
        "        layer.bias_momentums = np.zeros_like(layer.biases)\n",
        "        layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "    # Update momentum with current gradients\n",
        "    layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights\n",
        "    if hasattr(layer, 'biases'):\n",
        "      layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases\n",
        "    # Get corrected momentum\n",
        "    # self.iteration is 0 at first pass\n",
        "    # and we need to start with 1 here\n",
        "    weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
        "    if hasattr(layer, 'biases'):\n",
        "      bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
        "    # Update cache with squared current gradients\n",
        "    layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights**2\n",
        "    if hasattr(layer, 'biases'):\n",
        "      layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases**2\n",
        "    # Get corrected cache\n",
        "    weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
        "    if hasattr(layer, 'biases'):\n",
        "      bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
        "    # Vanilla SGD parameter update + normalization\n",
        "    # with square rooted cache\n",
        "    layer.weights += -self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
        "    if hasattr(layer, 'biases'):\n",
        "      layer.biases += -self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
        "\n",
        "  # Call once after any parameter updates\n",
        "  def post_update_params(self):\n",
        "    self.iterations += 1\n",
        "\n",
        "\n",
        "class Rmsprop:\n",
        "  def __init__(self, learning_rate=0.1, decay=0., epsilon=1e-8, rho=0.9):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.current_learning_rate = learning_rate\n",
        "    self.decay = decay\n",
        "    self.iterations = 0\n",
        "    self.epsilon = epsilon\n",
        "    self.rho = rho\n",
        "\n",
        "  def pre_update_params(self):\n",
        "    if self.decay:\n",
        "      self.current_learning_rate = self.learning_rate * \\\n",
        "          (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "  def update_params(self, layer):\n",
        "    if not hasattr(layer, 'wight_cache'):\n",
        "      layer.weight_cache = np.zeros_like(layer.weights)\n",
        "    if hasattr(layer, 'biases'):\n",
        "      layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "    layer.weight_cache = self.rho * layer.weight_cache + \\\n",
        "        (1 - self.rho) * layer.dweights**2\n",
        "    if hasattr(layer, 'biases'):\n",
        "      layer.bias_cache = self.rho * layer.bias_cache + \\\n",
        "          (1 - self.rho) * layer.dbiases**2\n",
        "\n",
        "    layer.weights += -self.current_learning_rate * \\\n",
        "                    layer.dweights / \\\n",
        "                    (np.sqrt(layer.weight_cache) + self.epsilon)\n",
        "    if hasattr(layer, 'biases'):\n",
        "      layer.biases += -self.current_learning_rate * \\\n",
        "                      layer.dbiases / \\\n",
        "                      (np.sqrt(layer.bias_cache) + self.epsilon)\n",
        "    return layer\n",
        "\n",
        "  def post_update_params(self):\n",
        "    self.iterations += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjZkcK5B96jb"
      },
      "outputs": [],
      "source": [
        "class LossBinaryCrossEntropy():\n",
        "  def forward(self, y_pred, y_true):\n",
        "    y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "\n",
        "    sample_losses = -(y_true * np.log(y_pred_clipped) + (1 - y_true) * np.log(1 - y_pred_clipped))\n",
        "    #sample_losses = np.mean(sample_losses, axis=1)\n",
        "\n",
        "    return sample_losses\n",
        "\n",
        "  def backward(self, dvalues, y_true):\n",
        "    samples = len(dvalues)\n",
        "    outputs = len(dvalues)\n",
        "\n",
        "    clipped_dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)\n",
        "\n",
        "    dinputs = -(y_true / clipped_dvalues - (1 - y_true)\n",
        "                     /(1 - clipped_dvalues))/outputs\n",
        "\n",
        "    return dinputs / samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEnCO7G4xxZB"
      },
      "outputs": [],
      "source": [
        "class Embedding:\n",
        "  def __init__(self, vocab_size, sentence_len, latent_dim):\n",
        "    self.vocab_size = vocab_size\n",
        "    self.sentence_len = sentence_len\n",
        "    self.layer1 = Dense(vocab_size, latent_dim, True)\n",
        "    self.activationLayer1 = Activation_ReLU()\n",
        "    self.layer2 = Dense(latent_dim, vocab_size, True)\n",
        "    self.activationLayer2 = Activation_Sigmoid()\n",
        "    self.optimizer = Optimizer_Adam(learning_rate=0.01)#Rmsprop(learning_rate=0.0001)\n",
        "    self.trainableLayers = [self.layer1, self.layer2]\n",
        "    self.lossFunction = LossBinaryCrossEntropy()\n",
        "\n",
        "  def forward(self, input):\n",
        "    self.layer1.forward(input)\n",
        "    self.activationLayer1.forward(self.layer1.output)\n",
        "    self.layer2.forward(self.activationLayer1.output)\n",
        "    self.activationLayer2.forward(self.layer2.output)\n",
        "    return self.activationLayer2.output\n",
        "\n",
        "  def backward(self, loss):\n",
        "    self.activationLayer2.backward(loss)\n",
        "    self.layer2.backward(self.activationLayer2.dinputs)\n",
        "    self.activationLayer1.backward(self.layer2.dinputs)\n",
        "    self.layer1.backward(self.activationLayer1.dinputs)\n",
        "\n",
        "  def generateOneHotVector(self, input):\n",
        "    oneHotVectors = []\n",
        "    for position in input:\n",
        "      oneHot = np.zeros(self.vocab_size)\n",
        "      oneHot[position] = 1\n",
        "      oneHotVectors.append(oneHot)\n",
        "    return oneHotVectors\n",
        "\n",
        "  def get_SumContext(self, vector):\n",
        "    y_labels=[]\n",
        "    for i in range(len(vector)):\n",
        "      contexts=[]\n",
        "      if i == 0:\n",
        "        contexts = vector[i+1:i+3]\n",
        "      else:\n",
        "        min=i-2\n",
        "        if min<0:\n",
        "          min=0\n",
        "        if len(vector[i+1:i+3])!=0 and len(vector[min:i])!=0:\n",
        "          contexts = np.concatenate((vector[min:i], vector[i+1:i+3]))\n",
        "        elif len(vector[i+1:i+3])==0:\n",
        "          contexts = vector[min:i]\n",
        "      context=[]\n",
        "      for array in contexts:\n",
        "        if len(context)==0:\n",
        "          context = array\n",
        "        else:\n",
        "          context = context + array\n",
        "      y_labels.append(context)\n",
        "    return y_labels\n",
        "\n",
        "  def train(self, input, epoch):\n",
        "    for i in range(epoch):\n",
        "      lossByEpoch=0\n",
        "      for sentence in input:\n",
        "        lossBySentence=0\n",
        "        x_labels= self.generateOneHotVector(sentence)\n",
        "        y_labels = self.get_SumContext(vector=x_labels)\n",
        "        loss=np.zeros((1, 13))\n",
        "        for i in range(len(x_labels)):\n",
        "          prediction = self.forward(y_labels[i])\n",
        "          #print(\"prediction: \", prediction)\n",
        "          #print(f\"waited {i}: {x_labels[i]}\")\n",
        "          loss = loss + (prediction - x_labels[i])#self.lossFunction.forward(prediction,x_labels[i])\n",
        "          self.backward(prediction - x_labels[i])\n",
        "          self.optimizer.pre_update_params()\n",
        "          for layer in self.trainableLayers:\n",
        "            self.optimizer.update_params(layer)\n",
        "          self.optimizer.post_update_params()\n",
        "        lossBySentence = loss/len(x_labels)#np.mean(np.sqrt(loss*loss))\n",
        "        lossByEpoch+=np.mean(lossBySentence)#/len(x_labels)\n",
        "      print(\"Loss: \", lossByEpoch/len(input))\n",
        "\n",
        "  def getEmbedding(self, input):\n",
        "    oneHotVectors = self.generateOneHotVector(input)\n",
        "    embeddings = []\n",
        "    for oneHotVector in oneHotVectors:\n",
        "      self.layer1.forward(oneHotVector)\n",
        "      self.activationLayer1.forward(self.layer1.output)\n",
        "      embeddings.append(self.activationLayer1.output)\n",
        "    return embeddings\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtAY5aM26F7z"
      },
      "source": [
        "Manejo de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yez6tYvi6IRa",
        "outputId": "f64715ba-f498-497b-d40a-61608775fae5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the king of England is a man\n"
          ]
        }
      ],
      "source": [
        "# Leer set de entrenamiento\n",
        "filename = './english-spanish.pkl'\n",
        "\n",
        "#dataset = load(open(filename, 'rb'))\n",
        "#print(dataset[120000,0])\n",
        "#print(dataset[120000,1])\n",
        "dataset = np.array([np.array([\"el rey de Inglaterra es un hombre\",\"the king of England is a man\"]),\n",
        "                    np.array([\"la reina de Inglaterra es una mujer\",\"the queen of England is a woman\"]),\n",
        "                    np.array([\"Carlos es un rey\",\"Carlos is a king\"]),\n",
        "                    np.array([\"Andrea es una reina\",\"Andrea is a queen\"]),\n",
        "                    np.array([\"Carlos es un hombre\",\"Carlos is a man\"]),\n",
        "                    np.array([\"Andrea es una mujer\",\"Andrea is a woman\"])\n",
        "                    ])\n",
        "print(dataset[0,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EAJkCaC6M8k",
        "outputId": "36bfba4c-8b95-465c-f9f1-f41b97ee7760"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['el', 'rey', 'de', 'Inglaterra', 'es', 'un', 'hombre']\n",
            "['the', 'king', 'of', 'England', 'is', 'a', 'man']\n"
          ]
        }
      ],
      "source": [
        "# Crear \"tokens\"\n",
        "source_tokens = []\n",
        "for sentence in dataset[:,0]:\n",
        "  source_tokens.append(sentence.split(' '))\n",
        "print(source_tokens[0])\n",
        "\n",
        "target_tokens = []\n",
        "for sentence in dataset[:,1]:\n",
        "  target_tokens.append(sentence.split(' '))\n",
        "print(target_tokens[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkYe94YR6RKW"
      },
      "outputs": [],
      "source": [
        "def build_token_dict(token_list):\n",
        "  token_dict = {\n",
        "  #    '<PAD>': 0,\n",
        "  #    '<START>': 1,\n",
        "  #    '<END>': 2\n",
        "  }\n",
        "  for tokens in token_list:\n",
        "    for token in tokens:\n",
        "      if token not in token_dict:\n",
        "        token_dict[token] = len(token_dict)\n",
        "  return token_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEo9slak6VVE",
        "outputId": "e56d0aa5-4b4a-4c6b-807d-dfa8341cc566"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'el': 0, 'rey': 1, 'de': 2, 'Inglaterra': 3, 'es': 4, 'un': 5, 'hombre': 6, 'la': 7, 'reina': 8, 'una': 9, 'mujer': 10, 'Carlos': 11, 'Andrea': 12}\n",
            "{'the': 0, 'king': 1, 'of': 2, 'England': 3, 'is': 4, 'a': 5, 'man': 6, 'queen': 7, 'woman': 8, 'Carlos': 9, 'Andrea': 10}\n",
            "{0: 'the', 1: 'king', 2: 'of', 3: 'England', 4: 'is', 5: 'a', 6: 'man', 7: 'queen', 8: 'woman', 9: 'Carlos', 10: 'Andrea'}\n"
          ]
        }
      ],
      "source": [
        "source_token_dict = build_token_dict(source_tokens)\n",
        "source_token_dict_inv = {v:k for k,v in source_token_dict.items()}\n",
        "target_token_dict = build_token_dict(target_tokens)\n",
        "target_token_dict_inv = {v:k for k,v in target_token_dict.items()}\n",
        "\n",
        "print(source_token_dict)\n",
        "print(target_token_dict)\n",
        "print(target_token_dict_inv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjZq7UVG6YuO"
      },
      "outputs": [],
      "source": [
        "# Agregar start, end y pad a cada frase del set de entrenamiento\n",
        "#encoder_tokens = [['<START>'] + tokens + ['<END>'] for tokens in source_tokens]\n",
        "encoder_tokens = [tokens for tokens in source_tokens]\n",
        "#decoder_tokens = [['<START>'] + tokens + ['<END>'] for tokens in target_tokens]\n",
        "decoder_tokens = [tokens for tokens in target_tokens]\n",
        "#output_tokens = [tokens + ['<END>'] for tokens in target_tokens]\n",
        "output_tokens = [tokens for tokens in target_tokens]\n",
        "\n",
        "source_max_len = max(map(len, encoder_tokens))\n",
        "target_max_len = max(map(len, decoder_tokens))\n",
        "\n",
        "#encoder_tokens = [tokens + ['<PAD>']*(source_max_len-len(tokens)) for tokens in encoder_tokens]\n",
        "#decoder_tokens = [tokens + ['<PAD>']*(target_max_len-len(tokens)) for tokens in decoder_tokens]\n",
        "#output_tokens = [tokens + ['<PAD>']*(target_max_len-len(tokens)) for tokens in output_tokens ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcMGqShN6cLZ",
        "outputId": "0c470136-157c-4374-bb0e-2a2e5e35cea3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['la', 'reina', 'de', 'Inglaterra', 'es', 'una', 'mujer']\n"
          ]
        }
      ],
      "source": [
        "print(encoder_tokens[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KS6KDyq6h14"
      },
      "outputs": [],
      "source": [
        "encoder_input = [list(map(lambda x: source_token_dict[x], tokens)) for tokens in encoder_tokens]\n",
        "decoder_input = [list(map(lambda x: target_token_dict[x], tokens)) for tokens in decoder_tokens]\n",
        "output_decoded = [list(map(lambda x: [target_token_dict[x]], tokens)) for tokens in output_tokens]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AvSmG0ZER9V",
        "outputId": "8913a730-1514-4015-eaed-2d857fb26a31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7, 8, 2, 3, 4, 9, 10]\n"
          ]
        }
      ],
      "source": [
        "print(encoder_input[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Re0auX7F-5kA"
      },
      "source": [
        "Entrenamos el Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mof4mwjW6i7X",
        "outputId": "2f779eec-0661-451b-eee8-cae1b54135dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13\n",
            "7\n",
            "6\n"
          ]
        }
      ],
      "source": [
        "vocab_size = len(source_token_dict)\n",
        "print(vocab_size)\n",
        "print(target_max_len)\n",
        "print(len(decoder_input))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDGZFqVG_JqH"
      },
      "outputs": [],
      "source": [
        "embedding = Embedding(vocab_size, source_max_len, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smPhBX2g_dbt",
        "outputId": "d619178c-0f6a-4ae8-b68a-471949443396"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss:  0.37820996052628675\n",
            "Loss:  0.21560944575712337\n",
            "Loss:  0.05183054119609048\n",
            "Loss:  0.011986202490468336\n",
            "Loss:  0.004316049483205192\n",
            "Loss:  0.00195344078547292\n",
            "Loss:  0.000975645471880073\n",
            "Loss:  0.0005462883935365994\n",
            "Loss:  0.0003701535322431868\n",
            "Loss:  0.0003112702839787297\n",
            "Loss:  0.0003035455936189935\n",
            "Loss:  0.0003173533614725685\n",
            "Loss:  0.00034313927966020944\n",
            "Loss:  0.0003824444521564523\n",
            "Loss:  0.00044289022981562403\n",
            "Loss:  0.0005355109085603977\n",
            "Loss:  0.0006735293428027431\n",
            "Loss:  0.0008719443398855778\n",
            "Loss:  0.001147315188219277\n",
            "Loss:  0.001516863754121507\n",
            "Loss:  0.001995285521965683\n",
            "Loss:  0.0025862612019439883\n",
            "Loss:  0.003264346705324665\n",
            "Loss:  0.00394715629351695\n",
            "Loss:  0.004481791624141587\n",
            "Loss:  0.004705046744922421\n",
            "Loss:  0.004568831397676397\n",
            "Loss:  0.004076638100977112\n",
            "Loss:  0.002750994003089362\n",
            "Loss:  0.001279941059721265\n",
            "Loss:  0.0005983907467830488\n",
            "Loss:  -0.0001957389060367079\n",
            "Loss:  -0.0010182162713223435\n",
            "Loss:  -0.0008420223677921222\n",
            "Loss:  -0.00047028488154106347\n",
            "Loss:  -0.0006926767330522225\n",
            "Loss:  -0.0003326401919179411\n",
            "Loss:  1.3513261558868324e-05\n",
            "Loss:  3.605744788988714e-05\n",
            "Loss:  0.0001295979390280253\n",
            "Loss:  9.737021919678454e-05\n",
            "Loss:  0.00017574489014061915\n",
            "Loss:  0.00015492563070616921\n",
            "Loss:  0.00030899691802273945\n",
            "Loss:  0.0005462433006651569\n",
            "Loss:  0.0008144762750205417\n",
            "Loss:  0.0010942799461629996\n",
            "Loss:  0.0013691883099567238\n",
            "Loss:  0.0016317007016875542\n",
            "Loss:  0.0018795938799405323\n",
            "Loss:  0.002113241006464908\n",
            "Loss:  0.002333926368289966\n",
            "Loss:  0.0025430749535341984\n",
            "Loss:  0.0027419639653230095\n",
            "Loss:  0.0029316714138744922\n",
            "Loss:  0.003113118208656233\n",
            "Loss:  0.0032871320379395477\n",
            "Loss:  0.003454501612673583\n",
            "Loss:  0.0036160100389260054\n",
            "Loss:  0.003772446096790736\n",
            "Loss:  0.003924597335433159\n",
            "Loss:  0.004073231432286004\n",
            "Loss:  0.0042190729161331484\n",
            "Loss:  0.004362781458069036\n",
            "Loss:  0.00450493598473044\n",
            "Loss:  0.00464602653969461\n",
            "Loss:  0.00478645374889383\n",
            "Loss:  0.004926534333409785\n",
            "Loss:  0.005066510463503839\n",
            "Loss:  0.005206560746215296\n",
            "Loss:  0.005346811068605209\n",
            "Loss:  0.0054873441563098\n",
            "Loss:  0.005628207369898719\n",
            "Loss:  0.005769418818978702\n",
            "Loss:  0.00591097224367546\n",
            "Loss:  0.0060528412582422884\n",
            "Loss:  0.0061949834832981145\n",
            "Loss:  0.006337344872935551\n",
            "Loss:  0.006479864273304765\n",
            "Loss:  0.006622478046586289\n",
            "Loss:  0.00676512454481451\n",
            "Loss:  0.006907748335448077\n",
            "Loss:  0.007120950586901685\n",
            "Loss:  0.007271576500526913\n",
            "Loss:  0.007459059100459385\n",
            "Loss:  0.007646026299763038\n",
            "Loss:  0.0077820819260869326\n",
            "Loss:  0.007936830502384182\n",
            "Loss:  0.008090922424366093\n",
            "Loss:  0.00824544841441247\n",
            "Loss:  0.008401029010215113\n",
            "Loss:  0.008557091947723774\n",
            "Loss:  0.008686594548193217\n",
            "Loss:  0.008839483532845627\n",
            "Loss:  0.00899475321249267\n",
            "Loss:  0.009149556113069543\n",
            "Loss:  0.009306986893933288\n",
            "Loss:  0.00946438509269564\n",
            "Loss:  0.009619258465634094\n",
            "Loss:  0.00977105765455032\n",
            "Loss:  0.009918969217068427\n",
            "Loss:  0.010061479407559804\n",
            "Loss:  0.01019727392238066\n",
            "Loss:  0.010325416863877826\n",
            "Loss:  0.010445064116667839\n",
            "Loss:  0.010555384689576203\n",
            "Loss:  0.010655661006656299\n",
            "Loss:  0.010745353575931331\n",
            "Loss:  0.010824106662142839\n",
            "Loss:  0.01089174742502133\n",
            "Loss:  0.010948288468797385\n",
            "Loss:  0.010993910824940159\n",
            "Loss:  0.010915450652164085\n",
            "Loss:  0.010831797918573862\n",
            "Loss:  0.01096327476269922\n",
            "Loss:  0.010756575513134978\n",
            "Loss:  0.010619507115994742\n",
            "Loss:  0.010394728961469167\n",
            "Loss:  0.010884590993712877\n",
            "Loss:  0.011434618352159975\n",
            "Loss:  0.010875920238903651\n",
            "Loss:  0.01153543773721234\n",
            "Loss:  0.01093690606990333\n",
            "Loss:  0.012376577278785057\n",
            "Loss:  0.00788664222690673\n",
            "Loss:  0.013382245664444126\n",
            "Loss:  0.012510828627317337\n",
            "Loss:  0.010407410802392209\n",
            "Loss:  0.011893252356824651\n",
            "Loss:  0.011092587544081866\n",
            "Loss:  0.01067463089767981\n",
            "Loss:  0.011449197045724085\n",
            "Loss:  0.010963020715267675\n",
            "Loss:  0.010573366127147868\n",
            "Loss:  0.011088124974776972\n",
            "Loss:  0.010634708673543112\n",
            "Loss:  0.010851192536742904\n",
            "Loss:  0.010596127798104926\n",
            "Loss:  0.010636312284649738\n",
            "Loss:  0.010495955250003556\n",
            "Loss:  0.010381340510468164\n",
            "Loss:  0.010582027742269337\n",
            "Loss:  0.010073483106172797\n",
            "Loss:  0.010094408819560856\n",
            "Loss:  0.010428059955888885\n",
            "Loss:  0.010193890827745694\n",
            "Loss:  0.010334696851134896\n",
            "Loss:  0.009738353886859225\n",
            "Loss:  0.009800984270432805\n",
            "Loss:  0.009751195145808427\n",
            "Loss:  0.009709604348581069\n",
            "Loss:  0.009934144427938864\n",
            "Loss:  0.00995398143360339\n",
            "Loss:  0.010067730464271505\n",
            "Loss:  0.009494629354572248\n",
            "Loss:  0.00945974421822193\n",
            "Loss:  0.009431515868477706\n",
            "Loss:  0.009405283036674981\n",
            "Loss:  0.009527421722580235\n",
            "Loss:  0.009471949418279961\n",
            "Loss:  0.00895398464169398\n",
            "Loss:  0.009135831155685835\n",
            "Loss:  0.009077343950579526\n",
            "Loss:  0.008984905141967179\n",
            "Loss:  0.00901956083808178\n",
            "Loss:  0.008840360643943114\n",
            "Loss:  0.00897748078702248\n",
            "Loss:  0.00962151012569471\n",
            "Loss:  0.009081683999114909\n",
            "Loss:  0.009200442410667269\n",
            "Loss:  0.008590213203331696\n",
            "Loss:  0.00881897211147363\n",
            "Loss:  0.008828016720425826\n",
            "Loss:  0.008796064374029718\n",
            "Loss:  0.008829253149335395\n",
            "Loss:  0.008746235943709746\n",
            "Loss:  0.008789527811957623\n",
            "Loss:  0.008707857260658024\n",
            "Loss:  0.008758569418304045\n",
            "Loss:  0.009075661972095347\n",
            "Loss:  0.00883407085443958\n",
            "Loss:  0.008540397319482031\n",
            "Loss:  0.008689203814924717\n",
            "Loss:  0.008643141846096656\n",
            "Loss:  0.009467805205829237\n",
            "Loss:  0.008277538113852989\n",
            "Loss:  0.008990912832851689\n",
            "Loss:  0.00854023443623344\n",
            "Loss:  0.00876155911549495\n",
            "Loss:  0.009052600302283886\n",
            "Loss:  0.008804888185931925\n",
            "Loss:  0.008572214873522251\n",
            "Loss:  0.008764891613404121\n",
            "Loss:  0.008715188882446196\n",
            "Loss:  0.008862019339524187\n",
            "Loss:  0.008729505096563702\n",
            "Loss:  0.008872740356562311\n",
            "Loss:  0.008737168446084546\n",
            "Loss:  0.008902778490787225\n",
            "Loss:  0.009214102651832787\n",
            "Loss:  0.008781809957432345\n",
            "Loss:  0.009384121451453563\n",
            "Loss:  0.008642464041414081\n",
            "Loss:  0.009042675994223232\n",
            "Loss:  0.009051280509515555\n",
            "Loss:  0.008923390797161665\n",
            "Loss:  0.009123435415367016\n",
            "Loss:  0.008925319238978631\n",
            "Loss:  0.009167349174330743\n",
            "Loss:  0.008955613220554118\n",
            "Loss:  0.009666035072183566\n",
            "Loss:  0.008845147330277737\n",
            "Loss:  0.009143460950723504\n",
            "Loss:  0.008961481997492046\n",
            "Loss:  0.009389518747640971\n",
            "Loss:  0.009096288704836808\n",
            "Loss:  0.009418104229534952\n",
            "Loss:  0.009126683283170638\n",
            "Loss:  0.009649866103411186\n",
            "Loss:  0.009379364812663214\n",
            "Loss:  0.009785788859632964\n",
            "Loss:  0.009292361572244131\n",
            "Loss:  0.00940454692238009\n",
            "Loss:  0.00961113874560986\n",
            "Loss:  0.009769593723745374\n",
            "Loss:  0.009781138478961558\n",
            "Loss:  0.009846583799218865\n",
            "Loss:  0.009815111529781297\n",
            "Loss:  0.00986366858548682\n",
            "Loss:  0.010439986924018823\n",
            "Loss:  0.0093356998332431\n",
            "Loss:  0.00996740816156642\n",
            "Loss:  0.010554047201012369\n",
            "Loss:  0.009717188987829209\n",
            "Loss:  0.009778824471556218\n",
            "Loss:  0.010212444766764401\n",
            "Loss:  0.010477770146295062\n",
            "Loss:  0.01051185143540065\n",
            "Loss:  0.010499217802285988\n",
            "Loss:  0.010526487901545431\n",
            "Loss:  0.010625403358734057\n",
            "Loss:  0.010682461249486222\n",
            "Loss:  0.010687423932998411\n",
            "Loss:  0.011329870732907173\n",
            "Loss:  0.01017487537486681\n",
            "Loss:  0.010597713199776817\n",
            "Loss:  0.011789973538035488\n",
            "Loss:  0.01137750543486137\n",
            "Loss:  0.011329019934982512\n",
            "Loss:  0.01123285797341872\n",
            "Loss:  0.011265035288986591\n",
            "Loss:  0.011299648476112605\n",
            "Loss:  0.011362902489828415\n",
            "Loss:  0.011606273746712192\n",
            "Loss:  0.011994030165183809\n",
            "Loss:  0.011917796423369658\n",
            "Loss:  0.011570247386394178\n",
            "Loss:  0.011819225938156255\n",
            "Loss:  0.011848119018811215\n",
            "Loss:  0.011994036569482848\n",
            "Loss:  0.011776098399810345\n",
            "Loss:  0.01202512856483539\n",
            "Loss:  0.01194571936026381\n",
            "Loss:  0.011950127472660783\n",
            "Loss:  0.012834535653384329\n",
            "Loss:  0.01107100031006448\n",
            "Loss:  0.011844671508462827\n",
            "Loss:  0.012476901477645042\n",
            "Loss:  0.0122938418965989\n",
            "Loss:  0.012485349536370843\n",
            "Loss:  0.012417861920881264\n",
            "Loss:  0.012365427826735836\n",
            "Loss:  0.012208734153407326\n",
            "Loss:  0.012208314361837837\n",
            "Loss:  0.01244946119357876\n",
            "Loss:  0.012425755521235453\n",
            "Loss:  0.01281277766209934\n",
            "Loss:  0.010989225756451315\n",
            "Loss:  0.012024404874699695\n",
            "Loss:  0.012337433079053592\n",
            "Loss:  0.012454984941372546\n",
            "Loss:  0.0122295836010555\n",
            "Loss:  0.012570897446774552\n",
            "Loss:  0.012247553524420099\n",
            "Loss:  0.01213612934200099\n",
            "Loss:  0.012210807511592087\n",
            "Loss:  0.01189402466896207\n",
            "Loss:  0.012360930813039864\n",
            "Loss:  0.011994015579685042\n",
            "Loss:  0.012569967702387982\n",
            "Loss:  0.010814970603951505\n",
            "Loss:  0.011319377992792869\n",
            "Loss:  0.012029850514647798\n",
            "Loss:  0.012243249917065276\n",
            "Loss:  0.01180942326426365\n",
            "Loss:  0.011817759653890094\n",
            "Loss:  0.011688111209716898\n",
            "Loss:  0.011985161409118811\n",
            "Loss:  0.011594705354123672\n",
            "Loss:  0.01159417498158871\n",
            "Loss:  0.011294989966188213\n",
            "Loss:  0.011408481203597026\n",
            "Loss:  0.012325108615773494\n",
            "Loss:  0.010245194465522743\n",
            "Loss:  0.01050629096877742\n",
            "Loss:  0.011241090277289803\n",
            "Loss:  0.011275481308703863\n",
            "Loss:  0.011511196725323726\n",
            "Loss:  0.01120260809712672\n",
            "Loss:  0.01104137723735061\n",
            "Loss:  0.01095432832206659\n",
            "Loss:  0.010952247338910941\n",
            "Loss:  0.010700714880158418\n",
            "Loss:  0.01116042347816868\n",
            "Loss:  0.010717167037387438\n",
            "Loss:  0.011111513590569588\n",
            "Loss:  0.00977347190667051\n",
            "Loss:  0.009847731912315536\n",
            "Loss:  0.01050175040823345\n",
            "Loss:  0.01082171962438948\n",
            "Loss:  0.01047947167771641\n",
            "Loss:  0.010316613032293481\n",
            "Loss:  0.01042069467966581\n",
            "Loss:  0.010055485520075116\n",
            "Loss:  0.010220491057582012\n",
            "Loss:  0.01046613966309412\n",
            "Loss:  0.010013073886097014\n",
            "Loss:  0.010377711666257092\n",
            "Loss:  0.009092619367804077\n",
            "Loss:  0.009323289420476447\n",
            "Loss:  0.009924004102672397\n",
            "Loss:  0.009600676903547262\n",
            "Loss:  0.010338920168425858\n",
            "Loss:  0.009585464483509989\n",
            "Loss:  0.009676954229765012\n",
            "Loss:  0.009584928702066722\n",
            "Loss:  0.009491960308954142\n",
            "Loss:  0.009460647825953817\n",
            "Loss:  0.009832521668622392\n",
            "Loss:  0.009944860496891242\n",
            "Loss:  0.008709911391918875\n",
            "Loss:  0.008474733807642092\n",
            "Loss:  0.009191688700991761\n",
            "Loss:  0.009185855883710273\n",
            "Loss:  0.009176719752083552\n",
            "Loss:  0.00911743228245903\n",
            "Loss:  0.00905203830070895\n",
            "Loss:  0.009563201338657632\n",
            "Loss:  0.008818421643431421\n",
            "Loss:  0.009107837539597327\n",
            "Loss:  0.008665674708082194\n",
            "Loss:  0.008830128896383495\n",
            "Loss:  0.009336020129796944\n",
            "Loss:  0.007990059071526265\n",
            "Loss:  0.007879113245022927\n",
            "Loss:  0.008976311671356126\n",
            "Loss:  0.008343832336293685\n",
            "Loss:  0.008599941401885572\n",
            "Loss:  0.008559420100371933\n",
            "Loss:  0.008346277203478593\n",
            "Loss:  0.008361223400305741\n",
            "Loss:  0.0082855229370785\n",
            "Loss:  0.008217312902412028\n",
            "Loss:  0.008378242102517282\n",
            "Loss:  0.008887903227548604\n",
            "Loss:  0.007928025896813093\n",
            "Loss:  0.007725992307738791\n",
            "Loss:  0.008018180347464206\n",
            "Loss:  0.008017681113521467\n",
            "Loss:  0.007980083922236963\n",
            "Loss:  0.007970987115559951\n",
            "Loss:  0.007869539361166475\n",
            "Loss:  0.008452658944043947\n",
            "Loss:  0.008395529780964478\n",
            "Loss:  0.0072833707770103675\n",
            "Loss:  0.007206005117469117\n",
            "Loss:  0.007611776958350108\n",
            "Loss:  0.007747894548518225\n",
            "Loss:  0.00772255649605047\n",
            "Loss:  0.00792215684785695\n",
            "Loss:  0.007446477048159379\n",
            "Loss:  0.007655700650282957\n",
            "Loss:  0.008150426516762907\n",
            "Loss:  0.007656189094007973\n",
            "Loss:  0.007592550210034313\n",
            "Loss:  0.007570649175579382\n",
            "Loss:  0.007997200909781845\n",
            "Loss:  0.007457881630612497\n",
            "Loss:  0.007034658952456366\n",
            "Loss:  0.007270501700148875\n",
            "Loss:  0.007343167646112522\n",
            "Loss:  0.007605015340365001\n",
            "Loss:  0.007139957682521388\n",
            "Loss:  0.007353599004637248\n",
            "Loss:  0.007325362123247736\n",
            "Loss:  0.007248724570685343\n",
            "Loss:  0.00832897560595822\n",
            "Loss:  0.0071906017089331\n",
            "Loss:  0.006709215677613521\n",
            "Loss:  0.007019595361920389\n",
            "Loss:  0.007071367631998307\n",
            "Loss:  0.0071016663079956355\n",
            "Loss:  0.007107325858122339\n",
            "Loss:  0.007344178253242503\n",
            "Loss:  0.006876869674272035\n",
            "Loss:  0.0070826878360230675\n",
            "Loss:  0.007105773078311304\n",
            "Loss:  0.00699768646582852\n",
            "Loss:  0.007603365818449542\n",
            "Loss:  0.006830367527210723\n",
            "Loss:  0.006944791354235098\n",
            "Loss:  0.006771924705881008\n",
            "Loss:  0.006801918663186471\n",
            "Loss:  0.00690211937053815\n",
            "Loss:  0.006920094688513039\n",
            "Loss:  0.0069223028055006115\n",
            "Loss:  0.007135919449644094\n",
            "Loss:  0.006760756762485898\n",
            "Loss:  0.007198850058464431\n",
            "Loss:  0.0071132394265531\n",
            "Loss:  0.0066040449066035135\n",
            "Loss:  0.006827289109612354\n",
            "Loss:  0.006786799774759117\n",
            "Loss:  0.006842045247352167\n",
            "Loss:  0.0070724052236047635\n",
            "Loss:  0.007655919438264054\n",
            "Loss:  0.0063947776413754475\n",
            "Loss:  0.0071236888472041545\n",
            "Loss:  0.0065833507688986895\n",
            "Loss:  0.0066637666205383835\n",
            "Loss:  0.006879717753754522\n",
            "Loss:  0.00714507737567422\n",
            "Loss:  0.006929874209745758\n",
            "Loss:  0.006428631031570045\n",
            "Loss:  0.0067315491240948835\n",
            "Loss:  0.006710106177038676\n",
            "Loss:  0.006804564964537887\n",
            "Loss:  0.0066879032962670094\n",
            "Loss:  0.006825980467640655\n",
            "Loss:  0.00708349949320129\n",
            "Loss:  0.005283100708368631\n",
            "Loss:  0.00513782415442455\n",
            "Loss:  0.006466144092512255\n",
            "Loss:  0.006715851550132983\n",
            "Loss:  0.007361931997692171\n",
            "Loss:  0.00683984910558351\n",
            "Loss:  0.007231256896581966\n",
            "Loss:  0.006835026934439701\n",
            "Loss:  0.0067323602710310905\n",
            "Loss:  0.006698193412084765\n",
            "Loss:  0.0067715344596452\n",
            "Loss:  0.006918416480620568\n",
            "Loss:  0.00734368971523305\n",
            "Loss:  0.0063405862795479434\n",
            "Loss:  0.00663606283587202\n",
            "Loss:  0.00668008555801784\n",
            "Loss:  0.006449735445954505\n",
            "Loss:  0.0066685271559870675\n",
            "Loss:  0.006508426193396308\n",
            "Loss:  0.006667393462104903\n",
            "Loss:  0.006444252233105599\n",
            "Loss:  0.0066601244118152285\n",
            "Loss:  0.006708274073127547\n",
            "Loss:  0.006993576510701542\n",
            "Loss:  0.006404204188758239\n",
            "Loss:  0.006638260046080406\n",
            "Loss:  0.006547821371486735\n",
            "Loss:  0.0066251694894703264\n",
            "Loss:  0.006914576153187478\n",
            "Loss:  0.00667861146020583\n",
            "Loss:  0.006637044400450732\n",
            "Loss:  0.0065328635273621165\n",
            "Loss:  0.006611742007375629\n",
            "Loss:  0.0067886339366505745\n",
            "Loss:  0.006864859823245071\n",
            "Loss:  0.0068101124365229195\n",
            "Loss:  0.007460068156719555\n",
            "Loss:  0.006834393241913118\n",
            "Loss:  0.007109496755632167\n",
            "Loss:  0.006841922619106299\n",
            "Loss:  0.006827919642689181\n",
            "Loss:  0.0069458268933482595\n",
            "Loss:  0.0071582254142042966\n",
            "Loss:  0.00639686025683452\n",
            "Loss:  0.006285046670461487\n",
            "Loss:  0.00681759646037277\n",
            "Loss:  0.006903944131311474\n",
            "Loss:  0.006858649596658606\n",
            "Loss:  0.00698573423915898\n",
            "Loss:  0.007038738652219621\n",
            "Loss:  0.007357280338471338\n",
            "Loss:  0.00672714723915635\n",
            "Loss:  0.006981483337364935\n",
            "Loss:  0.006954624465691413\n",
            "Loss:  0.006902619003470609\n",
            "Loss:  0.006999724803479854\n",
            "Loss:  0.006870148193632411\n",
            "Loss:  0.007017943884850928\n",
            "Loss:  0.007031200900096554\n",
            "Loss:  0.007099743832364796\n"
          ]
        }
      ],
      "source": [
        "embedding.train(encoder_input, 500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLlcUFV1ILch",
        "outputId": "1c41cc04-5851-4660-a624-01c5ff5c6e4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n",
            "un\n"
          ]
        }
      ],
      "source": [
        "clave=source_token_dict['un']\n",
        "print(clave)\n",
        "valor=source_token_dict_inv[clave]\n",
        "print(valor)\n",
        "#print(source_token_dict['women'])\n",
        "#print(source_token_dict['queen'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faSpTmrsH0vm",
        "outputId": "b25e466c-2ee9-4dee-c349-69bb500515ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13\n"
          ]
        }
      ],
      "source": [
        "embedding_dic={}\n",
        "for i in range(len(source_token_dict)):\n",
        "  response = embedding.getEmbedding([i])\n",
        "  embedding_dic[i]=response\n",
        "print(len(embedding_dic))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open(\"embeddings.pkl\", \"wb\") as tf:\n",
        "    pickle.dump(embedding_dic, tf)"
      ],
      "metadata": {
        "id": "ZPZhimdGF86R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Notamos que los resultados son muy variados, por lo que cargamos uno por defecto\n",
        "with open(\"embeddings.pkl\", \"rb\") as tf:\n",
        "    embedding_dic = pickle.load(tf)"
      ],
      "metadata": {
        "id": "_yGtFw21Psw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "x=[]\n",
        "y=[]\n",
        "for key, value in embedding_dic.items():\n",
        "  x.append(embedding_dic[key][0][0][0])\n",
        "  y.append(embedding_dic[key][0][0][1])\n",
        "  #words.append(source_token_dict_inv[key])\n",
        "plt.scatter(x, y)\n",
        "\n",
        "for key, value in embedding_dic.items():\n",
        "  plt.annotate(source_token_dict_inv[key], (embedding_dic[key][0][0][0], embedding_dic[key][0][0][1]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "oj8jDxPiHb3w",
        "outputId": "53688abe-dc58-44e4-b691-3369a370d5ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqkwovI8Vey-",
        "outputId": "fbfd2649-879b-4745-9ada-7bbe51d479ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[2.79838548, 3.99352233]])]\n"
          ]
        }
      ],
      "source": [
        "embedding_of_rey = embedding_dic[1]\n",
        "print(embedding_of_rey)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baYuk5cRV4EV",
        "outputId": "a00efc84-47e7-42a6-8305-810e4a18e64f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[0.        , 3.72389873]])]\n"
          ]
        }
      ],
      "source": [
        "embedding_of_hombre = embedding_dic[6]\n",
        "print(embedding_of_hombre)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMXE_23iWH2E",
        "outputId": "fcd5c17d-a0fa-493d-b854-cff5e8bc4cdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[2.2120002 , 0.57856849]])]\n"
          ]
        }
      ],
      "source": [
        "embedding_of_mujer = embedding_dic[10]\n",
        "print(embedding_of_mujer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21MF3a8KWSHv",
        "outputId": "90c4a15a-eb1e-4795-c9bc-28753bc9bf72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[5.01038568 0.84819209]]\n",
            "('reina', 0.6479086502553051)\n",
            "[array([[4.49982801, 1.24708618]])]\n",
            "[array([[4.49982801, 1.24708618]])]\n"
          ]
        }
      ],
      "source": [
        "analogia = embedding_of_rey[0]-embedding_of_hombre[0]+embedding_of_mujer[0]\n",
        "print(analogia)\n",
        "distances={}\n",
        "for key, value in embedding_dic.items():\n",
        "  distance=cosine_similarity(analogia, value)\n",
        "  distances[source_token_dict_inv[key]]=distance\n",
        "sorted_similarities = sorted(distances.items(), key=lambda x: x[1])\n",
        "print(sorted_similarities[0])\n",
        "print(embedding_dic[source_token_dict[sorted_similarities[0][0]]])\n",
        "print(embedding_dic[8])#source_token_dict[sorted_similarities[0][0]]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "erdqVc5iYT3i"
      },
      "outputs": [],
      "source": [
        "def cosine_similarity(v1, v2):\n",
        "  #return (np.array(v1)@np.array(v2))/(np.linalg.norm(v1)*np.linalg.norm(v2))\n",
        "  return np.sqrt(np.sum((np.power((v1-v2),2))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvgKb5MRzuwV"
      },
      "outputs": [],
      "source": [
        "def most_similar(word, word_dict, top_k=4):\n",
        "  if word not in word_dict:\n",
        "    raise ValueError(f\"{word} not found in the dictionary\")\n",
        "  else:\n",
        "    key = word_dict[word]\n",
        "\n",
        "    queryVector = embedding_dic[key]\n",
        "\n",
        "    similarities = {}\n",
        "    for key2, value in embedding_dic.items():\n",
        "      if key!=key2:\n",
        "        similarity = cosine_similarity(queryVector[0][0], value[0][0])\n",
        "        similarities[source_token_dict_inv[key2]]=similarity\n",
        "  sorted_similarities = sorted(similarities.items(), key=lambda x: x[1])\n",
        "  return sorted_similarities[:top_k]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jTlJoK92GOm",
        "outputId": "a63eeb5c-68fe-4d77-98c8-6c61d9d7339e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('es', 0.016783978304294678),\n",
              " ('la', 1.4846223221923354),\n",
              " ('el', 2.2444004849108032),\n",
              " ('rey', 2.8113445485102053)]"
            ]
          },
          "metadata": {},
          "execution_count": 790
        }
      ],
      "source": [
        "most_similar('hombre', source_token_dict, 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U74LNNu1Io2U"
      },
      "source": [
        "**Ahora veremos el término \"Positional Encoding\"**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcA79xihIwbb"
      },
      "outputs": [],
      "source": [
        "def get_positionalEncoding(len_Sentence, dim_embedding):\n",
        "  pos = np.arange(len_Sentence)[:, np.newaxis]\n",
        "  angle_rads = pos / np.power(10000, (2 * (np.arange(dim_embedding) // 2)) / np.float32(dim_embedding))\n",
        "  # Aplicar seno a los índices pares en el arreglo\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "  # Aplicar coseno a los índices impares en el arreglo\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "  return angle_rads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dWkFjGvqi7z",
        "outputId": "e06198b9-7a11-4db8-b5f9-2d40c3a1cc26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 1.]\n",
            "[0.84147098 0.54030231]\n",
            "[ 0.90929743 -0.41614684]\n",
            "[ 0.14112001 -0.9899925 ]\n",
            "[-0.7568025  -0.65364362]\n",
            "[-0.95892427  0.28366219]\n",
            "[-0.2794155   0.96017029]\n"
          ]
        }
      ],
      "source": [
        "positional_encoding = get_positionalEncoding(7, 2)\n",
        "for vector in positional_encoding:\n",
        "  print(vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        },
        "id": "O0as2s2T0Y0Y",
        "outputId": "02ab8588-8dfc-486a-aa06-d7efecf981f1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.colorbar.Colorbar at 0x7cf4cb3dbca0>"
            ]
          },
          "metadata": {},
          "execution_count": 781
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x933.333 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "P = get_positionalEncoding(7, 3)\n",
        "cax = plt.matshow(P)\n",
        "plt.gcf().colorbar(cax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsTckgK6Hrj4"
      },
      "source": [
        "Sumamos el embedding + la codificación posicional para la frase **la reina de inglarterra es una mujer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oapwTAJmH4v3",
        "outputId": "613cca37-7f16-4bf7-b810-5400a8871066"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding\n",
            "[0.         5.20852105]\n",
            "[4.49982801 1.24708618]\n",
            "[2.08608161 0.        ]\n",
            "[0. 0.]\n",
            "[0.         3.70711475]\n",
            "[3.43717866 0.        ]\n",
            "[2.2120002  0.57856849]\n",
            "\n",
            "Positional Encoding\n",
            "[0. 1.]\n",
            "[0.84147098 0.54030231]\n",
            "[ 0.90929743 -0.41614684]\n",
            "[ 0.14112001 -0.9899925 ]\n",
            "[-0.7568025  -0.65364362]\n",
            "[-0.95892427  0.28366219]\n",
            "[-0.2794155   0.96017029]\n",
            "\n",
            "Input for Encoder\n",
            "[0.         6.20852105]\n",
            "[5.341299   1.78738849]\n",
            "[ 2.99537904 -0.41614684]\n",
            "[ 0.14112001 -0.9899925 ]\n",
            "[-0.7568025   3.05347113]\n",
            "[2.47825438 0.28366219]\n",
            "[1.9325847  1.53873878]\n"
          ]
        }
      ],
      "source": [
        "embedding_of_sentence = embedding.getEmbedding(encoder_input[1])\n",
        "print(\"Embedding\")\n",
        "for value in embedding_of_sentence:\n",
        "  print(value[0])\n",
        "print(\"\\nPositional Encoding\")\n",
        "for value in np.array(positional_encoding):\n",
        "  print(value)\n",
        "print(\"\\nInput for Encoder\")\n",
        "for i in range(len(embedding_of_sentence)):\n",
        "  print(embedding_of_sentence[i][0]+np.array(positional_encoding)[i])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASM0zjgWcXC8"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xb76eFqcRLR",
        "outputId": "62963558-c027-4232-e390-13ef1bfa5b5c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3.1622776601683795"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.sqrt(np.sum(np.power(np.array([3, 2, 5])-np.array([2, 2, 2]), 2)))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOmqajQG7RxJnLRoKvQQB3i",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}