{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JuanDavid1217/Generative-AI-from-Scratch/blob/main/Transformers/Embedding_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAvXfaR8qZTX"
      },
      "source": [
        "**Primero se necesita entender como funcionan lo Embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BT8gperVqUhz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from pickle import load\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wXqWbx_BxQo1"
      },
      "outputs": [],
      "source": [
        "class Activation_ReLU:\n",
        "  def __init__(self):\n",
        "    self.input=[]\n",
        "    self.output=[]\n",
        "    self.dinputs=[]\n",
        "\n",
        "  def forward(self, input):\n",
        "    self.input = input\n",
        "    self.output=np.maximum(0, input)\n",
        "\n",
        "  def backward(self, dvalues):\n",
        "    dvalues = np.sum(dvalues, axis=0, keepdims=True)\n",
        "    self.dinputs=dvalues*np.where(self.output > 0, 1, 0)\n",
        "\n",
        "class Activation_Sigmoid:\n",
        "  def __init__(self):\n",
        "    self.input=[]\n",
        "    self.output=[]\n",
        "    self.dinputs=[]\n",
        "\n",
        "  def forward(self, input):\n",
        "    self.output = 1 / (1 + np.exp(-input))\n",
        "\n",
        "  def backward(self, dvalues):\n",
        "    self.dinputs = dvalues * (1 - self.output) * self.output\n",
        "\n",
        "class Activation_SoftMax:\n",
        "  def __init__(self):\n",
        "    self.input=[]\n",
        "    self.output=[]\n",
        "    self.dinputs=[]\n",
        "\n",
        "  def forward(self, input):\n",
        "    exp_z = np.exp(input - np.max(input))\n",
        "    self.output = exp_z / np.sum(exp_z, axis=0)\n",
        "\n",
        "  def backward(self, dvalues):\n",
        "    # Create uninitialized array\n",
        "    self.dinputs = np.empty_like(dvalues)\n",
        "    # Enumerate outputs and gradients\n",
        "    for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
        "      # Flatten output array\n",
        "      single_output = single_output.reshape(-1, 1)\n",
        "      # Calculate Jacobian matrix of the output and\n",
        "      jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
        "      # Calculate sample-wise gradient\n",
        "      # and add it to the array of sample gradients\n",
        "      self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VkywBDDRxZhn"
      },
      "outputs": [],
      "source": [
        "class Dense:\n",
        "  def __init__(self, input_dim, neurons, bias=False):\n",
        "    self.weights=0.1*np.random.randn(input_dim, neurons)\n",
        "    self.bias=bias\n",
        "    if self.bias:\n",
        "      self.biases=np.zeros((1, neurons))\n",
        "      self.dbiases=0\n",
        "    self.input=[]\n",
        "    self.output=[]\n",
        "    self.dweights=0\n",
        "    self.dinputs=0\n",
        "\n",
        "  def  forward(self, input):\n",
        "    self.input=input\n",
        "    if self.bias:\n",
        "      self.output = np.dot(self.input, self.weights)+self.biases\n",
        "    else:\n",
        "      self.output=np.dot(self.input, self.weights)\n",
        "\n",
        "  def backward(self, dvalues):\n",
        "    if self.bias:\n",
        "      self.dbiases=dvalues\n",
        "    else:\n",
        "      if dvalues.shape[0]!=1:\n",
        "        dvalues=dvalues.reshape(1, -1)\n",
        "\n",
        "    if self.input.shape[0]!=1:\n",
        "        self.input=self.input.reshape(1, -1)\n",
        "    self.dweights=np.dot(self.input.T, dvalues)\n",
        "    self.dinputs=np.dot(dvalues, self.weights.T)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "utieyHkpxvbo"
      },
      "outputs": [],
      "source": [
        "class Optimizer_Adam:\n",
        "  def __init__(self, learning_rate=0.01, decay=0., epsilon=1e-7,beta_1=0.9, beta_2=0.999):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.current_learning_rate = learning_rate\n",
        "    self.decay = decay\n",
        "    self.iterations = 0\n",
        "    self.epsilon = epsilon\n",
        "    self.beta_1 = beta_1\n",
        "    self.beta_2 = beta_2\n",
        "\n",
        "\n",
        "  def pre_update_params(self):\n",
        "    if self.decay:\n",
        "      self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "  def update_params(self, layer):\n",
        "    if not hasattr(layer, 'weight_cache'):\n",
        "      layer.weight_momentums = np.zeros_like(layer.weights)\n",
        "      layer.weight_cache = np.zeros_like(layer.weights)\n",
        "      if hasattr(layer, 'biases'):\n",
        "        layer.bias_momentums = np.zeros_like(layer.biases)\n",
        "        layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "    # Update momentum with current gradients\n",
        "    layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights\n",
        "    if hasattr(layer, 'biases'):\n",
        "      layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases\n",
        "    # Get corrected momentum\n",
        "    # self.iteration is 0 at first pass\n",
        "    # and we need to start with 1 here\n",
        "    weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
        "    if hasattr(layer, 'biases'):\n",
        "      bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
        "    # Update cache with squared current gradients\n",
        "    layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights**2\n",
        "    if hasattr(layer, 'biases'):\n",
        "      layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases**2\n",
        "    # Get corrected cache\n",
        "    weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
        "    if hasattr(layer, 'biases'):\n",
        "      bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
        "    # Vanilla SGD parameter update + normalization\n",
        "    # with square rooted cache\n",
        "    layer.weights += -self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
        "    if hasattr(layer, 'biases'):\n",
        "      layer.biases += -self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
        "\n",
        "  # Call once after any parameter updates\n",
        "  def post_update_params(self):\n",
        "    self.iterations += 1\n",
        "\n",
        "\n",
        "class Rmsprop:\n",
        "  def __init__(self, learning_rate=0.1, decay=0., epsilon=1e-8, rho=0.9):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.current_learning_rate = learning_rate\n",
        "    self.decay = decay\n",
        "    self.iterations = 0\n",
        "    self.epsilon = epsilon\n",
        "    self.rho = rho\n",
        "\n",
        "  def pre_update_params(self):\n",
        "    if self.decay:\n",
        "      self.current_learning_rate = self.learning_rate * \\\n",
        "          (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "  def update_params(self, layer):\n",
        "    if not hasattr(layer, 'wight_cache'):\n",
        "      layer.weight_cache = np.zeros_like(layer.weights)\n",
        "    if hasattr(layer, 'biases'):\n",
        "      layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "    layer.weight_cache = self.rho * layer.weight_cache + \\\n",
        "        (1 - self.rho) * layer.dweights**2\n",
        "    if hasattr(layer, 'biases'):\n",
        "      layer.bias_cache = self.rho * layer.bias_cache + \\\n",
        "          (1 - self.rho) * layer.dbiases**2\n",
        "\n",
        "    layer.weights += -self.current_learning_rate * \\\n",
        "                    layer.dweights / \\\n",
        "                    (np.sqrt(layer.weight_cache) + self.epsilon)\n",
        "    if hasattr(layer, 'biases'):\n",
        "      layer.biases += -self.current_learning_rate * \\\n",
        "                      layer.dbiases / \\\n",
        "                      (np.sqrt(layer.bias_cache) + self.epsilon)\n",
        "    return layer\n",
        "\n",
        "  def post_update_params(self):\n",
        "    self.iterations += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hjZkcK5B96jb"
      },
      "outputs": [],
      "source": [
        "class LossBinaryCrossEntropy():\n",
        "  def forward(self, y_pred, y_true):\n",
        "    y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "\n",
        "    sample_losses = -(y_true * np.log(y_pred_clipped) + (1 - y_true) * np.log(1 - y_pred_clipped))\n",
        "    #sample_losses = np.mean(sample_losses, axis=1)\n",
        "\n",
        "    return sample_losses\n",
        "\n",
        "  def backward(self, dvalues, y_true):\n",
        "    samples = len(dvalues)\n",
        "    outputs = len(dvalues)\n",
        "\n",
        "    clipped_dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)\n",
        "\n",
        "    dinputs = -(y_true / clipped_dvalues - (1 - y_true)\n",
        "                     /(1 - clipped_dvalues))/outputs\n",
        "\n",
        "    return dinputs / samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "kEnCO7G4xxZB"
      },
      "outputs": [],
      "source": [
        "class Embedding:\n",
        "  def __init__(self, vocab_size, sentence_len, latent_dim):\n",
        "    self.vocab_size = vocab_size\n",
        "    self.sentence_len = sentence_len\n",
        "    self.layer1 = Dense(vocab_size, latent_dim, True)\n",
        "    self.activationLayer1 = Activation_ReLU()\n",
        "    self.layer2 = Dense(latent_dim, vocab_size, True)\n",
        "    self.activationLayer2 = Activation_Sigmoid()\n",
        "    self.optimizer = Optimizer_Adam(learning_rate=0.0001)#Rmsprop(learning_rate=0.0001)\n",
        "    self.trainableLayers = [self.layer1, self.layer2]\n",
        "    self.lossFunction = LossBinaryCrossEntropy()\n",
        "\n",
        "  def forward(self, input):\n",
        "    self.layer1.forward(input)\n",
        "    self.activationLayer1.forward(self.layer1.output)\n",
        "    self.layer2.forward(self.activationLayer1.output)\n",
        "    self.activationLayer2.forward(self.layer2.output)\n",
        "    return self.activationLayer2.output\n",
        "\n",
        "  def backward(self, loss):\n",
        "    self.activationLayer2.backward(loss)\n",
        "    self.layer2.backward(self.activationLayer2.dinputs)\n",
        "    self.activationLayer1.backward(self.layer2.dinputs)\n",
        "    self.layer1.backward(self.activationLayer1.dinputs)\n",
        "\n",
        "  def generateOneHotVector(self, input):\n",
        "    oneHotVectors = []\n",
        "    for position in input:\n",
        "      oneHot = np.zeros(self.vocab_size)\n",
        "      oneHot[position] = 1\n",
        "      oneHotVectors.append(oneHot)\n",
        "    return oneHotVectors\n",
        "\n",
        "  def get_SumContext(self, vector):\n",
        "    y_labels=[]\n",
        "    for i in range(len(vector)):\n",
        "      contexts=[]\n",
        "      if i == 0:\n",
        "        contexts = vector[i+1:i+3]\n",
        "      else:\n",
        "        min=i-2\n",
        "        if min<0:\n",
        "          min=0\n",
        "        if len(vector[i+1:i+3])!=0 and len(vector[min:i])!=0:\n",
        "          contexts = np.concatenate((vector[min:i], vector[i+1:i+3]))\n",
        "        elif len(vector[i+1:i+3])==0:\n",
        "          contexts = vector[min:i]\n",
        "      context=[]\n",
        "      for array in contexts:\n",
        "        if len(context)==0:\n",
        "          context = array\n",
        "        else:\n",
        "          context = context + array\n",
        "      y_labels.append(context)\n",
        "    return y_labels\n",
        "\n",
        "  def train(self, input, epoch):\n",
        "    for i in range(epoch):\n",
        "      lossByEpoch=0\n",
        "      for sentence in input:\n",
        "        lossBySentence=0\n",
        "        x_labels= self.generateOneHotVector(sentence)\n",
        "        y_labels = self.get_SumContext(vector=x_labels)\n",
        "        loss=np.zeros((1, 13))\n",
        "        for i in range(len(x_labels)):\n",
        "          prediction = self.forward(y_labels[i])\n",
        "          #print(\"prediction: \", prediction)\n",
        "          #print(f\"waited {i}: {x_labels[i]}\")\n",
        "          loss = loss + (prediction - x_labels[i])#self.lossFunction.forward(prediction,x_labels[i])\n",
        "          self.backward(prediction - x_labels[i])\n",
        "          self.optimizer.pre_update_params()\n",
        "          for layer in self.trainableLayers:\n",
        "            self.optimizer.update_params(layer)\n",
        "          self.optimizer.post_update_params()\n",
        "        lossBySentence = loss/len(x_labels)#np.mean(np.sqrt(loss*loss))\n",
        "        lossByEpoch+=np.mean(lossBySentence)#/len(x_labels)\n",
        "      print(\"Loss: \", lossByEpoch/len(input))\n",
        "\n",
        "  def getEmbedding(self, input):\n",
        "    oneHotVectors = self.generateOneHotVector(input)\n",
        "    embeddings = []\n",
        "    for oneHotVector in oneHotVectors:\n",
        "      self.layer1.forward(oneHotVector)\n",
        "      self.activationLayer1.forward(self.layer1.output)\n",
        "      embeddings.append(self.activationLayer1.output)\n",
        "    return embeddings\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtAY5aM26F7z"
      },
      "source": [
        "Manejo de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yez6tYvi6IRa",
        "outputId": "6c2c4537-cdb9-4d19-cf49-f9c7dc84e503"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the king of England is a man\n"
          ]
        }
      ],
      "source": [
        "# Leer set de entrenamiento\n",
        "filename = './english-spanish.pkl'\n",
        "\n",
        "#dataset = load(open(filename, 'rb'))\n",
        "#print(dataset[120000,0])\n",
        "#print(dataset[120000,1])\n",
        "dataset = np.array([np.array([\"el rey de Inglaterra es un hombre\",\"the king of England is a man\"]),\n",
        "                    np.array([\"la reina de Inglaterra es una mujer\",\"the queen of England is a woman\"]),\n",
        "                    np.array([\"Carlos es un rey\",\"Carlos is a king\"]),\n",
        "                    np.array([\"Andrea es una reina\",\"Andrea is a queen\"]),\n",
        "                    np.array([\"Carlos es un hombre\",\"Carlos is a man\"]),\n",
        "                    np.array([\"Andrea es una mujer\",\"Andrea is a woman\"])\n",
        "                    ])\n",
        "print(dataset[0,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EAJkCaC6M8k",
        "outputId": "ae43e7aa-03e0-4d6b-e414-13fbd49452de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['el', 'rey', 'de', 'Inglaterra', 'es', 'un', 'hombre']\n",
            "['the', 'king', 'of', 'England', 'is', 'a', 'man']\n"
          ]
        }
      ],
      "source": [
        "# Crear \"tokens\"\n",
        "source_tokens = []\n",
        "for sentence in dataset[:,0]:\n",
        "  source_tokens.append(sentence.split(' '))\n",
        "print(source_tokens[0])\n",
        "\n",
        "target_tokens = []\n",
        "for sentence in dataset[:,1]:\n",
        "  target_tokens.append(sentence.split(' '))\n",
        "print(target_tokens[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "vkYe94YR6RKW"
      },
      "outputs": [],
      "source": [
        "def build_token_dict(token_list):\n",
        "  token_dict = {\n",
        "  #    '<PAD>': 0,\n",
        "  #    '<START>': 1,\n",
        "  #    '<END>': 2\n",
        "  }\n",
        "  for tokens in token_list:\n",
        "    for token in tokens:\n",
        "      if token not in token_dict:\n",
        "        token_dict[token] = len(token_dict)\n",
        "  return token_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEo9slak6VVE",
        "outputId": "a95be083-4135-4228-8473-04febde082ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'el': 0, 'rey': 1, 'de': 2, 'Inglaterra': 3, 'es': 4, 'un': 5, 'hombre': 6, 'la': 7, 'reina': 8, 'una': 9, 'mujer': 10, 'Carlos': 11, 'Andrea': 12}\n",
            "{'the': 0, 'king': 1, 'of': 2, 'England': 3, 'is': 4, 'a': 5, 'man': 6, 'queen': 7, 'woman': 8, 'Carlos': 9, 'Andrea': 10}\n",
            "{0: 'the', 1: 'king', 2: 'of', 3: 'England', 4: 'is', 5: 'a', 6: 'man', 7: 'queen', 8: 'woman', 9: 'Carlos', 10: 'Andrea'}\n"
          ]
        }
      ],
      "source": [
        "source_token_dict = build_token_dict(source_tokens)\n",
        "source_token_dict_inv = {v:k for k,v in source_token_dict.items()}\n",
        "target_token_dict = build_token_dict(target_tokens)\n",
        "target_token_dict_inv = {v:k for k,v in target_token_dict.items()}\n",
        "\n",
        "print(source_token_dict)\n",
        "print(target_token_dict)\n",
        "print(target_token_dict_inv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "tjZq7UVG6YuO"
      },
      "outputs": [],
      "source": [
        "# Agregar start, end y pad a cada frase del set de entrenamiento\n",
        "#encoder_tokens = [['<START>'] + tokens + ['<END>'] for tokens in source_tokens]\n",
        "encoder_tokens = [tokens for tokens in source_tokens]\n",
        "#decoder_tokens = [['<START>'] + tokens + ['<END>'] for tokens in target_tokens]\n",
        "decoder_tokens = [tokens for tokens in target_tokens]\n",
        "#output_tokens = [tokens + ['<END>'] for tokens in target_tokens]\n",
        "output_tokens = [tokens for tokens in target_tokens]\n",
        "\n",
        "source_max_len = max(map(len, encoder_tokens))\n",
        "target_max_len = max(map(len, decoder_tokens))\n",
        "\n",
        "#encoder_tokens = [tokens + ['<PAD>']*(source_max_len-len(tokens)) for tokens in encoder_tokens]\n",
        "#decoder_tokens = [tokens + ['<PAD>']*(target_max_len-len(tokens)) for tokens in decoder_tokens]\n",
        "#output_tokens = [tokens + ['<PAD>']*(target_max_len-len(tokens)) for tokens in output_tokens ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcMGqShN6cLZ",
        "outputId": "007ce141-990c-4dbe-c6c1-89c8fa8042c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['la', 'reina', 'de', 'Inglaterra', 'es', 'una', 'mujer']\n"
          ]
        }
      ],
      "source": [
        "print(encoder_tokens[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "9KS6KDyq6h14"
      },
      "outputs": [],
      "source": [
        "encoder_input = [list(map(lambda x: source_token_dict[x], tokens)) for tokens in encoder_tokens]\n",
        "decoder_input = [list(map(lambda x: target_token_dict[x], tokens)) for tokens in decoder_tokens]\n",
        "output_decoded = [list(map(lambda x: [target_token_dict[x]], tokens)) for tokens in output_tokens]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AvSmG0ZER9V",
        "outputId": "0ee77930-a124-4ee6-ed9f-e371bd7f5590"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7, 8, 2, 3, 4, 9, 10]\n"
          ]
        }
      ],
      "source": [
        "print(encoder_input[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Re0auX7F-5kA"
      },
      "source": [
        "Entrenamos el Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mof4mwjW6i7X",
        "outputId": "10dde699-1665-4f77-b365-445680b39778"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13\n",
            "7\n",
            "6\n"
          ]
        }
      ],
      "source": [
        "vocab_size = len(source_token_dict)\n",
        "print(vocab_size)\n",
        "print(target_max_len)\n",
        "print(len(decoder_input))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 314,
      "metadata": {
        "id": "nDGZFqVG_JqH"
      },
      "outputs": [],
      "source": [
        "embedding = Embedding(vocab_size, source_max_len, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 315,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smPhBX2g_dbt",
        "outputId": "1bd7581c-c173-4c79-aaa9-e2f9773827eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss:  0.42207083686530483\n",
            "Loss:  0.42135375746482734\n",
            "Loss:  0.42062935717497535\n",
            "Loss:  0.4198763703425623\n",
            "Loss:  0.41911735241539594\n",
            "Loss:  0.4183571247526883\n",
            "Loss:  0.4175949777679373\n",
            "Loss:  0.4168303106324935\n",
            "Loss:  0.41606266351222265\n",
            "Loss:  0.4152916523944572\n",
            "Loss:  0.4145134760608751\n",
            "Loss:  0.4137247979339838\n",
            "Loss:  0.41292959785146444\n",
            "Loss:  0.4121184046168833\n",
            "Loss:  0.4112493125851284\n",
            "Loss:  0.410342213993169\n",
            "Loss:  0.40941723831313376\n",
            "Loss:  0.40845347002098314\n",
            "Loss:  0.40747681235238753\n",
            "Loss:  0.40648752956755546\n",
            "Loss:  0.4054847990462835\n",
            "Loss:  0.40445819131390626\n",
            "Loss:  0.40340827061549994\n",
            "Loss:  0.40234223590785473\n",
            "Loss:  0.4012597733750176\n",
            "Loss:  0.40016047739508825\n",
            "Loss:  0.39904401623214625\n",
            "Loss:  0.3979093591715251\n",
            "Loss:  0.39675214398102915\n",
            "Loss:  0.3955747306179422\n",
            "Loss:  0.39436803126775266\n",
            "Loss:  0.39312744500548275\n",
            "Loss:  0.39186434922852736\n",
            "Loss:  0.39057700470022666\n",
            "Loss:  0.3892560199646408\n",
            "Loss:  0.3878914236465598\n",
            "Loss:  0.38649477433573765\n",
            "Loss:  0.38507092991385306\n",
            "Loss:  0.383619360789534\n",
            "Loss:  0.38213976368928976\n",
            "Loss:  0.3806320545101052\n",
            "Loss:  0.3790963183376875\n",
            "Loss:  0.3775316507402611\n",
            "Loss:  0.37592959062074144\n",
            "Loss:  0.37428072093090475\n",
            "Loss:  0.37258963558588903\n",
            "Loss:  0.3708665218396643\n",
            "Loss:  0.36911255961801753\n",
            "Loss:  0.36732835715182227\n",
            "Loss:  0.3655146284322958\n",
            "Loss:  0.363672186420801\n",
            "Loss:  0.3618019200007621\n",
            "Loss:  0.35990477538564014\n",
            "Loss:  0.35798174104005903\n",
            "Loss:  0.3560338352527002\n",
            "Loss:  0.3540620958525295\n",
            "Loss:  0.3520675717213686\n",
            "Loss:  0.3500513158324237\n",
            "Loss:  0.34800513847029\n",
            "Loss:  0.34592867742391253\n",
            "Loss:  0.34383209154698496\n",
            "Loss:  0.3417168277328657\n",
            "Loss:  0.33958393042771745\n",
            "Loss:  0.3374344184243576\n",
            "Loss:  0.33526929745844697\n",
            "Loss:  0.3330895585959097\n",
            "Loss:  0.3308961765833186\n",
            "Loss:  0.32869010861439496\n",
            "Loss:  0.32645808552737\n",
            "Loss:  0.3242035256657525\n",
            "Loss:  0.32193705364109604\n",
            "Loss:  0.3196608022511344\n",
            "Loss:  0.31737569205543265\n",
            "Loss:  0.3150825808985238\n",
            "Loss:  0.312782309743104\n",
            "Loss:  0.3104757034745101\n",
            "Loss:  0.3081635701565378\n",
            "Loss:  0.3058467004498858\n",
            "Loss:  0.30352586720570573\n",
            "Loss:  0.3012018251931429\n",
            "Loss:  0.2988753109289086\n",
            "Loss:  0.29654704258525494\n",
            "Loss:  0.2942177199587469\n",
            "Loss:  0.29188802448659096\n",
            "Loss:  0.2895586193004895\n",
            "Loss:  0.28723014931038654\n",
            "Loss:  0.28490324131226147\n",
            "Loss:  0.28257850411549135\n",
            "Loss:  0.2802565286863413\n",
            "Loss:  0.2779378883049302\n",
            "Loss:  0.2756231387336312\n",
            "Loss:  0.27331281839533\n",
            "Loss:  0.27100744856032205\n",
            "Loss:  0.268707533540898\n",
            "Loss:  0.26641356089287815\n",
            "Loss:  0.26412600162350713\n",
            "Loss:  0.26184531040523606\n",
            "Loss:  0.2595719257950056\n",
            "Loss:  0.25730627045869975\n",
            "Loss:  0.2550487514004846\n",
            "Loss:  0.2527997601967699\n",
            "Loss:  0.2505596732345523\n",
            "Loss:  0.24832885195390086\n",
            "Loss:  0.2461076430943545\n",
            "Loss:  0.24389637894499283\n",
            "Loss:  0.24169537759794146\n",
            "Loss:  0.23950494320506469\n",
            "Loss:  0.2373253662375898\n",
            "Loss:  0.23515692374840216\n",
            "Loss:  0.23299987963674107\n",
            "Loss:  0.2308544849150189\n",
            "Loss:  0.22872097797748217\n",
            "Loss:  0.22659958487042742\n",
            "Loss:  0.2244905195636797\n",
            "Loss:  0.2223939842230417\n",
            "Loss:  0.22031016948341856\n",
            "Loss:  0.2182392547223251\n",
            "Loss:  0.21618140833348276\n",
            "Loss:  0.2141367880002167\n",
            "Loss:  0.21210554096836864\n",
            "Loss:  0.21008780431844457\n",
            "Loss:  0.20808370523672393\n",
            "Loss:  0.20609336128506336\n",
            "Loss:  0.20411688066913683\n",
            "Loss:  0.2021543625048622\n",
            "Loss:  0.20020589708277356\n",
            "Loss:  0.19827156613010977\n",
            "Loss:  0.19635144307039887\n",
            "Loss:  0.19444559328033026\n",
            "Loss:  0.19255407434371638\n",
            "Loss:  0.19067693630235882\n",
            "Loss:  0.18881422190364353\n",
            "Loss:  0.18696596684470349\n",
            "Loss:  0.18513220001299757\n",
            "Loss:  0.18331294372316695\n",
            "Loss:  0.1815082139500411\n",
            "Loss:  0.1797180205576789\n",
            "Loss:  0.177942367524339\n",
            "Loss:  0.17618125316328812\n",
            "Loss:  0.17443467033936258\n",
            "Loss:  0.17270260668121443\n",
            "Loss:  0.17098504478917786\n",
            "Loss:  0.16928196243870733\n",
            "Loss:  0.1675933327793437\n",
            "Loss:  0.165919124529177\n",
            "Loss:  0.16425930216478243\n",
            "Loss:  0.16261382610661365\n",
            "Loss:  0.16098265289984745\n",
            "Loss:  0.15936573539068022\n",
            "Loss:  0.1577630228980837\n",
            "Loss:  0.156174461381036\n",
            "Loss:  0.15459999360124824\n",
            "Loss:  0.1530395592814149\n",
            "Loss:  0.15149309525902097\n",
            "Loss:  0.1499605356357436\n",
            "Loss:  0.14844181192249284\n",
            "Loss:  0.1469368531801382\n",
            "Loss:  0.14544558615597356\n",
            "Loss:  0.14396793541597616\n",
            "Loss:  0.14250382347291904\n",
            "Loss:  0.1410531709103994\n",
            "Loss:  0.13961589650284836\n",
            "Loss:  0.13819191733158936\n",
            "Loss:  0.13678114889701629\n",
            "Loss:  0.13538350522696255\n",
            "Loss:  0.1339988989813352\n",
            "Loss:  0.1326272415530894\n",
            "Loss:  0.1312684431656195\n",
            "Loss:  0.12992241296664467\n",
            "Loss:  0.12858905911866597\n",
            "Loss:  0.12726828888607553\n",
            "Loss:  0.1259600087189953\n",
            "Loss:  0.12466412433392605\n",
            "Loss:  0.12338054079128498\n",
            "Loss:  0.1221091625699121\n",
            "Loss:  0.12084989363862418\n",
            "Loss:  0.11960263752489464\n",
            "Loss:  0.1183672973807381\n",
            "Loss:  0.11714377604587693\n",
            "Loss:  0.11593197610826635\n",
            "Loss:  0.11473179996205489\n",
            "Loss:  0.11354314986305468\n",
            "Loss:  0.11236592798179583\n",
            "Loss:  0.11120003645423865\n",
            "Loss:  0.11004537743021459\n",
            "Loss:  0.1089018531196679\n",
            "Loss:  0.10776936583676718\n",
            "Loss:  0.10664781804195493\n",
            "Loss:  0.10553711238200243\n",
            "Loss:  0.10443715172813622\n",
            "Loss:  0.10334783921229955\n",
            "Loss:  0.10226907826161274\n",
            "Loss:  0.10120077263109366\n",
            "Loss:  0.1001428264346987\n",
            "Loss:  0.09909514417474326\n",
            "Loss:  0.09805763076975833\n",
            "Loss:  0.09703019158084085\n",
            "Loss:  0.0960127324365501\n",
            "Loss:  0.09500515965640559\n",
            "Loss:  0.09400738007303673\n",
            "Loss:  0.09301930105303474\n",
            "Loss:  0.0920408305165568\n",
            "Loss:  0.09107187695572842\n",
            "Loss:  0.09011234945189188\n",
            "Loss:  0.08916215769174417\n",
            "Loss:  0.08822121198240895\n",
            "Loss:  0.08728942326548454\n",
            "Loss:  0.08636670313010825\n",
            "Loss:  0.08545296382507774\n",
            "Loss:  0.08454811827006709\n",
            "Loss:  0.0836520800659748\n",
            "Loss:  0.08276476350444013\n",
            "Loss:  0.08188608357656235\n",
            "Loss:  0.08101595598085616\n",
            "Loss:  0.08015429713047663\n",
            "Loss:  0.0793010241597445\n",
            "Loss:  0.07845605493000209\n",
            "Loss:  0.07761930803482964\n",
            "Loss:  0.07679070280464963\n",
            "Loss:  0.07597015931074706\n",
            "Loss:  0.07515759836873152\n",
            "Loss:  0.0743529415414662\n",
            "Loss:  0.07355611114148858\n",
            "Loss:  0.07276703023294635\n",
            "Loss:  0.07198562263307064\n",
            "Loss:  0.07121181291320884\n",
            "Loss:  0.07044552639943731\n",
            "Loss:  0.06968668917277489\n",
            "Loss:  0.06893522806901566\n",
            "Loss:  0.06819107067819985\n",
            "Loss:  0.06745414534374097\n",
            "Loss:  0.06672438116122559\n",
            "Loss:  0.06600170797690254\n",
            "Loss:  0.06528605638587731\n",
            "Loss:  0.06457735773002622\n",
            "Loss:  0.06387554409564518\n",
            "Loss:  0.06318054831084634\n",
            "Loss:  0.06249230394271638\n",
            "Loss:  0.06181074529424833\n",
            "Loss:  0.06113580740105946\n",
            "Loss:  0.06046742602790631\n",
            "Loss:  0.05980553766500799\n",
            "Loss:  0.059150079524188104\n",
            "Loss:  0.058500989534845005\n",
            "Loss:  0.05785820633976014\n",
            "Loss:  0.05722166929075309\n",
            "Loss:  0.056591318444192094\n",
            "Loss:  0.05596709455636767\n",
            "Loss:  0.05534893907873754\n",
            "Loss:  0.05473679415304952\n",
            "Loss:  0.054130602606349575\n",
            "Loss:  0.053530307945881116\n",
            "Loss:  0.052935854353881956\n",
            "Loss:  0.052347186682284365\n",
            "Loss:  0.05176425044732367\n",
            "Loss:  0.05118699182406023\n",
            "Loss:  0.05061535764081974\n",
            "Loss:  0.05004929537355616\n",
            "Loss:  0.049488753140140895\n",
            "Loss:  0.04893367969458296\n",
            "Loss:  0.04838402442118248\n",
            "Loss:  0.04783973732862156\n",
            "Loss:  0.04730076904399511\n",
            "Loss:  0.046767070806784716\n",
            "Loss:  0.046238594462777366\n",
            "Loss:  0.04571529245793212\n",
            "Loss:  0.045197117832196015\n",
            "Loss:  0.044684024213271584\n",
            "Loss:  0.04417596581033718\n",
            "Loss:  0.0436728974077221\n",
            "Loss:  0.04317477435853725\n",
            "Loss:  0.0426815525782628\n",
            "Loss:  0.04219318853829373\n",
            "Loss:  0.04170963925944385\n",
            "Loss:  0.041230862305409746\n",
            "Loss:  0.0407568157761942\n",
            "Loss:  0.040287458301490366\n",
            "Loss:  0.03982274903402675\n",
            "Loss:  0.03936264764287337\n",
            "Loss:  0.038907114306709366\n",
            "Loss:  0.0384561097070521\n",
            "Loss:  0.03800959502144808\n",
            "Loss:  0.03756753191662556\n",
            "Loss:  0.03712988254160903\n",
            "Loss:  0.03669660952079551\n",
            "Loss:  0.03626767594699292\n",
            "Loss:  0.0358430453744201\n",
            "Loss:  0.035422681811668966\n",
            "Loss:  0.03500654971462871\n",
            "Loss:  0.03459461397937205\n",
            "Loss:  0.03418683993500346\n",
            "Loss:  0.03378319333647011\n",
            "Loss:  0.03338364035733518\n",
            "Loss:  0.032988147582513914\n",
            "Loss:  0.0325966820009728\n",
            "Loss:  0.032209210998392364\n",
            "Loss:  0.03182570234979354\n",
            "Loss:  0.03144612421212852\n",
            "Loss:  0.031070445116836567\n",
            "Loss:  0.03069863396236523\n",
            "Loss:  0.030330660006657618\n",
            "Loss:  0.02996649285960669\n",
            "Loss:  0.029606102475477295\n",
            "Loss:  0.02924945914529652\n",
            "Loss:  0.028896533489213953\n",
            "Loss:  0.02854729644883219\n",
            "Loss:  0.028201719279509197\n",
            "Loss:  0.027859773542633456\n",
            "Loss:  0.027521431097873403\n",
            "Loss:  0.02718666409540257\n",
            "Loss:  0.02685544496810202\n",
            "Loss:  0.026527746423741613\n",
            "Loss:  0.026203541437142545\n",
            "Loss:  0.025882803242322876\n",
            "Loss:  0.02556550532462865\n",
            "Loss:  0.025251621412853503\n",
            "Loss:  0.02494112547134976\n",
            "Loss:  0.02463399169213444\n",
            "Loss:  0.024330194486994323\n",
            "Loss:  0.024029708479594392\n",
            "Loss:  0.02373250849759491\n",
            "Loss:  0.023438569564782904\n",
            "Loss:  0.023147866893224203\n",
            "Loss:  0.022860375875443522\n",
            "Loss:  0.022576072076640575\n",
            "Loss:  0.022294931226951125\n",
            "Loss:  0.022016929213762734\n",
            "Loss:  0.02174204207409633\n",
            "Loss:  0.021470245987065057\n",
            "Loss:  0.02120151726642348\n",
            "Loss:  0.02093583235322082\n",
            "Loss:  0.020673167808573128\n",
            "Loss:  0.020413500306569927\n",
            "Loss:  0.020156806627331936\n",
            "Loss:  0.01990306365023726\n",
            "Loss:  0.01965224834733387\n",
            "Loss:  0.01940433777695696\n",
            "Loss:  0.01915930907756973\n",
            "Loss:  0.018917139461847057\n",
            "Loss:  0.018677806211020262\n",
            "Loss:  0.018441286669502416\n",
            "Loss:  0.018207558239811836\n",
            "Loss:  0.017976598377811678\n",
            "Loss:  0.017748384588281874\n",
            "Loss:  0.017522894420839173\n",
            "Loss:  0.01730010546621905\n",
            "Loss:  0.01707999535293214\n",
            "Loss:  0.016862541744305553\n",
            "Loss:  0.01664772233591829\n",
            "Loss:  0.016435514853437014\n",
            "Loss:  0.016225897050856808\n",
            "Loss:  0.01601884670914935\n",
            "Loss:  0.01581434163531815\n",
            "Loss:  0.015612359661858824\n",
            "Loss:  0.015412878646620038\n",
            "Loss:  0.015215876473058136\n",
            "Loss:  0.015021331050876918\n",
            "Loss:  0.014829220317042091\n",
            "Loss:  0.014639522237157733\n",
            "Loss:  0.014452214807190894\n",
            "Loss:  0.014267276055528834\n",
            "Loss:  0.014084684045351964\n",
            "Loss:  0.01390441687730467\n",
            "Loss:  0.013726452692445212\n",
            "Loss:  0.01355076967545484\n",
            "Loss:  0.013377346058085729\n",
            "Loss:  0.013206160122827134\n",
            "Loss:  0.013037190206767668\n",
            "Loss:  0.012870414705632896\n",
            "Loss:  0.012705812077975442\n",
            "Loss:  0.012543360849495816\n",
            "Loss:  0.012383039617471525\n",
            "Loss:  0.012224827055271925\n",
            "Loss:  0.012068701916936778\n",
            "Loss:  0.0119146430417957\n",
            "Loss:  0.01176262935910649\n",
            "Loss:  0.011612639892690194\n",
            "Loss:  0.011464653765541248\n",
            "Loss:  0.011318650204391175\n",
            "Loss:  0.011174608544205012\n",
            "Loss:  0.011032508232590067\n",
            "Loss:  0.010892328834097445\n",
            "Loss:  0.010754050034397744\n",
            "Loss:  0.01061765164431283\n",
            "Loss:  0.010483113603687278\n",
            "Loss:  0.010350415985084026\n",
            "Loss:  0.010219538997290048\n",
            "Loss:  0.010090462988619335\n",
            "Loss:  0.009963168450002391\n",
            "Loss:  0.00983763601785234\n",
            "Loss:  0.009713846476700141\n",
            "Loss:  0.00959178076159258\n",
            "Loss:  0.009471419960248592\n",
            "Loss:  0.009352745314971178\n",
            "Loss:  0.009235738224313882\n",
            "Loss:  0.009120380244502303\n",
            "Loss:  0.009006653090612354\n",
            "Loss:  0.008894538637509134\n",
            "Loss:  0.008784018920551013\n",
            "Loss:  0.008675076136064616\n",
            "Loss:  0.008567692641598313\n",
            "Loss:  0.008461850955961763\n",
            "Loss:  0.008357533759060466\n",
            "Loss:  0.008254723891535065\n",
            "Loss:  0.008153404354215046\n",
            "Loss:  0.008053558307397651\n",
            "Loss:  0.007955169069962797\n",
            "Loss:  0.007858220118334577\n",
            "Loss:  0.007762695085300932\n",
            "Loss:  0.007668577758702132\n",
            "Loss:  0.0075758520799991185\n",
            "Loss:  0.00748450214273243\n",
            "Loss:  0.007394512190882317\n",
            "Loss:  0.00730586661714001\n",
            "Loss:  0.00721854996110033\n",
            "Loss:  0.0071325469073848236\n",
            "Loss:  0.007047842283704677\n",
            "Loss:  0.0069644210588722215\n",
            "Loss:  0.0068822683407689005\n",
            "Loss:  0.006801369374278169\n",
            "Loss:  0.006721709539190033\n",
            "Loss:  0.00664327434808501\n",
            "Loss:  0.006566049444203622\n",
            "Loss:  0.006490020599307931\n",
            "Loss:  0.006415173711541063\n",
            "Loss:  0.006341494803290365\n",
            "Loss:  0.0062689700190592295\n",
            "Loss:  0.006197585623352968\n",
            "Loss:  0.006127327998583245\n",
            "Loss:  0.006058183642995611\n",
            "Loss:  0.005990139168624456\n",
            "Loss:  0.005923181299279354\n",
            "Loss:  0.005857296868566879\n",
            "Loss:  0.0057924728179513\n",
            "Loss:  0.00572869619485791\n",
            "Loss:  0.005665954150822219\n",
            "Loss:  0.00560423393968823\n",
            "Loss:  0.00554352291585903\n",
            "Loss:  0.005483808532602278\n",
            "Loss:  0.005425078340413715\n",
            "Loss:  0.005367319985441095\n",
            "Loss:  0.00531052120797099\n",
            "Loss:  0.00525466984098107\n",
            "Loss:  0.00519975380875952\n",
            "Loss:  0.005145761125594083\n",
            "Loss:  0.0050926798945323155\n",
            "Loss:  0.005040498306214571\n",
            "Loss:  0.004989204637781364\n",
            "Loss:  0.004938787251856278\n",
            "Loss:  0.004889234595605447\n",
            "Loss:  0.0048405351998744356\n",
            "Loss:  0.004792677678403027\n",
            "Loss:  0.004745650727118421\n",
            "Loss:  0.004699443123507012\n",
            "Loss:  0.004654043726064339\n",
            "Loss:  0.00460944147382323\n",
            "Loss:  0.00456562538595915\n",
            "Loss:  0.004522584561472392\n",
            "Loss:  0.004480308178945532\n",
            "Loss:  0.004438785496375069\n",
            "Loss:  0.004398005851075536\n",
            "Loss:  0.004357958659654497\n",
            "Loss:  0.004318633418055893\n",
            "Loss:  0.004280019701669993\n",
            "Loss:  0.004242107165507096\n",
            "Loss:  0.00420488554443245\n",
            "Loss:  0.004168344653459266\n",
            "Loss:  0.0041324743880971\n",
            "Loss:  0.004097264724751942\n",
            "Loss:  0.004062705721174862\n",
            "Loss:  0.004028787516955688\n",
            "Loss:  0.00399550033405779\n",
            "Loss:  0.003962834477390522\n",
            "Loss:  0.003930780335415155\n",
            "Loss:  0.003899328380780654\n",
            "Loss:  0.0038684691709850727\n",
            "Loss:  0.0038381933490587218\n",
            "Loss:  0.0038084916442648205\n",
            "Loss:  0.003779354872814078\n",
            "Loss:  0.003750773938588603\n",
            "Loss:  0.0037227398338717083\n",
            "Loss:  0.0036952436400791793\n",
            "Loss:  0.003668276528488642\n",
            "Loss:  0.0036418297609628696\n",
            "Loss:  0.003615894690663561\n",
            "Loss:  0.00359046276275204\n",
            "Loss:  0.0035655255150733624\n",
            "Loss:  0.0035410745788205\n",
            "Loss:  0.0035171016791756358\n",
            "Loss:  0.003493598635925438\n",
            "Loss:  0.003470557364047437\n",
            "Loss:  0.0034479698742650804\n",
            "Loss:  0.0034258282735686826\n",
            "Loss:  0.0034041247657002525\n",
            "Loss:  0.0033828516516000197\n",
            "Loss:  0.0033620013298125344\n",
            "Loss:  0.0033415662968509976\n",
            "Loss:  0.003321539147517968\n",
            "Loss:  0.003301912575181299\n",
            "Loss:  0.0032826793720041013\n",
            "Loss:  0.003263832429127645\n",
            "Loss:  0.003245364736806654\n",
            "Loss:  0.0032272693844962587\n",
            "Loss:  0.003209539560890189\n",
            "Loss:  0.0031921685539100784\n",
            "Loss:  0.0031751497506454715\n",
            "Loss:  0.0031584766372451858\n",
            "Loss:  0.0031421427987595586\n",
            "Loss:  0.003126141918934557\n",
            "Loss:  0.0031104677799578683\n",
            "Loss:  0.003095114262157814\n",
            "Loss:  0.0030800753436557218\n",
            "Loss:  0.003065345099972761\n",
            "Loss:  0.003050917703591939\n",
            "Loss:  0.0030367874234767302\n",
            "Loss:  0.0030229486245472123\n",
            "Loss:  0.0030093957671149378\n",
            "Loss:  0.0029961234062779916\n",
            "Loss:  0.0029831261912775385\n",
            "Loss:  0.002970398864817265\n",
            "Loss:  0.0029579362623472437\n",
            "Loss:  0.002945733311313588\n",
            "Loss:  0.0029337850303756234\n",
            "Loss:  0.0029220865285920477\n",
            "Loss:  0.0029106330045776583\n",
            "Loss:  0.0028994197456323257\n",
            "Loss:  0.0028884421268437245\n",
            "Loss:  0.0028776956101654685\n",
            "Loss:  0.0028671757434723076\n",
            "Loss:  0.002856878159593823\n",
            "Loss:  0.0028467985753285394\n",
            "Loss:  0.0028369327904394764\n",
            "Loss:  0.002827276686633295\n",
            "Loss:  0.002817826226524093\n",
            "Loss:  0.002808577452583554\n",
            "Loss:  0.0027995264860788648\n",
            "Loss:  0.0027906695259998053\n",
            "Loss:  0.0027820028479763836\n",
            "Loss:  0.0027735228031884663\n",
            "Loss:  0.0027652258172685845\n",
            "Loss:  0.002757108389199225\n",
            "Loss:  0.00274916709020586\n",
            "Loss:  0.002741398562646874\n",
            "Loss:  0.0027337995189015266\n",
            "Loss:  0.002726366740257037\n",
            "Loss:  0.0027190970757957865\n",
            "Loss:  0.0027119874412837707\n",
            "Loss:  0.0027050348180610757\n",
            "Loss:  0.0026982362519354336\n",
            "Loss:  0.002691588852079662\n",
            "Loss:  0.0026850897899337906\n",
            "Loss:  0.0026787362981126237\n",
            "Loss:  0.0026725256693195545\n",
            "Loss:  0.002666455255267234\n",
            "Loss:  0.0026605224656057747\n",
            "Loss:  0.002654724766859197\n",
            "Loss:  0.002649059681370348\n",
            "Loss:  0.00264352478625518\n",
            "Loss:  0.00263811771236681\n",
            "Loss:  0.0026328361432695446\n",
            "Loss:  0.002627677814223546\n",
            "Loss:  0.002622640511180624\n",
            "Loss:  0.002617722069790993\n",
            "Loss:  0.002612920374421871\n",
            "Loss:  0.0026082333571879417\n",
            "Loss:  0.0026036589969938902\n",
            "Loss:  0.0025991953185893466\n",
            "Loss:  0.0025948403916363715\n",
            "Loss:  0.0025905923297897715\n",
            "Loss:  0.002586449289790181\n",
            "Loss:  0.0025824094705702368\n",
            "Loss:  0.0025784711123739367\n",
            "Loss:  0.0025746324958890172\n",
            "Loss:  0.002570891941392771\n",
            "Loss:  0.0025672478079110423\n",
            "Loss:  0.0025636984923906995\n",
            "Loss:  0.0025602424288853156\n",
            "Loss:  0.0025568780877542707\n",
            "Loss:  0.002553603974875151\n",
            "Loss:  0.002550418630869479\n",
            "Loss:  0.002547320630341465\n",
            "Loss:  0.002544308581130303\n",
            "Loss:  0.0025413811235752014\n",
            "Loss:  0.002538536929793729\n",
            "Loss:  0.002535774702972886\n",
            "Loss:  0.0025330931766731866\n",
            "Loss:  0.002530491114145314\n",
            "Loss:  0.002527967307659517\n",
            "Loss:  0.002525520577847402\n",
            "Loss:  0.0025231497730561618\n",
            "Loss:  0.002520853768715034\n",
            "Loss:  0.002518631466713849\n",
            "Loss:  0.0025164817947934425\n",
            "Loss:  0.0025144037059480332\n",
            "Loss:  0.002512396177839205\n",
            "Loss:  0.0025104582122213027\n",
            "Loss:  0.00250858883437834\n",
            "Loss:  0.002506787092571934\n",
            "Loss:  0.0025050520575003836\n",
            "Loss:  0.002503382821768643\n",
            "Loss:  0.002501778499368924\n",
            "Loss:  0.0025002382251718288\n",
            "Loss:  0.0024987611544280493\n",
            "Loss:  0.002497346462280047\n",
            "Loss:  0.002495993343283953\n",
            "Loss:  0.002494701010941308\n",
            "Loss:  0.002493468697240497\n",
            "Loss:  0.002492295652207874\n",
            "Loss:  0.002491181143468149\n",
            "Loss:  0.0024901244558140022\n",
            "Loss:  0.0024891248907849615\n",
            "Loss:  0.0024881817662550446\n",
            "Loss:  0.002487294416029162\n",
            "Loss:  0.0024864621894482002\n",
            "Loss:  0.0024856844510025705\n",
            "Loss:  0.0024849605799539113\n",
            "Loss:  0.002484289969965039\n",
            "Loss:  0.0024836720287378764\n",
            "Loss:  0.0024831061776592737\n",
            "Loss:  0.0024825918514544084\n",
            "Loss:  0.002482128497847794\n",
            "Loss:  0.002481715577231726\n",
            "Loss:  0.0024813525623419035\n",
            "Loss:  0.0024810389379402446\n",
            "Loss:  0.002480774200504746\n",
            "Loss:  0.002480557857925983\n",
            "Loss:  0.002480389429210559\n",
            "Loss:  0.002480268444190956\n",
            "Loss:  0.002480194443241958\n",
            "Loss:  0.0024801669770033855\n",
            "Loss:  0.002480185606108911\n",
            "Loss:  0.002480249900921146\n",
            "Loss:  0.002480359441272504\n",
            "Loss:  0.002480513816212027\n",
            "Loss:  0.0024807126237577806\n",
            "Loss:  0.0024809554706550635\n",
            "Loss:  0.002481241972139873\n",
            "Loss:  0.0024815717517079118\n",
            "Loss:  0.0024819444408888066\n",
            "Loss:  0.0024823596790254365\n",
            "Loss:  0.002482817113058407\n",
            "Loss:  0.0024833163973153045\n",
            "Loss:  0.0024838571933050503\n",
            "Loss:  0.002484439169516756\n",
            "Loss:  0.002485062001223387\n",
            "Loss:  0.002485725370289817\n",
            "Loss:  0.0024864289649855044\n",
            "Loss:  0.002487172479801453\n",
            "Loss:  0.002487955615271343\n",
            "Loss:  0.0024887780777970154\n",
            "Loss:  0.002489639579477964\n",
            "Loss:  0.0024905398379446624\n",
            "Loss:  0.0024914785761961885\n",
            "Loss:  0.002492455522441338\n",
            "Loss:  0.002493470409943612\n",
            "Loss:  0.002494522976869838\n",
            "Loss:  0.0024956129661425817\n",
            "Loss:  0.0024967401252956726\n",
            "Loss:  0.0024979042063335236\n",
            "Loss:  0.002499104965593665\n",
            "Loss:  0.002500342163612619\n",
            "Loss:  0.0025016155649948725\n",
            "Loss:  0.0025029249382851827\n",
            "Loss:  0.0025042700558438025\n",
            "Loss:  0.0025056506937247594\n",
            "Loss:  0.002507066631557146\n",
            "Loss:  0.002508517652429208\n",
            "Loss:  0.0025100035427752885\n",
            "Loss:  0.0025115240922653974\n",
            "Loss:  0.002513079093697783\n",
            "Loss:  0.0025146683428938\n",
            "Loss:  0.0025162916385954273\n",
            "Loss:  0.0025179487823654857\n",
            "Loss:  0.002519639578490135\n",
            "Loss:  0.0025213638338837936\n",
            "Loss:  0.00252312135799638\n",
            "Loss:  0.0025249119627229655\n",
            "Loss:  0.0025267354623156055\n",
            "Loss:  0.0025285916732973174\n",
            "Loss:  0.0025304804143782248\n",
            "Loss:  0.0025324015063737933\n",
            "Loss:  0.002534354772125154\n",
            "Loss:  0.0025363400364213514\n",
            "Loss:  0.0025383571259236255\n",
            "Loss:  0.0025404058690914524\n",
            "Loss:  0.0025424860961106572\n",
            "Loss:  0.0025445976388231107\n",
            "Loss:  0.002546740330658387\n",
            "Loss:  0.002548914006567113\n",
            "Loss:  0.0025511185029558903\n",
            "Loss:  0.0025533536576240203\n",
            "Loss:  0.002555619309701872\n",
            "Loss:  0.0025579152995905967\n",
            "Loss:  0.0025602414689035818\n",
            "Loss:  0.0025625976604093967\n",
            "Loss:  0.002564983717976103\n",
            "Loss:  0.002567399486517122\n",
            "Loss:  0.002569844811938475\n",
            "Loss:  0.0025723195410872262\n",
            "Loss:  0.0025748235217015266\n",
            "Loss:  0.002577356602361756\n",
            "Loss:  0.0025799186324430455\n",
            "Loss:  0.0025825094620690017\n",
            "Loss:  0.0025851289420666325\n",
            "Loss:  0.0025877769239225587\n",
            "Loss:  0.0025904532597401304\n",
            "Loss:  0.0025931578021979804\n",
            "Loss:  0.002595890404509563\n",
            "Loss:  0.0025986509203836747\n",
            "Loss:  0.0026014392039860642\n",
            "Loss:  0.0026042551099021947\n",
            "Loss:  0.0026070984931008886\n",
            "Loss:  0.002609969208898949\n",
            "Loss:  0.002612867112926781\n",
            "Loss:  0.0026157920610949867\n",
            "Loss:  0.0026187439095616987\n",
            "Loss:  0.002621722514701099\n",
            "Loss:  0.002624727733072431\n",
            "Loss:  0.002627759421390164\n",
            "Loss:  0.0026308174364949367\n",
            "Loss:  0.0026339016353250978\n",
            "Loss:  0.0026370118748893223\n",
            "Loss:  0.0026401480122398412\n",
            "Loss:  0.002643309904446461\n",
            "Loss:  0.0026464974085712526\n",
            "Loss:  0.0026497103816440592\n",
            "Loss:  0.00265294868063872\n",
            "Loss:  0.002656212162449875\n",
            "Loss:  0.0026595006838705\n",
            "Loss:  0.002662814101570137\n",
            "Loss:  0.002666152272073712\n",
            "Loss:  0.002669515051740897\n",
            "Loss:  0.002672902296746366\n",
            "Loss:  0.002676313863060277\n",
            "Loss:  0.0026797496064296777\n",
            "Loss:  0.002683209382360236\n",
            "Loss:  0.0026866930460986154\n",
            "Loss:  0.0026902004526155113\n",
            "Loss:  0.0026937314565890305\n",
            "Loss:  0.002697285912388716\n",
            "Loss:  0.0027008636740600543\n",
            "Loss:  0.0027044645953094824\n",
            "Loss:  0.002708088529489859\n",
            "Loss:  0.002711735329586436\n",
            "Loss:  0.0027154048482033563\n",
            "Loss:  0.0027190969375504724\n",
            "Loss:  0.002722811449430769\n",
            "Loss:  0.002726548235228154\n",
            "Loss:  0.002730307145895627\n",
            "Loss:  0.0027340880319439258\n",
            "Loss:  0.0027378907434306584\n",
            "Loss:  0.0027417151299496084\n",
            "Loss:  0.002745561040620745\n",
            "Loss:  0.0027494283240803914\n",
            "Loss:  0.0027533168284718267\n",
            "Loss:  0.002757226401436316\n",
            "Loss:  0.0027611568901043846\n",
            "Loss:  0.002765108141087625\n",
            "Loss:  0.0027690800004707035\n",
            "Loss:  0.002773072313803686\n",
            "Loss:  0.0027770849260949446\n",
            "Loss:  0.0027811176818040434\n",
            "Loss:  0.0027851704248351996\n",
            "Loss:  0.0027892429985309987\n",
            "Loss:  0.0027933352456663697\n",
            "Loss:  0.00279744700844281\n",
            "Loss:  0.002801578128483114\n",
            "Loss:  0.0028057284468262504\n",
            "Loss:  0.0028098978039224447\n",
            "Loss:  0.0028140860396287243\n",
            "Loss:  0.0028182929932046054\n",
            "Loss:  0.002822518503308077\n",
            "Loss:  0.0028267624079919332\n",
            "Loss:  0.0028310245447002134\n",
            "Loss:  0.0028353047502650123\n",
            "Loss:  0.002839602860903486\n",
            "Loss:  0.0028439187122150655\n",
            "Loss:  0.0028482521391791442\n",
            "Loss:  0.0028526029761525577\n",
            "Loss:  0.0028569710568676836\n",
            "Loss:  0.0028613562144306306\n",
            "Loss:  0.0028657582813196455\n",
            "Loss:  0.0028701770893836513\n",
            "Loss:  0.0028746124698412275\n",
            "Loss:  0.002879064253279528\n",
            "Loss:  0.002883532269653599\n",
            "Loss:  0.002888016348285803\n",
            "Loss:  0.0028925163178655133\n",
            "Loss:  0.002897032006448894\n",
            "Loss:  0.00290156324145891\n",
            "Loss:  0.0029061098496856765\n",
            "Loss:  0.00291067165728668\n",
            "Loss:  0.002915248489787496\n",
            "Loss:  0.002919840172082433\n",
            "Loss:  0.002924446528435518\n",
            "Loss:  0.002929067382481561\n",
            "Loss:  0.002933702557227462\n",
            "Loss:  0.0029383518750536624\n",
            "Loss:  0.002943015157715542\n",
            "Loss:  0.0029476922263453656\n",
            "Loss:  0.0029523829014540693\n",
            "Loss:  0.0029570870029334046\n",
            "Loss:  0.0029618043500579962\n",
            "Loss:  0.002966534761487741\n",
            "Loss:  0.0029712780552703278\n",
            "Loss:  0.002976034048843872\n",
            "Loss:  0.0029808025590396547\n",
            "Loss:  0.002985583402084984\n",
            "Loss:  0.0029903763936063245\n",
            "Loss:  0.0029951813486324163\n",
            "Loss:  0.002999998081597542\n",
            "Loss:  0.0030048264063450274\n",
            "Loss:  0.0030096661361307415\n",
            "Loss:  0.003014517083626799\n",
            "Loss:  0.0030193790609253975\n",
            "Loss:  0.003024251879542626\n",
            "Loss:  0.003029135350422626\n",
            "Loss:  0.003034029283941675\n",
            "Loss:  0.0030389334899124892\n",
            "Loss:  0.0030438477775886577\n",
            "Loss:  0.0030487719556690305\n",
            "Loss:  0.003053705832302384\n",
            "Loss:  0.003058649215092106\n",
            "Loss:  0.0030636019111011117\n",
            "Loss:  0.003068563726856672\n",
            "Loss:  0.003073534468355522\n",
            "Loss:  0.0030785139410689864\n",
            "Loss:  0.0030835019499481906\n",
            "Loss:  0.003088498299429431\n",
            "Loss:  0.003093502793439639\n",
            "Loss:  0.0030985152354019493\n",
            "Loss:  0.003103535428241278\n",
            "Loss:  0.003108563174390121\n",
            "Loss:  0.0031135982757944255\n",
            "Loss:  0.0031186405339194436\n",
            "Loss:  0.0031236897497559073\n",
            "Loss:  0.0031287457238259983\n",
            "Loss:  0.0031338082561897366\n",
            "Loss:  0.003138877146451243\n",
            "Loss:  0.0031439521937651443\n",
            "Loss:  0.003149033196843181\n",
            "Loss:  0.0031541199539606574\n",
            "Loss:  0.0031592122629633345\n",
            "Loss:  0.0031643099212740893\n",
            "Loss:  0.0031694127258999143\n",
            "Loss:  0.0031745204734388633\n",
            "Loss:  0.0031796329600871404\n",
            "Loss:  0.0031847499816462766\n",
            "Loss:  0.0031898713335304345\n",
            "Loss:  0.003194996810773763\n",
            "Loss:  0.0032001262080378645\n",
            "Loss:  0.0032052593196193555\n",
            "Loss:  0.0032103959394575387\n",
            "Loss:  0.0032155358611421305\n",
            "Loss:  0.003220678877921197\n",
            "Loss:  0.0032258247827090113\n",
            "Loss:  0.0032309733680941474\n",
            "Loss:  0.0032361244263476503\n",
            "Loss:  0.0032412777494312413\n",
            "Loss:  0.0032464331290057123\n",
            "Loss:  0.0032515903564393623\n",
            "Loss:  0.0032567492228165935\n",
            "Loss:  0.003261909518946488\n",
            "Loss:  0.003267071035371645\n",
            "Loss:  0.003272233562377068\n",
            "Loss:  0.003277396889999037\n",
            "Loss:  0.0032825608080343746\n",
            "Loss:  0.003287725106049457\n",
            "Loss:  0.003292889573389687\n",
            "Loss:  0.0032980539991887534\n",
            "Loss:  0.0033032181723783205\n",
            "Loss:  0.003308381881697552\n",
            "Loss:  0.0033135449157030007\n",
            "Loss:  0.0033187070627783834\n",
            "Loss:  0.003323868111144669\n",
            "Loss:  0.0033290278488701847\n",
            "Loss:  0.0033341860638808093\n",
            "Loss:  0.0033393425439703795\n",
            "Loss:  0.0033444970768112346\n",
            "Loss:  0.003349649449964807\n",
            "Loss:  0.0033547994508924587\n",
            "Loss:  0.0033599468669661876\n",
            "Loss:  0.003365091485479853\n",
            "Loss:  0.0033702330936601827\n",
            "Loss:  0.003375371478678261\n",
            "Loss:  0.0033805064276607615\n",
            "Loss:  0.003385637727701628\n",
            "Loss:  0.0033907651658737895\n",
            "Loss:  0.003395888529241072\n",
            "Loss:  0.0034010076048701537\n",
            "Loss:  0.0034061221798427203\n",
            "Loss:  0.0034112320412678903\n",
            "Loss:  0.003416336976294575\n",
            "Loss:  0.0034214367721241788\n",
            "Loss:  0.003426531216023392\n",
            "Loss:  0.003431620095337097\n",
            "Loss:  0.00343670319750152\n",
            "Loss:  0.0034417803100575592\n",
            "Loss:  0.003446851220664118\n",
            "Loss:  0.0034519157171117886\n",
            "Loss:  0.003456973587336688\n",
            "Loss:  0.003462024619434411\n",
            "Loss:  0.0034670686016741365\n",
            "Loss:  0.0034721053225129825\n",
            "Loss:  0.0034771345706104636\n",
            "Loss:  0.0034821561348433237\n",
            "Loss:  0.003487169804320263\n",
            "Loss:  0.003492175368397108\n",
            "Loss:  0.0034971726166919963\n",
            "Loss:  0.0035021613391009176\n",
            "Loss:  0.0035071413258133156\n",
            "Loss:  0.0035121123673279415\n",
            "Loss:  0.003517074254468949\n",
            "Loss:  0.0035220267784020986\n",
            "Loss:  0.0035269697306512617\n",
            "Loss:  0.0035319029031151117\n",
            "Loss:  0.0035368260880839393\n",
            "Loss:  0.003541739078256769\n",
            "Loss:  0.0035466416667587655\n",
            "Loss:  0.003551533647158599\n",
            "Loss:  0.0035564148134863837\n",
            "Loss:  0.0035612849602514774\n",
            "Loss:  0.003566143882460798\n",
            "Loss:  0.0035709913756371953\n",
            "Loss:  0.0035758272358380958\n",
            "Loss:  0.003580651259674407\n",
            "Loss:  0.0035854632443296235\n",
            "Loss:  0.003590262987579229\n",
            "Loss:  0.0035950502878101154\n",
            "Loss:  0.0035998249440406978\n",
            "Loss:  0.003604586755940723\n",
            "Loss:  0.003609335523851642\n",
            "Loss:  0.0036140710488072277\n",
            "Loss:  0.0036187931325543186\n",
            "Loss:  0.0036235015775739243\n",
            "Loss:  0.003628196187102428\n",
            "Loss:  0.0036328767651532725\n",
            "Loss:  0.0036375431165385995\n",
            "Loss:  0.003642195046891393\n",
            "Loss:  0.003646832362687836\n",
            "Loss:  0.003650680101669022\n",
            "Loss:  0.003643401192019169\n",
            "Loss:  0.003635303704283851\n",
            "Loss:  0.0036274024959219234\n",
            "Loss:  0.0036197342985423314\n",
            "Loss:  0.0036122938201753716\n",
            "Loss:  0.0036050741782509867\n",
            "Loss:  0.0035980686379468267\n",
            "Loss:  0.003591270679363377\n",
            "Loss:  0.0035846739928570813\n",
            "Loss:  0.0035782724715958778\n",
            "Loss:  0.0035720602043019314\n",
            "Loss:  0.0035660314682938125\n",
            "Loss:  0.0035601807228172744\n",
            "Loss:  0.0035545026026497493\n",
            "Loss:  0.0035489919119648192\n",
            "Loss:  0.0035436436184433143\n",
            "Loss:  0.003538452847618638\n",
            "Loss:  0.0035334148774452787\n",
            "Loss:  0.003528525133079139\n",
            "Loss:  0.003523779181859927\n",
            "Loss:  0.00351917272848603\n",
            "Loss:  0.003514701610373002\n",
            "Loss:  0.003510361793187165\n",
            "Loss:  0.0035061493665464838\n",
            "Loss:  0.0035020605398813505\n",
            "Loss:  0.003498091638448263\n",
            "Loss:  0.0034942390994896918\n",
            "Loss:  0.0034904994685340533\n",
            "Loss:  0.0034868693958298217\n",
            "Loss:  0.0034833456329081687\n",
            "Loss:  0.0034799250292691015\n",
            "Loss:  0.0034766045291857394\n",
            "Loss:  0.0034733811686223012\n",
            "Loss:  0.0034702520722612513\n",
            "Loss:  0.0034672144506351595\n",
            "Loss:  0.0034642655973596563\n",
            "Loss:  0.00346140288646305\n",
            "Loss:  0.0034586237698094463\n",
            "Loss:  0.003455925774611559\n",
            "Loss:  0.003453306501030111\n",
            "Loss:  0.003450763619856391\n",
            "Loss:  0.003448294870275225\n",
            "Loss:  0.003445898057705156\n",
            "Loss:  0.0034435710517132213\n",
            "Loss:  0.0034413117840017965\n",
            "Loss:  0.0034391182464646244\n",
            "Loss:  0.003436988489309774\n",
            "Loss:  0.003434920619247402\n",
            "Loss:  0.0034329127977395595\n",
            "Loss:  0.0034309632393103146\n",
            "Loss:  0.0034290702099140417\n",
            "Loss:  0.0034272320253596524\n",
            "Loss:  0.0034254470497890126\n",
            "Loss:  0.0034237136942079096\n",
            "Loss:  0.0034220304150672444\n",
            "Loss:  0.0034203957128932667\n",
            "Loss:  0.003418808130964916\n",
            "Loss:  0.00341726625403671\n",
            "Loss:  0.0034157687071057733\n",
            "Loss:  0.0034143141542212533\n",
            "Loss:  0.0034129012973350067\n",
            "Loss:  0.003411528875191954\n",
            "Loss:  0.003410195662258786\n",
            "Loss:  0.0034089004676898485\n",
            "Loss:  0.0034076421343288235\n",
            "Loss:  0.003406419537745042\n",
            "Loss:  0.0034052315853033755\n",
            "Loss:  0.0034022506342172355\n",
            "Loss:  0.0033903662447052393\n",
            "Loss:  0.0033775208565911988\n",
            "Loss:  0.0033648871720108873\n",
            "Loss:  0.003352509180352898\n",
            "Loss:  0.0033403821855114376\n",
            "Loss:  0.0033284995883605616\n",
            "Loss:  0.003316854906239197\n",
            "Loss:  0.0033054418536949174\n",
            "Loss:  0.0032942543395291096\n",
            "Loss:  0.0032832864604768267\n",
            "Loss:  0.0032725324949873587\n",
            "Loss:  0.0032619868972429866\n",
            "Loss:  0.003251644291409931\n",
            "Loss:  0.0032414994661101298\n",
            "Loss:  0.0032315473691033984\n",
            "Loss:  0.00322178310216959\n",
            "Loss:  0.0032122019161812724\n",
            "Loss:  0.0032027992063582544\n",
            "Loss:  0.0031935705076952016\n",
            "Loss:  0.003184511490555252\n",
            "Loss:  0.003175617956421175\n",
            "Loss:  0.003166885833798171\n",
            "Loss:  0.0031583111742611303\n",
            "Loss:  0.0031498901486405675\n",
            "Loss:  0.0031416190433410872\n",
            "Loss:  0.003133494256787104\n",
            "Loss:  0.003125512295990647\n",
            "Loss:  0.0031176697732361197\n",
            "Loss:  0.00310996340287761\n",
            "Loss:  0.003102389998244191\n",
            "Loss:  0.0030949464686490275\n",
            "Loss:  0.0030876298164983188\n",
            "Loss:  0.0030804371344962515\n",
            "Loss:  0.003073365602942496\n",
            "Loss:  0.003066412487118659\n",
            "Loss:  0.003059575134760458\n",
            "Loss:  0.0030528509736124886\n",
            "Loss:  0.0030462375090626304\n",
            "Loss:  0.003039732321853117\n",
            "Loss:  0.0030333330658656884\n",
            "Loss:  0.003027037465978088\n",
            "Loss:  0.0030208433159894924\n",
            "Loss:  0.0030147484766122183\n",
            "Loss:  0.0030087508735277195\n",
            "Loss:  0.003002848495504426\n",
            "Loss:  0.0029970393925752696\n",
            "Loss:  0.0029913216742730106\n",
            "Loss:  0.002985693507921132\n",
            "Loss:  0.0029801531169786513\n",
            "Loss:  0.0029746987794366255\n",
            "Loss:  0.0029693288262650553\n",
            "Loss:  0.00296404163990812\n",
            "Loss:  0.0029588356528260497\n",
            "Loss:  0.0029537093460822784\n",
            "Loss:  0.0029486612479741104\n",
            "Loss:  0.0029436899327055168\n",
            "Loss:  0.0029387940191005096\n",
            "Loss:  0.0029339721693556775\n",
            "Loss:  0.0029292230878307235\n",
            "Loss:  0.0029245455198754016\n",
            "Loss:  0.0029199382506915838\n",
            "Loss:  0.002915400104229778\n",
            "Loss:  0.00291092994211771\n",
            "Loss:  0.0029065266626211957\n",
            "Loss:  0.0029021891996351503\n",
            "Loss:  0.0028979165217039825\n",
            "Loss:  0.002893707631070499\n",
            "Loss:  0.002889561562751958\n",
            "Loss:  0.0028854773836423063\n",
            "Loss:  0.0028814541916400197\n",
            "Loss:  0.0028774911147998916\n",
            "Loss:  0.002873587310508565\n",
            "Loss:  0.0028697419646825993\n",
            "Loss:  0.002865954290988013\n",
            "Loss:  0.0028622235300809333\n",
            "Loss:  0.0028585489488679987\n",
            "Loss:  0.0028549298397862045\n",
            "Loss:  0.002851365520101115\n",
            "Loss:  0.002847855331222946\n",
            "Loss:  0.0028443986380395025\n",
            "Loss:  0.0028409948282654015\n",
            "Loss:  0.0028376433118071994\n",
            "Loss:  0.0028343435201431838\n",
            "Loss:  0.0028310949057177744\n",
            "Loss:  0.002827896941349453\n",
            "Loss:  0.0028247491196519803\n",
            "Loss:  0.002821650952468139\n",
            "Loss:  0.002818601970315471\n",
            "Loss:  0.002815601721843506\n",
            "Loss:  0.002812649773302077\n",
            "Loss:  0.002809745708019916\n",
            "Loss:  0.0028068891258933386\n",
            "Loss:  0.002804079642884499\n",
            "Loss:  0.0028013168905286338\n",
            "Loss:  0.002798600515450129\n",
            "Loss:  0.00279593017888663\n",
            "Loss:  0.002793305556221168\n",
            "Loss:  0.0027907263365217812\n",
            "Loss:  0.0027881922220882173\n",
            "Loss:  0.0027857029280054756\n",
            "Loss:  0.0027832581817037426\n",
            "Loss:  0.002780857722524568\n",
            "Loss:  0.0027785013012929702\n",
            "Loss:  0.0027761886798950523\n",
            "Loss:  0.0027739196308610562\n",
            "Loss:  0.00277169393695337\n",
            "Loss:  0.002769511390759606\n",
            "Loss:  0.0027673717942902853\n",
            "Loss:  0.0027652749585809298\n",
            "Loss:  0.0027632207032986006\n",
            "Loss:  0.002761208856352512\n",
            "Loss:  0.002759239253508579\n",
            "Loss:  0.0027573117380078533\n",
            "Loss:  0.002755426160188973\n",
            "Loss:  0.0027535823771138307\n",
            "Loss:  0.0027517802521970953\n",
            "Loss:  0.002750019654839253\n",
            "Loss:  0.0027483004600629083\n",
            "Loss:  0.002746622548152744\n",
            "Loss:  0.0027449858042985467\n",
            "Loss:  0.0027433901182418487\n",
            "Loss:  0.002741835383925887\n",
            "Loss:  0.00274032149914896\n",
            "Loss:  0.00273884836522113\n",
            "Loss:  0.0027374158866245355\n",
            "Loss:  0.0027360239706770537\n",
            "Loss:  0.0027346725271997115\n",
            "Loss:  0.0027333614681877938\n",
            "Loss:  0.0027320907074855627\n",
            "Loss:  0.0027308601604650235\n",
            "Loss:  0.0027296697437086807\n",
            "Loss:  0.0027285193746963646\n",
            "Loss:  0.0027274089714964312\n",
            "Loss:  0.0027263384524613514\n",
            "Loss:  0.0027253077359278934\n",
            "Loss:  0.0027243167399221887\n",
            "Loss:  0.0027233653818696433\n",
            "Loss:  0.002722453578310032\n",
            "Loss:  0.002721581244618064\n",
            "Loss:  0.0027207482947292807\n",
            "Loss:  0.002719954640871993\n",
            "Loss:  0.0027192001933050223\n",
            "Loss:  0.0027184848600617846\n",
            "Loss:  0.0027178085467007753\n",
            "Loss:  0.0027171711560627435\n",
            "Loss:  0.0027165725880348465\n",
            "Loss:  0.0027160127393220167\n",
            "Loss:  0.002715491503225611\n",
            "Loss:  0.0027150087694298077\n",
            "Loss:  0.0027145644237960265\n",
            "Loss:  0.0027141583481653173\n",
            "Loss:  0.0027137904201692295\n",
            "Loss:  0.00271346051304938\n",
            "Loss:  0.0027131684954858916\n",
            "Loss:  0.0027129142314349214\n",
            "Loss:  0.0027126975799755585\n",
            "Loss:  0.002712518395166322\n",
            "Loss:  0.0027123765259114666\n",
            "Loss:  0.002712271815837253\n",
            "Loss:  0.002712204103178442\n",
            "Loss:  0.002712173220675303\n",
            "Loss:  0.0027121789954810942\n",
            "Loss:  0.002712221249080326\n",
            "Loss:  0.00271229979721809\n",
            "Loss:  0.002712414449840254\n",
            "Loss:  0.0027125650110450088\n",
            "Loss:  0.002712751279045654\n",
            "Loss:  0.00271297304614492\n",
            "Loss:  0.002713230098720674\n",
            "Loss:  0.0027135222172233516\n",
            "Loss:  0.002713849176184845\n",
            "Loss:  0.0027142107442391997\n",
            "Loss:  0.00271460668415488\n",
            "Loss:  0.002715036752878715\n",
            "Loss:  0.002715500701591489\n",
            "Loss:  0.002715998275775047\n",
            "Loss:  0.0027165292152908894\n",
            "Loss:  0.0027170932544701436\n",
            "Loss:  0.0027176901222147488\n",
            "Loss:  0.0027183195421097476\n",
            "Loss:  0.002718981232546393\n",
            "Loss:  0.0027196749068560196\n",
            "Loss:  0.0027204002734543785\n",
            "Loss:  0.002721157035995993\n",
            "Loss:  0.002721944893538383\n",
            "Loss:  0.002722763540716129\n",
            "Loss:  0.0027236126679235605\n",
            "Loss:  0.0027244919615067617\n",
            "Loss:  0.0027254011039636165\n",
            "Loss:  0.002726339774151772\n",
            "Loss:  0.0027273076475041896\n"
          ]
        }
      ],
      "source": [
        "# Después de varia pruebas notamos que con un learning_rate de 0.0001 y 1200 epoch la diferencia de resultados\n",
        "# entre entrenamientos se reduce considerablemente.\n",
        "embedding.train(encoder_input, 1200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 279,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLlcUFV1ILch",
        "outputId": "d6682059-df06-4b90-9da1-2f119ba016fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n",
            "un\n"
          ]
        }
      ],
      "source": [
        "clave=source_token_dict['un']\n",
        "print(clave)\n",
        "valor=source_token_dict_inv[clave]\n",
        "print(valor)\n",
        "#print(source_token_dict['women'])\n",
        "#print(source_token_dict['queen'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 316,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faSpTmrsH0vm",
        "outputId": "480404f5-f6f0-4a7a-ba43-00ff7b08db5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13\n"
          ]
        }
      ],
      "source": [
        "embedding_dic={}\n",
        "for i in range(len(source_token_dict)):\n",
        "  response = embedding.getEmbedding([i])\n",
        "  embedding_dic[i]=response\n",
        "print(len(embedding_dic))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open(\"embeddings.pkl\", \"wb\") as tf:\n",
        "    pickle.dump(embedding_dic, tf)"
      ],
      "metadata": {
        "id": "ZPZhimdGF86R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Notamos que los resultados son muy variados, por lo que cargamos uno por defecto\n",
        "with open(\"embeddings.pkl\", \"rb\") as tf:\n",
        "    embedding_dic = pickle.load(tf)"
      ],
      "metadata": {
        "id": "_yGtFw21Psw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "x=[]\n",
        "y=[]\n",
        "for key, value in embedding_dic.items():\n",
        "  x.append(embedding_dic[key][0][0][0])\n",
        "  y.append(embedding_dic[key][0][0][1])\n",
        "  #words.append(source_token_dict_inv[key])\n",
        "plt.scatter(x, y)\n",
        "\n",
        "for key, value in embedding_dic.items():\n",
        "  plt.annotate(source_token_dict_inv[key], (embedding_dic[key][0][0][0], embedding_dic[key][0][0][1]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "oj8jDxPiHb3w",
        "outputId": "2ac18ec3-8780-4364-bbd2-d134ec20ce72"
      },
      "execution_count": 317,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 326,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqkwovI8Vey-",
        "outputId": "4672ea39-6ced-45e3-e2be-ab6014a0f9bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[1.31746691, 0.8024541 ]])]\n"
          ]
        }
      ],
      "source": [
        "embedding_of_rey = embedding_dic[1]\n",
        "print(embedding_of_rey)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 327,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baYuk5cRV4EV",
        "outputId": "a8d9c82b-d975-4b2f-bf5b-7e00ed2d61eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[1.76220504, 0.14203406]])]\n"
          ]
        }
      ],
      "source": [
        "embedding_of_hombre = embedding_dic[6]\n",
        "print(embedding_of_hombre)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 328,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMXE_23iWH2E",
        "outputId": "6236642e-a7bb-488f-a7a1-dfc13eb7040a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[1.75954317, 0.30691424]])]\n"
          ]
        }
      ],
      "source": [
        "embedding_of_mujer = embedding_dic[10]\n",
        "print(embedding_of_mujer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 329,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21MF3a8KWSHv",
        "outputId": "75d02f11-e324-4aa4-da1d-718b1d270a14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.31480504 0.96733428]]\n",
            "('reina', 0.052395594453998426)\n",
            "[array([[1.28156446, 0.92683289]])]\n",
            "[array([[1.28156446, 0.92683289]])]\n"
          ]
        }
      ],
      "source": [
        "analogia = embedding_of_rey[0]-embedding_of_hombre[0]+embedding_of_mujer[0]\n",
        "print(analogia)\n",
        "distances={}\n",
        "for key, value in embedding_dic.items():\n",
        "  distance=cosine_similarity(analogia, value)\n",
        "  distances[source_token_dict_inv[key]]=distance\n",
        "sorted_similarities = sorted(distances.items(), key=lambda x: x[1])\n",
        "print(sorted_similarities[0])\n",
        "print(embedding_dic[source_token_dict[sorted_similarities[0][0]]])\n",
        "print(embedding_dic[8])#source_token_dict[sorted_similarities[0][0]]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "erdqVc5iYT3i"
      },
      "outputs": [],
      "source": [
        "def cosine_similarity(v1, v2):\n",
        "  #return (np.array(v1)@np.array(v2))/(np.linalg.norm(v1)*np.linalg.norm(v2))\n",
        "  return np.sqrt(np.sum((np.power((v1-v2),2))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "LvgKb5MRzuwV"
      },
      "outputs": [],
      "source": [
        "def most_similar(word, word_dict, top_k=4):\n",
        "  if word not in word_dict:\n",
        "    raise ValueError(f\"{word} not found in the dictionary\")\n",
        "  else:\n",
        "    key = word_dict[word]\n",
        "\n",
        "    queryVector = embedding_dic[key]\n",
        "\n",
        "    similarities = {}\n",
        "    for key2, value in embedding_dic.items():\n",
        "      if key!=key2:\n",
        "        similarity = cosine_similarity(queryVector[0][0], value[0][0])\n",
        "        similarities[source_token_dict_inv[key2]]=similarity\n",
        "  sorted_similarities = sorted(similarities.items(), key=lambda x: x[1])\n",
        "  return sorted_similarities[:top_k]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 330,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jTlJoK92GOm",
        "outputId": "3ca5e10d-847b-4233-bb92-b17fce9415f2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('reina', 0.12945682581864398),\n",
              " ('Inglaterra', 0.5437545156373942),\n",
              " ('una', 0.6182244957037457),\n",
              " ('de', 0.619551973834132)]"
            ]
          },
          "metadata": {},
          "execution_count": 330
        }
      ],
      "source": [
        "most_similar('rey', source_token_dict, 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U74LNNu1Io2U"
      },
      "source": [
        "**Ahora veremos el término \"Positional Encoding\"**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcA79xihIwbb"
      },
      "outputs": [],
      "source": [
        "def get_positionalEncoding(len_Sentence, dim_embedding):\n",
        "  pos = np.arange(len_Sentence)[:, np.newaxis]\n",
        "  angle_rads = pos / np.power(10000, (2 * (np.arange(dim_embedding) // 2)) / np.float32(dim_embedding))\n",
        "  # Aplicar seno a los índices pares en el arreglo\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "  # Aplicar coseno a los índices impares en el arreglo\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "  return angle_rads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dWkFjGvqi7z",
        "outputId": "9ee3d30c-52f5-4625-bf6d-a0877cf655c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 1.]\n",
            "[0.84147098 0.54030231]\n",
            "[ 0.90929743 -0.41614684]\n",
            "[ 0.14112001 -0.9899925 ]\n",
            "[-0.7568025  -0.65364362]\n",
            "[-0.95892427  0.28366219]\n",
            "[-0.2794155   0.96017029]\n"
          ]
        }
      ],
      "source": [
        "positional_encoding = get_positionalEncoding(7, 2)\n",
        "for vector in positional_encoding:\n",
        "  print(vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        },
        "id": "O0as2s2T0Y0Y",
        "outputId": "1acca3c7-8b87-48a4-ae49-d2583a8419a7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.colorbar.Colorbar at 0x79e828bd0850>"
            ]
          },
          "metadata": {},
          "execution_count": 110
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x933.333 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "P = get_positionalEncoding(7, 3)\n",
        "cax = plt.matshow(P)\n",
        "plt.gcf().colorbar(cax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsTckgK6Hrj4"
      },
      "source": [
        "Sumamos el embedding + la codificación posicional para la frase **la reina de inglarterra es una mujer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oapwTAJmH4v3",
        "outputId": "2fe85425-65d0-48c5-e198-fcbd5f4d1957"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding\n",
            "[4.36784609 0.03817434]\n",
            "[5.66969563 0.        ]\n",
            "[0. 0.]\n",
            "[1.26963357 0.        ]\n",
            "[7.23264316 0.        ]\n",
            "[0. 0.]\n",
            "[5.52944602 0.12864978]\n",
            "\n",
            "Positional Encoding\n",
            "[0. 1.]\n",
            "[0.84147098 0.54030231]\n",
            "[ 0.90929743 -0.41614684]\n",
            "[ 0.14112001 -0.9899925 ]\n",
            "[-0.7568025  -0.65364362]\n",
            "[-0.95892427  0.28366219]\n",
            "[-0.2794155   0.96017029]\n",
            "\n",
            "Input for Encoder\n",
            "[4.36784609 1.03817434]\n",
            "[6.51116662 0.54030231]\n",
            "[ 0.90929743 -0.41614684]\n",
            "[ 1.41075358 -0.9899925 ]\n",
            "[ 6.47584067 -0.65364362]\n",
            "[-0.95892427  0.28366219]\n",
            "[5.25003052 1.08882006]\n"
          ]
        }
      ],
      "source": [
        "embedding_of_sentence = embedding.getEmbedding(encoder_input[1])\n",
        "print(\"Embedding\")\n",
        "for value in embedding_of_sentence:\n",
        "  print(value[0])\n",
        "print(\"\\nPositional Encoding\")\n",
        "for value in np.array(positional_encoding):\n",
        "  print(value)\n",
        "print(\"\\nInput for Encoder\")\n",
        "for i in range(len(embedding_of_sentence)):\n",
        "  print(embedding_of_sentence[i][0]+np.array(positional_encoding)[i])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASM0zjgWcXC8"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xb76eFqcRLR",
        "outputId": "62963558-c027-4232-e390-13ef1bfa5b5c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3.1622776601683795"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.sqrt(np.sum(np.power(np.array([3, 2, 5])-np.array([2, 2, 2]), 2)))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOHJLIqrbbrq5RVDGRw8bBM",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}