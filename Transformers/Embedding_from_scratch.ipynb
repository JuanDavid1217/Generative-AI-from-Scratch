{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JuanDavid1217/Generative-AI-from-Scratch/blob/main/Transformers/Embedding_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAvXfaR8qZTX"
      },
      "source": [
        "**Primero se necesita entender como funcionan lo Embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BT8gperVqUhz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from pickle import load\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wXqWbx_BxQo1"
      },
      "outputs": [],
      "source": [
        "class Activation_ReLU:\n",
        "  def __init__(self):\n",
        "    self.input=[]\n",
        "    self.output=[]\n",
        "    self.dinputs=[]\n",
        "\n",
        "  def forward(self, input):\n",
        "    self.input = input\n",
        "    self.output=np.maximum(0, input)\n",
        "\n",
        "  def backward(self, dvalues):\n",
        "    dvalues = np.sum(dvalues, axis=0, keepdims=True)\n",
        "    self.dinputs=dvalues*np.where(self.output > 0, 1, 0)\n",
        "\n",
        "class Activation_Sigmoid:\n",
        "  def __init__(self):\n",
        "    self.input=[]\n",
        "    self.output=[]\n",
        "    self.dinputs=[]\n",
        "\n",
        "  def forward(self, input):\n",
        "    self.output = 1 / (1 + np.exp(-input))\n",
        "\n",
        "  def backward(self, dvalues):\n",
        "    self.dinputs = dvalues * (1 - self.output) * self.output\n",
        "\n",
        "class Activation_SoftMax:\n",
        "  def __init__(self):\n",
        "    self.input=[]\n",
        "    self.output=[]\n",
        "    self.dinputs=[]\n",
        "\n",
        "  def forward(self, input):\n",
        "    exp_z = np.exp(input - np.max(input))\n",
        "    self.output = exp_z / np.sum(exp_z, axis=0)\n",
        "\n",
        "  def backward(self, dvalues):\n",
        "    # Create uninitialized array\n",
        "    self.dinputs = np.empty_like(dvalues)\n",
        "    # Enumerate outputs and gradients\n",
        "    for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
        "      # Flatten output array\n",
        "      single_output = single_output.reshape(-1, 1)\n",
        "      # Calculate Jacobian matrix of the output and\n",
        "      jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
        "      # Calculate sample-wise gradient\n",
        "      # and add it to the array of sample gradients\n",
        "      self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VkywBDDRxZhn"
      },
      "outputs": [],
      "source": [
        "class Dense:\n",
        "  def __init__(self, input_dim, neurons, bias=False):\n",
        "    self.weights=0.1*np.random.randn(input_dim, neurons)\n",
        "    self.bias=bias\n",
        "    if self.bias:\n",
        "      self.biases=np.zeros((1, neurons))\n",
        "      self.dbiases=0\n",
        "    self.input=[]\n",
        "    self.output=[]\n",
        "    self.dweights=0\n",
        "    self.dinputs=0\n",
        "\n",
        "  def  forward(self, input):\n",
        "    self.input=input\n",
        "    if self.bias:\n",
        "      self.output = np.dot(self.input, self.weights)+self.biases\n",
        "    else:\n",
        "      self.output=np.dot(self.input, self.weights)\n",
        "\n",
        "  def backward(self, dvalues):\n",
        "    if self.bias:\n",
        "      self.dbiases=dvalues\n",
        "    else:\n",
        "      if dvalues.shape[0]!=1:\n",
        "        dvalues=dvalues.reshape(1, -1)\n",
        "\n",
        "    if self.input.shape[0]!=1:\n",
        "        self.input=self.input.reshape(1, -1)\n",
        "    self.dweights=np.dot(self.input.T, dvalues)\n",
        "    self.dinputs=np.dot(dvalues, self.weights.T)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "utieyHkpxvbo"
      },
      "outputs": [],
      "source": [
        "class Optimizer_Adam:\n",
        "  def __init__(self, learning_rate=0.01, decay=0., epsilon=1e-7,beta_1=0.9, beta_2=0.999):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.current_learning_rate = learning_rate\n",
        "    self.decay = decay\n",
        "    self.iterations = 0\n",
        "    self.epsilon = epsilon\n",
        "    self.beta_1 = beta_1\n",
        "    self.beta_2 = beta_2\n",
        "\n",
        "\n",
        "  def pre_update_params(self):\n",
        "    if self.decay:\n",
        "      self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "  def update_params(self, layer):\n",
        "    if not hasattr(layer, 'weight_cache'):\n",
        "      layer.weight_momentums = np.zeros_like(layer.weights)\n",
        "      layer.weight_cache = np.zeros_like(layer.weights)\n",
        "      if hasattr(layer, 'biases'):\n",
        "        layer.bias_momentums = np.zeros_like(layer.biases)\n",
        "        layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "    # Update momentum with current gradients\n",
        "    layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights\n",
        "    if hasattr(layer, 'biases'):\n",
        "      layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases\n",
        "    # Get corrected momentum\n",
        "    # self.iteration is 0 at first pass\n",
        "    # and we need to start with 1 here\n",
        "    weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
        "    if hasattr(layer, 'biases'):\n",
        "      bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
        "    # Update cache with squared current gradients\n",
        "    layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights**2\n",
        "    if hasattr(layer, 'biases'):\n",
        "      layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases**2\n",
        "    # Get corrected cache\n",
        "    weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
        "    if hasattr(layer, 'biases'):\n",
        "      bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
        "    # Vanilla SGD parameter update + normalization\n",
        "    # with square rooted cache\n",
        "    layer.weights += -self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
        "    if hasattr(layer, 'biases'):\n",
        "      layer.biases += -self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
        "\n",
        "  # Call once after any parameter updates\n",
        "  def post_update_params(self):\n",
        "    self.iterations += 1\n",
        "\n",
        "\n",
        "class Rmsprop:\n",
        "  def __init__(self, learning_rate=0.1, decay=0., epsilon=1e-8, rho=0.9):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.current_learning_rate = learning_rate\n",
        "    self.decay = decay\n",
        "    self.iterations = 0\n",
        "    self.epsilon = epsilon\n",
        "    self.rho = rho\n",
        "\n",
        "  def pre_update_params(self):\n",
        "    if self.decay:\n",
        "      self.current_learning_rate = self.learning_rate * \\\n",
        "          (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "  def update_params(self, layer):\n",
        "    if not hasattr(layer, 'wight_cache'):\n",
        "      layer.weight_cache = np.zeros_like(layer.weights)\n",
        "    if hasattr(layer, 'biases'):\n",
        "      layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "    layer.weight_cache = self.rho * layer.weight_cache + \\\n",
        "        (1 - self.rho) * layer.dweights**2\n",
        "    if hasattr(layer, 'biases'):\n",
        "      layer.bias_cache = self.rho * layer.bias_cache + \\\n",
        "          (1 - self.rho) * layer.dbiases**2\n",
        "\n",
        "    layer.weights += -self.current_learning_rate * \\\n",
        "                    layer.dweights / \\\n",
        "                    (np.sqrt(layer.weight_cache) + self.epsilon)\n",
        "    if hasattr(layer, 'biases'):\n",
        "      layer.biases += -self.current_learning_rate * \\\n",
        "                      layer.dbiases / \\\n",
        "                      (np.sqrt(layer.bias_cache) + self.epsilon)\n",
        "    return layer\n",
        "\n",
        "  def post_update_params(self):\n",
        "    self.iterations += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hjZkcK5B96jb"
      },
      "outputs": [],
      "source": [
        "class LossBinaryCrossEntropy():\n",
        "  def forward(self, y_pred, y_true):\n",
        "    y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "\n",
        "    sample_losses = -(y_true * np.log(y_pred_clipped) + (1 - y_true) * np.log(1 - y_pred_clipped))\n",
        "    #sample_losses = np.mean(sample_losses, axis=1)\n",
        "\n",
        "    return sample_losses\n",
        "\n",
        "  def backward(self, dvalues, y_true):\n",
        "    samples = len(dvalues)\n",
        "    outputs = len(dvalues)\n",
        "\n",
        "    clipped_dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)\n",
        "\n",
        "    dinputs = -(y_true / clipped_dvalues - (1 - y_true)\n",
        "                     /(1 - clipped_dvalues))/outputs\n",
        "\n",
        "    return dinputs / samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "kEnCO7G4xxZB"
      },
      "outputs": [],
      "source": [
        "class Embedding:\n",
        "  def __init__(self, vocab_size, sentence_len, latent_dim):\n",
        "    self.vocab_size = vocab_size\n",
        "    self.sentence_len = sentence_len\n",
        "    self.layer1 = Dense(vocab_size, latent_dim, True)\n",
        "    self.activationLayer1 = Activation_ReLU()\n",
        "    self.layer2 = Dense(latent_dim, vocab_size, True)\n",
        "    self.activationLayer2 = Activation_Sigmoid()\n",
        "    self.optimizer = Optimizer_Adam(learning_rate=0.0001)#Rmsprop(learning_rate=0.0001)\n",
        "    self.trainableLayers = [self.layer1, self.layer2]\n",
        "    self.lossFunction = LossBinaryCrossEntropy()\n",
        "\n",
        "  def forward(self, input):\n",
        "    self.layer1.forward(input)\n",
        "    self.activationLayer1.forward(self.layer1.output)\n",
        "    self.layer2.forward(self.activationLayer1.output)\n",
        "    self.activationLayer2.forward(self.layer2.output)\n",
        "    return self.activationLayer2.output\n",
        "\n",
        "  def backward(self, loss):\n",
        "    self.activationLayer2.backward(loss)\n",
        "    self.layer2.backward(self.activationLayer2.dinputs)\n",
        "    self.activationLayer1.backward(self.layer2.dinputs)\n",
        "    self.layer1.backward(self.activationLayer1.dinputs)\n",
        "\n",
        "  def generateOneHotVector(self, input):\n",
        "    oneHotVectors = []\n",
        "    for position in input:\n",
        "      oneHot = np.zeros(self.vocab_size)\n",
        "      oneHot[position] = 1\n",
        "      oneHotVectors.append(oneHot)\n",
        "    return oneHotVectors\n",
        "\n",
        "  def get_SumContext(self, vector):\n",
        "    y_labels=[]\n",
        "    for i in range(len(vector)):\n",
        "      contexts=[]\n",
        "      if i == 0:\n",
        "        contexts = vector[i+1:i+3]\n",
        "      else:\n",
        "        min=i-2\n",
        "        if min<0:\n",
        "          min=0\n",
        "        if len(vector[i+1:i+3])!=0 and len(vector[min:i])!=0:\n",
        "          contexts = np.concatenate((vector[min:i], vector[i+1:i+3]))\n",
        "        elif len(vector[i+1:i+3])==0:\n",
        "          contexts = vector[min:i]\n",
        "      context=[]\n",
        "      for array in contexts:\n",
        "        if len(context)==0:\n",
        "          context = array\n",
        "        else:\n",
        "          context = context + array\n",
        "      y_labels.append(context)\n",
        "    return y_labels\n",
        "\n",
        "  def train(self, input, epoch):\n",
        "    for i in range(epoch):\n",
        "      lossByEpoch=0\n",
        "      for sentence in input:\n",
        "        lossBySentence=0\n",
        "        x_labels= self.generateOneHotVector(sentence)\n",
        "        y_labels = self.get_SumContext(vector=x_labels)\n",
        "        loss=np.zeros((1, 13))\n",
        "        for i in range(len(x_labels)):\n",
        "          prediction = self.forward(x_labels[i])\n",
        "          #print(\"prediction: \", prediction)\n",
        "          #print(f\"waited {i}: {x_labels[i]}\")\n",
        "          loss = loss + (prediction - y_labels[i])#self.lossFunction.forward(prediction,x_labels[i])\n",
        "          self.backward(prediction - y_labels[i])\n",
        "          self.optimizer.pre_update_params()\n",
        "          for layer in self.trainableLayers:\n",
        "            self.optimizer.update_params(layer)\n",
        "          self.optimizer.post_update_params()\n",
        "        lossBySentence = loss/len(x_labels)#np.mean(np.sqrt(loss*loss))\n",
        "        lossByEpoch+=np.mean(lossBySentence)#/len(x_labels)\n",
        "      print(\"Loss: \", lossByEpoch/len(input))\n",
        "\n",
        "  def getEmbedding(self, input):\n",
        "    oneHotVectors = self.generateOneHotVector(input)\n",
        "    embeddings = []\n",
        "    for oneHotVector in oneHotVectors:\n",
        "      self.layer1.forward(oneHotVector)\n",
        "      self.activationLayer1.forward(self.layer1.output)\n",
        "      embeddings.append(self.activationLayer1.output)\n",
        "    return embeddings\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtAY5aM26F7z"
      },
      "source": [
        "Manejo de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yez6tYvi6IRa",
        "outputId": "8417a36a-d3ea-4754-a7a0-8198814edc97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the king of England is a man\n"
          ]
        }
      ],
      "source": [
        "# Leer set de entrenamiento\n",
        "filename = './english-spanish.pkl'\n",
        "\n",
        "#dataset = load(open(filename, 'rb'))\n",
        "#print(dataset[120000,0])\n",
        "#print(dataset[120000,1])\n",
        "dataset = np.array([np.array([\"el rey de Inglaterra es un hombre\",\"the king of England is a man\"]),\n",
        "                    np.array([\"la reina de Inglaterra es una mujer\",\"the queen of England is a woman\"]),\n",
        "                    np.array([\"Carlos es un rey\",\"Carlos is a king\"]),\n",
        "                    np.array([\"Andrea es una reina\",\"Andrea is a queen\"]),\n",
        "                    np.array([\"Carlos es un hombre\",\"Carlos is a man\"]),\n",
        "                    np.array([\"Andrea es una mujer\",\"Andrea is a woman\"])\n",
        "                    ])\n",
        "print(dataset[0,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EAJkCaC6M8k",
        "outputId": "6a193e9e-8aef-4030-b1af-708a79e39573"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['el', 'rey', 'de', 'Inglaterra', 'es', 'un', 'hombre']\n",
            "['the', 'king', 'of', 'England', 'is', 'a', 'man']\n"
          ]
        }
      ],
      "source": [
        "# Crear \"tokens\"\n",
        "source_tokens = []\n",
        "for sentence in dataset[:,0]:\n",
        "  source_tokens.append(sentence.split(' '))\n",
        "print(source_tokens[0])\n",
        "\n",
        "target_tokens = []\n",
        "for sentence in dataset[:,1]:\n",
        "  target_tokens.append(sentence.split(' '))\n",
        "print(target_tokens[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vkYe94YR6RKW"
      },
      "outputs": [],
      "source": [
        "def build_token_dict(token_list):\n",
        "  token_dict = {\n",
        "  #    '<PAD>': 0,\n",
        "  #    '<START>': 1,\n",
        "  #    '<END>': 2\n",
        "  }\n",
        "  for tokens in token_list:\n",
        "    for token in tokens:\n",
        "      if token not in token_dict:\n",
        "        token_dict[token] = len(token_dict)\n",
        "  return token_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEo9slak6VVE",
        "outputId": "0bafa41c-834b-4d0d-f2c7-5f8ac29e4494"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'el': 0, 'rey': 1, 'de': 2, 'Inglaterra': 3, 'es': 4, 'un': 5, 'hombre': 6, 'la': 7, 'reina': 8, 'una': 9, 'mujer': 10, 'Carlos': 11, 'Andrea': 12}\n",
            "{'the': 0, 'king': 1, 'of': 2, 'England': 3, 'is': 4, 'a': 5, 'man': 6, 'queen': 7, 'woman': 8, 'Carlos': 9, 'Andrea': 10}\n",
            "{0: 'the', 1: 'king', 2: 'of', 3: 'England', 4: 'is', 5: 'a', 6: 'man', 7: 'queen', 8: 'woman', 9: 'Carlos', 10: 'Andrea'}\n"
          ]
        }
      ],
      "source": [
        "source_token_dict = build_token_dict(source_tokens)\n",
        "source_token_dict_inv = {v:k for k,v in source_token_dict.items()}\n",
        "target_token_dict = build_token_dict(target_tokens)\n",
        "target_token_dict_inv = {v:k for k,v in target_token_dict.items()}\n",
        "\n",
        "print(source_token_dict)\n",
        "print(target_token_dict)\n",
        "print(target_token_dict_inv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "tjZq7UVG6YuO"
      },
      "outputs": [],
      "source": [
        "# Agregar start, end y pad a cada frase del set de entrenamiento\n",
        "#encoder_tokens = [['<START>'] + tokens + ['<END>'] for tokens in source_tokens]\n",
        "encoder_tokens = [tokens for tokens in source_tokens]\n",
        "#decoder_tokens = [['<START>'] + tokens + ['<END>'] for tokens in target_tokens]\n",
        "decoder_tokens = [tokens for tokens in target_tokens]\n",
        "#output_tokens = [tokens + ['<END>'] for tokens in target_tokens]\n",
        "output_tokens = [tokens for tokens in target_tokens]\n",
        "\n",
        "source_max_len = max(map(len, encoder_tokens))\n",
        "target_max_len = max(map(len, decoder_tokens))\n",
        "\n",
        "#encoder_tokens = [tokens + ['<PAD>']*(source_max_len-len(tokens)) for tokens in encoder_tokens]\n",
        "#decoder_tokens = [tokens + ['<PAD>']*(target_max_len-len(tokens)) for tokens in decoder_tokens]\n",
        "#output_tokens = [tokens + ['<PAD>']*(target_max_len-len(tokens)) for tokens in output_tokens ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcMGqShN6cLZ",
        "outputId": "a4a24a58-4b36-459b-e76a-9e46c8880d65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['la', 'reina', 'de', 'Inglaterra', 'es', 'una', 'mujer']\n"
          ]
        }
      ],
      "source": [
        "print(encoder_tokens[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "9KS6KDyq6h14"
      },
      "outputs": [],
      "source": [
        "encoder_input = [list(map(lambda x: source_token_dict[x], tokens)) for tokens in encoder_tokens]\n",
        "decoder_input = [list(map(lambda x: target_token_dict[x], tokens)) for tokens in decoder_tokens]\n",
        "output_decoded = [list(map(lambda x: [target_token_dict[x]], tokens)) for tokens in output_tokens]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AvSmG0ZER9V",
        "outputId": "0ee77930-a124-4ee6-ed9f-e371bd7f5590"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7, 8, 2, 3, 4, 9, 10]\n"
          ]
        }
      ],
      "source": [
        "print(encoder_input[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Re0auX7F-5kA"
      },
      "source": [
        "Entrenamos el Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mof4mwjW6i7X",
        "outputId": "a90d64ba-4435-41c8-f688-793ca0445b88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13\n",
            "7\n",
            "6\n"
          ]
        }
      ],
      "source": [
        "vocab_size = len(source_token_dict)\n",
        "print(vocab_size)\n",
        "print(target_max_len)\n",
        "print(len(decoder_input))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {
        "id": "nDGZFqVG_JqH"
      },
      "outputs": [],
      "source": [
        "embedding = Embedding(vocab_size, source_max_len, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smPhBX2g_dbt",
        "outputId": "8a64ed8a-4c6b-44cb-dbfe-9767218b6b41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss:  0.29111866769006334\n",
            "Loss:  0.29067112603138173\n",
            "Loss:  0.2902212697797371\n",
            "Loss:  0.28977216859945604\n",
            "Loss:  0.2893239173141066\n",
            "Loss:  0.28887647542157585\n",
            "Loss:  0.28842978025532623\n",
            "Loss:  0.28798376156781974\n",
            "Loss:  0.28753834585758814\n",
            "Loss:  0.2870934581810443\n",
            "Loss:  0.28664902300536116\n",
            "Loss:  0.28620387102804506\n",
            "Loss:  0.28575700933861425\n",
            "Loss:  0.28531031942156054\n",
            "Loss:  0.2848635872809829\n",
            "Loss:  0.284416663800055\n",
            "Loss:  0.2839694324114233\n",
            "Loss:  0.28351764463340584\n",
            "Loss:  0.28305641246862173\n",
            "Loss:  0.28259042745318763\n",
            "Loss:  0.2821202577752859\n",
            "Loss:  0.28164832113492255\n",
            "Loss:  0.2811742362378817\n",
            "Loss:  0.28069769707024256\n",
            "Loss:  0.2802184685668562\n",
            "Loss:  0.2797363545663878\n",
            "Loss:  0.2792511831137599\n",
            "Loss:  0.2787627991178792\n",
            "Loss:  0.278271060267176\n",
            "Loss:  0.2777728766228917\n",
            "Loss:  0.27726640842541944\n",
            "Loss:  0.276748784861343\n",
            "Loss:  0.27622471862696835\n",
            "Loss:  0.27569588136481715\n",
            "Loss:  0.27516195552018935\n",
            "Loss:  0.27462271724385107\n",
            "Loss:  0.2740780264718492\n",
            "Loss:  0.27352791802700066\n",
            "Loss:  0.27296944150888863\n",
            "Loss:  0.2723982868991026\n",
            "Loss:  0.2718031460927699\n",
            "Loss:  0.2712005649980133\n",
            "Loss:  0.2705911784790062\n",
            "Loss:  0.2699743032262932\n",
            "Loss:  0.2693496086575568\n",
            "Loss:  0.26871552411837246\n",
            "Loss:  0.2680590835889038\n",
            "Loss:  0.26739096278476\n",
            "Loss:  0.2667144831311857\n",
            "Loss:  0.2660278181034464\n",
            "Loss:  0.26532733513069595\n",
            "Loss:  0.26461347745786157\n",
            "Loss:  0.26388979487256087\n",
            "Loss:  0.26315638614744286\n",
            "Loss:  0.2624132323395442\n",
            "Loss:  0.26166038275558035\n",
            "Loss:  0.26089792916009913\n",
            "Loss:  0.2601259921363788\n",
            "Loss:  0.25934042128729135\n",
            "Loss:  0.25853879657357876\n",
            "Loss:  0.25772780074802776\n",
            "Loss:  0.2569074682178292\n",
            "Loss:  0.2560778619000214\n",
            "Loss:  0.25523912604109333\n",
            "Loss:  0.25439144321234575\n",
            "Loss:  0.2535350168736258\n",
            "Loss:  0.2526666663207508\n",
            "Loss:  0.2517762042321941\n",
            "Loss:  0.2508772765445961\n",
            "Loss:  0.24997010166629322\n",
            "Loss:  0.2490547006216427\n",
            "Loss:  0.24813120964345473\n",
            "Loss:  0.2471998185943037\n",
            "Loss:  0.2462607428574292\n",
            "Loss:  0.24531421102901332\n",
            "Loss:  0.24436045869498466\n",
            "Loss:  0.24339972487622516\n",
            "Loss:  0.24243224980552677\n",
            "Loss:  0.24145827344310544\n",
            "Loss:  0.2404780344383367\n",
            "Loss:  0.23949176938058583\n",
            "Loss:  0.2384997122481989\n",
            "Loss:  0.23750209399954417\n",
            "Loss:  0.23649914226949367\n",
            "Loss:  0.23549108114627307\n",
            "Loss:  0.23447813101078727\n",
            "Loss:  0.23346050842520602\n",
            "Loss:  0.23243842606076556\n",
            "Loss:  0.23141209265698004\n",
            "Loss:  0.23038171300608326\n",
            "Loss:  0.2293474879577413\n",
            "Loss:  0.22830961444001274\n",
            "Loss:  0.22726828549326752\n",
            "Loss:  0.22622369031435988\n",
            "Loss:  0.22517601430882186\n",
            "Loss:  0.22412543914923155\n",
            "Loss:  0.2230721428382264\n",
            "Loss:  0.22201629977489465\n",
            "Loss:  0.22095808082349877\n",
            "Loss:  0.21989765338366804\n",
            "Loss:  0.21883518146134948\n",
            "Loss:  0.21777082573993814\n",
            "Loss:  0.21670474365111594\n",
            "Loss:  0.2156370894450175\n",
            "Loss:  0.2145680142594227\n",
            "Loss:  0.21349766618773702\n",
            "Loss:  0.21242619034557586\n",
            "Loss:  0.21135372893581694\n",
            "Loss:  0.2102804213120195\n",
            "Loss:  0.20920640404014326\n",
            "Loss:  0.20813181095852684\n",
            "Loss:  0.20705677323610483\n",
            "Loss:  0.20598141942886314\n",
            "Loss:  0.20490587553454578\n",
            "Loss:  0.2038302650456385\n",
            "Loss:  0.20275470900066309\n",
            "Loss:  0.2016793260338261\n",
            "Loss:  0.20060423242306868\n",
            "Loss:  0.199529542136571\n",
            "Loss:  0.1984553668777663\n",
            "Loss:  0.19738181612892244\n",
            "Loss:  0.1963089971933505\n",
            "Loss:  0.19523701523629974\n",
            "Loss:  0.1941659733245994\n",
            "Loss:  0.19309597246510582\n",
            "Loss:  0.19202711164201455\n",
            "Loss:  0.19095948785309394\n",
            "Loss:  0.1898931961448973\n",
            "Loss:  0.18882832964700733\n",
            "Loss:  0.18776497960536556\n",
            "Loss:  0.18670323541473857\n",
            "Loss:  0.18564318465036833\n",
            "Loss:  0.18458491309885527\n",
            "Loss:  0.18352850478831695\n",
            "Loss:  0.18247404201786677\n",
            "Loss:  0.18142160538645155\n",
            "Loss:  0.1803712738210872\n",
            "Loss:  0.17932312460452868\n",
            "Loss:  0.17827723340240742\n",
            "Loss:  0.17723367428986894\n",
            "Loss:  0.1761925197777394\n",
            "Loss:  0.17515384083824959\n",
            "Loss:  0.17411770693034048\n",
            "Loss:  0.17308418602457487\n",
            "Loss:  0.17205334462767466\n",
            "Loss:  0.1710252478067038\n",
            "Loss:  0.16999995921291325\n",
            "Loss:  0.16897754110526145\n",
            "Loss:  0.16795805437362357\n",
            "Loss:  0.1669415585616991\n",
            "Loss:  0.1659281118896242\n",
            "Loss:  0.16491777127629512\n",
            "Loss:  0.16391059236140448\n",
            "Loss:  0.1629066295271898\n",
            "Loss:  0.1619059359198921\n",
            "Loss:  0.16090856347091864\n",
            "Loss:  0.15991456291770043\n",
            "Loss:  0.15892398382423345\n",
            "Loss:  0.157936874601288\n",
            "Loss:  0.15695328252626792\n",
            "Loss:  0.15597325376269705\n",
            "Loss:  0.154996833379308\n",
            "Loss:  0.15402406536870258\n",
            "Loss:  0.1530549926655506\n",
            "Loss:  0.1520896571642885\n",
            "Loss:  0.1511280997362767\n",
            "Loss:  0.15017036024636696\n",
            "Loss:  0.14921647756883016\n",
            "Loss:  0.14826648960258734\n",
            "Loss:  0.14732043328568453\n",
            "Loss:  0.14637834460894594\n",
            "Loss:  0.1454402586287363\n",
            "Loss:  0.1445062094787601\n",
            "Loss:  0.14357623038082049\n",
            "Loss:  0.14265035365445825\n",
            "Loss:  0.1417286107253897\n",
            "Loss:  0.1408110321326603\n",
            "Loss:  0.13989764753443065\n",
            "Loss:  0.13898848571231287\n",
            "Loss:  0.1380835745741793\n",
            "Loss:  0.13718294115536825\n",
            "Loss:  0.13628661161822034\n",
            "Loss:  0.13539461124988808\n",
            "Loss:  0.134506964458373\n",
            "Loss:  0.13362369476676111\n",
            "Loss:  0.1327448248056451\n",
            "Loss:  0.13187037630374518\n",
            "Loss:  0.13100037007676482\n",
            "Loss:  0.13013482601454782\n",
            "Loss:  0.12927376306663618\n",
            "Loss:  0.1284171992263641\n",
            "Loss:  0.1275651515136622\n",
            "Loss:  0.12671763595678756\n",
            "Loss:  0.12587466757323856\n",
            "Loss:  0.12503626035015536\n",
            "Loss:  0.12420242722455037\n",
            "Loss:  0.12337318006375309\n",
            "Loss:  0.12254852964649075\n",
            "Loss:  0.12172848564505599\n",
            "Loss:  0.12091305660903923\n",
            "Loss:  0.12010224995111707\n",
            "Loss:  0.11929607193539331\n",
            "Loss:  0.11849452766878237\n",
            "Loss:  0.11769762109590577\n",
            "Loss:  0.11690535499793664\n",
            "Loss:  0.1161177309957821\n",
            "Loss:  0.11533474955793104\n",
            "Loss:  0.11455641001322125\n",
            "Loss:  0.11378271056869525\n",
            "Loss:  0.11301364833262106\n",
            "Loss:  0.11224921934265321\n",
            "Loss:  0.11148941859900625\n",
            "Loss:  0.11073424010241102\n",
            "Loss:  0.10998367689652196\n",
            "Loss:  0.10923772111435293\n",
            "Loss:  0.10849636402823405\n",
            "Loss:  0.10775959610271361\n",
            "Loss:  0.10702740704977036\n",
            "Loss:  0.10629978588566567\n",
            "Loss:  0.10557672098873926\n",
            "Loss:  0.10485820015744861\n",
            "Loss:  0.10414421066796363\n",
            "Loss:  0.10343473933065377\n",
            "Loss:  0.10272977254484593\n",
            "Loss:  0.10202929635128381\n",
            "Loss:  0.10133329648177941\n",
            "Loss:  0.10064175840561618\n",
            "Loss:  0.09995466737233566\n",
            "Loss:  0.09927200845061379\n",
            "Loss:  0.09859376656300563\n",
            "Loss:  0.09791992651641201\n",
            "Loss:  0.09725047302818772\n",
            "Loss:  0.09658539074787646\n",
            "Loss:  0.09592466427461528\n",
            "Loss:  0.09526827817030427\n",
            "Loss:  0.0946162169686829\n",
            "Loss:  0.09396846518049567\n",
            "Loss:  0.09332500729496278\n",
            "Loss:  0.09268582777780042\n",
            "Loss:  0.0920509110660595\n",
            "Loss:  0.09142024156006961\n",
            "Loss:  0.09079380361279112\n",
            "Loss:  0.09017158151688838\n",
            "Loss:  0.08955355948984695\n",
            "Loss:  0.08893972165746171\n",
            "Loss:  0.08833005203602791\n",
            "Loss:  0.0877245345135667\n",
            "Loss:  0.08712315283041722\n",
            "Loss:  0.0865258905595233\n",
            "Loss:  0.08593273108673923\n",
            "Loss:  0.08534365759147061\n",
            "Loss:  0.0847586530279586\n",
            "Loss:  0.08417770010750293\n",
            "Loss:  0.08360078128190594\n",
            "Loss:  0.08302787872840163\n",
            "Loss:  0.0824589743363155\n",
            "Loss:  0.08189404969567621\n",
            "Loss:  0.0813330860879768\n",
            "Loss:  0.08077606447925285\n",
            "Loss:  0.08022296551561459\n",
            "Loss:  0.07967376952133783\n",
            "Loss:  0.07912845649958221\n",
            "Loss:  0.07858700613576938\n",
            "Loss:  0.07804939780361732\n",
            "Loss:  0.07751561057378908\n",
            "Loss:  0.07698562322507857\n",
            "Loss:  0.07645941425802046\n",
            "Loss:  0.07593696191077866\n",
            "Loss:  0.07541824417713797\n",
            "Loss:  0.07490323882639625\n",
            "Loss:  0.07439192342493196\n",
            "Loss:  0.07388427535920399\n",
            "Loss:  0.07338027185992617\n",
            "Loss:  0.07287989002715066\n",
            "Loss:  0.07238310685599052\n",
            "Loss:  0.07188989926271143\n",
            "Loss:  0.07140024411092995\n",
            "Loss:  0.07091411823766319\n",
            "Loss:  0.07043149847898969\n",
            "Loss:  0.06995236169509701\n",
            "Loss:  0.06947668479451148\n",
            "Loss:  0.06900444475732674\n",
            "Loss:  0.06853561865727097\n",
            "Loss:  0.06807018368247637\n",
            "Loss:  0.06760811715483857\n",
            "Loss:  0.06714939654787826\n",
            "Loss:  0.0666939995030404\n",
            "Loss:  0.06624190384438863\n",
            "Loss:  0.06579308759167402\n",
            "Loss:  0.06534752897177591\n",
            "Loss:  0.0649052064285315\n",
            "Loss:  0.06446609863098417\n",
            "Loss:  0.06403018448009722\n",
            "Loss:  0.06359744311398795\n",
            "Loss:  0.06316785391174953\n",
            "Loss:  0.06274139649593324\n",
            "Loss:  0.06231805073377209\n",
            "Loss:  0.06189779673722858\n",
            "Loss:  0.06148061486195405\n",
            "Loss:  0.06106648570524723\n",
            "Loss:  0.060655390103100636\n",
            "Loss:  0.06024730912642181\n",
            "Loss:  0.05984222407651541\n",
            "Loss:  0.0594401164799099\n",
            "Loss:  0.05904096808260841\n",
            "Loss:  0.05864476084384196\n",
            "Loss:  0.05825147692939761\n",
            "Loss:  0.05786109870459157\n",
            "Loss:  0.05747360872695232\n",
            "Loss:  0.05708898973867604\n",
            "Loss:  0.05670722465891045\n",
            "Loss:  0.056328296575921886\n",
            "Loss:  0.055952188739194275\n",
            "Loss:  0.05557888455150652\n",
            "Loss:  0.05520836756103035\n",
            "Loss:  0.054840621453488114\n",
            "Loss:  0.05447563004440661\n",
            "Loss:  0.054113377271500014\n",
            "Loss:  0.05375384718721293\n",
            "Loss:  0.05339702395145161\n",
            "Loss:  0.053042891824529516\n",
            "Loss:  0.052691435160351664\n",
            "Loss:  0.05234263839985966\n",
            "Loss:  0.051996486064758755\n",
            "Loss:  0.05165296275154577\n",
            "Loss:  0.05131205312585615\n",
            "Loss:  0.050973741917146646\n",
            "Loss:  0.05063801391372879\n",
            "Loss:  0.05030485395816751\n",
            "Loss:  0.04997424694305758\n",
            "Loss:  0.04964617780718896\n",
            "Loss:  0.04932063153211141\n",
            "Loss:  0.0489975931391057\n",
            "Loss:  0.04867704768656825\n",
            "Loss:  0.048358980267813094\n",
            "Loss:  0.048043376009292864\n",
            "Loss:  0.04773022006923814\n",
            "Loss:  0.047419497636711584\n",
            "Loss:  0.047111193931070466\n",
            "Loss:  0.04680529420182755\n",
            "Loss:  0.04650178372889786\n",
            "Loss:  0.04620064782321475\n",
            "Loss:  0.045901871827695756\n",
            "Loss:  0.04560544111853532\n",
            "Loss:  0.045311341106798526\n",
            "Loss:  0.04501955724028669\n",
            "Loss:  0.044730075005643605\n",
            "Loss:  0.04444287993066842\n",
            "Loss:  0.044157957586800016\n",
            "Loss:  0.04387529359173561\n",
            "Loss:  0.04359487361214651\n",
            "Loss:  0.04331668336645266\n",
            "Loss:  0.04304070862761881\n",
            "Loss:  0.04276693522593544\n",
            "Loss:  0.04249534905174909\n",
            "Loss:  0.0422259360581085\n",
            "Loss:  0.04195868226329567\n",
            "Loss:  0.04169357375321273\n",
            "Loss:  0.04143059668359909\n",
            "Loss:  0.0411697372820563\n",
            "Loss:  0.04091098184986142\n",
            "Loss:  0.040654316763552766\n",
            "Loss:  0.04039972847627593\n",
            "Loss:  0.0401472035188809\n",
            "Loss:  0.039896728500764395\n",
            "Loss:  0.03964829011045537\n",
            "Loss:  0.03940187511594385\n",
            "Loss:  0.03915747036475666\n",
            "Loss:  0.03891506278378674\n",
            "Loss:  0.038674639378884004\n",
            "Loss:  0.038436187234219675\n",
            "Loss:  0.03819969351143679\n",
            "Loss:  0.03796514544860236\n",
            "Loss:  0.037732530358978465\n",
            "Loss:  0.037501835629630914\n",
            "Loss:  0.03727304871989571\n",
            "Loss:  0.03704615715972546\n",
            "Loss:  0.03682114854793823\n",
            "Loss:  0.03659801055039303\n",
            "Loss:  0.036376730898117014\n",
            "Loss:  0.03615729738540952\n",
            "Loss:  0.03593969786794934\n",
            "Loss:  0.0357239202609309\n",
            "Loss:  0.03550995253725594\n",
            "Loss:  0.035297782725805235\n",
            "Loss:  0.03508739890981547\n",
            "Loss:  0.03487878922538421\n",
            "Loss:  0.03467194186012415\n",
            "Loss:  0.03446684505198582\n",
            "Loss:  0.03426348708826499\n",
            "Loss:  0.03406185630480794\n",
            "Loss:  0.033861941085424424\n",
            "Loss:  0.03366372986151384\n",
            "Loss:  0.033467211111906675\n",
            "Loss:  0.03327237336291856\n",
            "Loss:  0.033079205188610246\n",
            "Loss:  0.03288769521124269\n",
            "Loss:  0.032697832101912544\n",
            "Loss:  0.032509604581349605\n",
            "Loss:  0.0323230014208549\n",
            "Loss:  0.03213801144335516\n",
            "Loss:  0.03195462352454793\n",
            "Loss:  0.03177282659410935\n",
            "Loss:  0.0315926096369376\n",
            "Loss:  0.031413961694402796\n",
            "Loss:  0.031236871865576887\n",
            "Loss:  0.031061329308417252\n",
            "Loss:  0.030887323240879255\n",
            "Loss:  0.030714842941936233\n",
            "Loss:  0.030543877752486893\n",
            "Loss:  0.030374417076133465\n",
            "Loss:  0.03020645037981635\n",
            "Loss:  0.03003996719429466\n",
            "Loss:  0.029874957114464046\n",
            "Loss:  0.029711409799506833\n",
            "Loss:  0.029549314972871454\n",
            "Loss:  0.02938866242208099\n",
            "Loss:  0.029229441998372744\n",
            "Loss:  0.029071643616172548\n",
            "Loss:  0.02891525725240939\n",
            "Loss:  0.02876027294567746\n",
            "Loss:  0.028606680795253456\n",
            "Loss:  0.028454470959979213\n",
            "Loss:  0.028303633657018892\n",
            "Loss:  0.02815415916050246\n",
            "Loss:  0.028006037800066635\n",
            "Loss:  0.0278592599593053\n",
            "Loss:  0.027713816074142365\n",
            "Loss:  0.027569696631139586\n",
            "Loss:  0.02742689216575318\n",
            "Loss:  0.02728539326055272\n",
            "Loss:  0.027145190543416367\n",
            "Loss:  0.027006274685716775\n",
            "Loss:  0.02686863640051247\n",
            "Loss:  0.02673226644075913\n",
            "Loss:  0.02659715559755559\n",
            "Loss:  0.02646329469843968\n",
            "Loss:  0.02633067460574794\n",
            "Loss:  0.026199286215053946\n",
            "Loss:  0.02606912045369865\n",
            "Loss:  0.025940168279425992\n",
            "Loss:  0.02581242067913554\n",
            "Loss:  0.025685868667763522\n",
            "Loss:  0.025560503287301414\n",
            "Loss:  0.025436315605960363\n",
            "Loss:  0.025313296717487124\n",
            "Loss:  0.025191437740636976\n",
            "Loss:  0.02507072981880415\n",
            "Loss:  0.024951164119811523\n",
            "Loss:  0.024832731835856516\n",
            "Loss:  0.02471542418360932\n",
            "Loss:  0.02459923240445749\n",
            "Loss:  0.024484147764888425\n",
            "Loss:  0.02437016155700011\n",
            "Loss:  0.024257265099128705\n",
            "Loss:  0.024145449736580723\n",
            "Loss:  0.024034706842455327\n",
            "Loss:  0.02392502781854411\n",
            "Loss:  0.023816404096292165\n",
            "Loss:  0.023708827137807215\n",
            "Loss:  0.023602288436901247\n",
            "Loss:  0.023496779520151806\n",
            "Loss:  0.023392291947968925\n",
            "Loss:  0.02328881731565574\n",
            "Loss:  0.02318634725445122\n",
            "Loss:  0.023084873432544898\n",
            "Loss:  0.02298438755605453\n",
            "Loss:  0.0228848813699587\n",
            "Loss:  0.022786346658978166\n",
            "Loss:  0.02268877524839999\n",
            "Loss:  0.022592159004840826\n",
            "Loss:  0.022496489836945896\n",
            "Loss:  0.022401759696021716\n",
            "Loss:  0.02230796057660192\n",
            "Loss:  0.022215084516945288\n",
            "Loss:  0.022123123599467817\n",
            "Loss:  0.022032069951109623\n",
            "Loss:  0.02194191574363921\n",
            "Loss:  0.021852653193898175\n",
            "Loss:  0.021764274563990122\n",
            "Loss:  0.02167677216141751\n",
            "Loss:  0.021590138339171422\n",
            "Loss:  0.02150436549577915\n",
            "Loss:  0.021419446075315055\n",
            "Loss:  0.021335372567380526\n",
            "Loss:  0.021252137507058625\n",
            "Loss:  0.021169733474849874\n",
            "Loss:  0.02108815309659477\n",
            "Loss:  0.021007389043389318\n",
            "Loss:  0.02092743403149955\n",
            "Loss:  0.02084828082228012\n",
            "Loss:  0.02076992222210265\n",
            "Loss:  0.020692351082298343\n",
            "Loss:  0.02061556029911942\n",
            "Loss:  0.02053954281372288\n",
            "Loss:  0.020464291612179435\n",
            "Loss:  0.02038979972551018\n",
            "Loss:  0.020316060229752464\n",
            "Loss:  0.020243066246055586\n",
            "Loss:  0.02017081094080629\n",
            "Loss:  0.020099287525783577\n",
            "Loss:  0.020028489258340765\n",
            "Loss:  0.019958409441613065\n",
            "Loss:  0.019889041424747414\n",
            "Loss:  0.019820378603151372\n",
            "Loss:  0.019752414418757316\n",
            "Loss:  0.01968514236029712\n",
            "Loss:  0.01961855596358356\n",
            "Loss:  0.01955264881179292\n",
            "Loss:  0.019487414535744704\n",
            "Loss:  0.019422846814173157\n",
            "Loss:  0.019358939373986188\n",
            "Loss:  0.019295685990507423\n",
            "Loss:  0.019233080487696726\n",
            "Loss:  0.019171116738345973\n",
            "Loss:  0.019109788664245963\n",
            "Loss:  0.019049090236322234\n",
            "Loss:  0.018989015474736873\n",
            "Loss:  0.018929558448954036\n",
            "Loss:  0.018870713277768448\n",
            "Loss:  0.01881247412929497\n",
            "Loss:  0.018754835220919395\n",
            "Loss:  0.018697790819209664\n",
            "Loss:  0.01864133523978869\n",
            "Loss:  0.01858546284716861\n",
            "Loss:  0.01853016805454814\n",
            "Loss:  0.018475445323574074\n",
            "Loss:  0.018421289164068697\n",
            "Loss:  0.01836769413372497\n",
            "Loss:  0.018314654837771404\n",
            "Loss:  0.018262165928608815\n",
            "Loss:  0.018210222105421423\n",
            "Loss:  0.01815881811376424\n",
            "Loss:  0.0181079487451296\n",
            "Loss:  0.01805760883649472\n",
            "Loss:  0.018007793269852704\n",
            "Loss:  0.01795849697172974\n",
            "Loss:  0.017909714912689948\n",
            "Loss:  0.01786144210683051\n",
            "Loss:  0.017813673611269017\n",
            "Loss:  0.01776640452562473\n",
            "Loss:  0.017719629991495844\n",
            "Loss:  0.017673345191934132\n",
            "Loss:  0.01762754535091866\n",
            "Loss:  0.01758222573283003\n",
            "Loss:  0.01753738164192636\n",
            "Loss:  0.017493008421822064\n",
            "Loss:  0.017449101454970726\n",
            "Loss:  0.017405656162152828\n",
            "Loss:  0.01736266800196913\n",
            "Loss:  0.017320132470340557\n",
            "Loss:  0.017278045100015106\n",
            "Loss:  0.01723640146008218\n",
            "Loss:  0.017195197155495376\n",
            "Loss:  0.01715442782660294\n",
            "Loss:  0.017114089148687617\n",
            "Loss:  0.01707417683151471\n",
            "Loss:  0.017034686618889943\n",
            "Loss:  0.016995614288225627\n",
            "Loss:  0.016956955650116855\n",
            "Loss:  0.016918706547926307\n",
            "Loss:  0.01688086285737848\n",
            "Loss:  0.01684342048616329\n",
            "Loss:  0.016806375373548208\n",
            "Loss:  0.016769723489999864\n",
            "Loss:  0.01673346083681426\n",
            "Loss:  0.016697583445755525\n",
            "Loss:  0.01666208737870325\n",
            "Loss:  0.01662696872730821\n",
            "Loss:  0.016592223612656012\n",
            "Loss:  0.016557848184938773\n",
            "Loss:  0.01652383862313441\n",
            "Loss:  0.01649019113469348\n",
            "Loss:  0.01645690195523341\n",
            "Loss:  0.016423967348239258\n",
            "Loss:  0.016391383604772044\n",
            "Loss:  0.016359147043182996\n",
            "Loss:  0.016327254008834658\n",
            "Loss:  0.016295700873828086\n",
            "Loss:  0.01626448403673594\n",
            "Loss:  0.016233599922341153\n",
            "Loss:  0.016203044981381653\n",
            "Loss:  0.01617281569030012\n",
            "Loss:  0.016142908550998834\n",
            "Loss:  0.01611332009059998\n",
            "Loss:  0.01608404686121019\n",
            "Loss:  0.01605508543969005\n",
            "Loss:  0.01602643242742836\n",
            "Loss:  0.01599808445011999\n",
            "Loss:  0.015970038157548497\n",
            "Loss:  0.015942290223372527\n",
            "Loss:  0.01591483734491614\n",
            "Loss:  0.015887676242962685\n",
            "Loss:  0.015860803661552594\n",
            "Loss:  0.015834216367784218\n",
            "Loss:  0.01580791115161843\n",
            "Loss:  0.01578188482568618\n",
            "Loss:  0.015756134225099163\n",
            "Loss:  0.015730656207263626\n",
            "Loss:  0.015705447651696985\n",
            "Loss:  0.01568050545984725\n",
            "Loss:  0.015655826554915254\n",
            "Loss:  0.015631407881679372\n",
            "Loss:  0.015607246406322729\n",
            "Loss:  0.015583339116263126\n",
            "Loss:  0.015559683019985069\n",
            "Loss:  0.015536275146874305\n",
            "Loss:  0.015513112547054453\n",
            "Loss:  0.015490192291225856\n",
            "Loss:  0.015467511470506559\n",
            "Loss:  0.015445067196275211\n",
            "Loss:  0.01542285660001625\n",
            "Loss:  0.015400876833166677\n",
            "Loss:  0.015379125066964846\n",
            "Loss:  0.015357598492301205\n",
            "Loss:  0.01533629431957072\n",
            "Loss:  0.01531520977852687\n",
            "Loss:  0.015294342118137698\n",
            "Loss:  0.015273688606443162\n",
            "Loss:  0.01525324653041441\n",
            "Loss:  0.015233013195814387\n",
            "Loss:  0.015212985927060047\n",
            "Loss:  0.015193162067086171\n",
            "Loss:  0.015173538977210658\n",
            "Loss:  0.01515411403700104\n",
            "Loss:  0.015134884644142704\n",
            "Loss:  0.015115848214308234\n",
            "Loss:  0.015097002181028442\n",
            "Loss:  0.015078343995564384\n",
            "Loss:  0.015059871126780857\n",
            "Loss:  0.015041581061021145\n",
            "Loss:  0.015023471301983088\n",
            "Loss:  0.015005539370596205\n",
            "Loss:  0.014987782804900302\n",
            "Loss:  0.014970199159925127\n",
            "Loss:  0.01495278600757121\n",
            "Loss:  0.01493554093649174\n",
            "Loss:  0.014918461551975848\n",
            "Loss:  0.014901545475832834\n",
            "Loss:  0.014884790346277587\n",
            "Loss:  0.01486819381781704\n",
            "Loss:  0.014851753561137776\n",
            "Loss:  0.014835467262994532\n",
            "Loss:  0.014819332626100105\n",
            "Loss:  0.014803347369015965\n",
            "Loss:  0.01478750922604409\n",
            "Loss:  0.014771815947119883\n",
            "Loss:  0.014756265297705929\n",
            "Loss:  0.014740855058686888\n",
            "Loss:  0.014725583026265523\n",
            "Loss:  0.014710447011859555\n",
            "Loss:  0.014695444841999632\n",
            "Loss:  0.014680574358228393\n",
            "Loss:  0.014665833417000368\n",
            "Loss:  0.014651219889583086\n",
            "Loss:  0.014636731661959182\n",
            "Loss:  0.014622366634729567\n",
            "Loss:  0.014608122723017572\n",
            "Loss:  0.014593997856374134\n",
            "Loss:  0.014579989978684454\n",
            "Loss:  0.01456609704807522\n",
            "Loss:  0.014552317036823442\n",
            "Loss:  0.014538647931265997\n",
            "Loss:  0.014525087731710852\n",
            "Loss:  0.014511634452349072\n",
            "Loss:  0.014498286121168373\n",
            "Loss:  0.014485040779867791\n",
            "Loss:  0.014471896483773653\n",
            "Loss:  0.014458851301757178\n",
            "Loss:  0.014445903316153136\n",
            "Loss:  0.014433050622680252\n",
            "Loss:  0.014420291330362912\n",
            "Loss:  0.014407623561454622\n",
            "Loss:  0.014395045451362882\n",
            "Loss:  0.014382555148576144\n",
            "Loss:  0.014370150814592093\n",
            "Loss:  0.014357830623847975\n",
            "Loss:  0.014345592763652812\n",
            "Loss:  0.014333435434121652\n",
            "Loss:  0.014321356848111722\n",
            "Loss:  0.01430935523116105\n",
            "Loss:  0.014297428821428959\n",
            "Loss:  0.01428557586963916\n",
            "Loss:  0.01427379463902527\n",
            "Loss:  0.014262083405278653\n",
            "Loss:  0.014250440456499097\n",
            "Loss:  0.014238864093148297\n",
            "Loss:  0.014227352628006023\n",
            "Loss:  0.014215904386129274\n",
            "Loss:  0.014204517704814734\n",
            "Loss:  0.014193190933564177\n",
            "Loss:  0.01418192243405345\n",
            "Loss:  0.01417071058010462\n",
            "Loss:  0.014159553757661996\n",
            "Loss:  0.014148450364771728\n",
            "Loss:  0.014137398811565198\n",
            "Loss:  0.014126397520246652\n",
            "Loss:  0.014115444925084555\n",
            "Loss:  0.01410453947240739\n",
            "Loss:  0.014093679620603884\n",
            "Loss:  0.014082863840127705\n",
            "Loss:  0.014072090613506791\n",
            "Loss:  0.014061358435357467\n",
            "Loss:  0.01405066581240326\n",
            "Loss:  0.014040011263499309\n",
            "Loss:  0.014029393319661264\n",
            "Loss:  0.014018810524099886\n",
            "Loss:  0.014008261432260999\n",
            "Loss:  0.01399774461187097\n",
            "Loss:  0.01398725864298783\n",
            "Loss:  0.013976802118057989\n",
            "Loss:  0.013966373641979066\n",
            "Loss:  0.013955971832168452\n",
            "Loss:  0.013945595318638154\n",
            "Loss:  0.013935242744075515\n",
            "Loss:  0.01392491276393047\n",
            "Loss:  0.013914604046508705\n",
            "Loss:  0.013904315273071392\n",
            "Loss:  0.013894045137941175\n",
            "Loss:  0.013883792348614803\n",
            "Loss:  0.013873555625881976\n",
            "Loss:  0.013863333703950731\n",
            "Loss:  0.013853125330579391\n",
            "Loss:  0.013842929267214822\n",
            "Loss:  0.013832744289137324\n",
            "Loss:  0.013822569185611783\n",
            "Loss:  0.01381240276004513\n",
            "Loss:  0.013802243830150293\n",
            "Loss:  0.013792091228116159\n",
            "Loss:  0.013781943800783672\n",
            "Loss:  0.013771800409827978\n",
            "Loss:  0.013761659931946354\n",
            "Loss:  0.01375152125905186\n",
            "Loss:  0.013741383298472505\n",
            "Loss:  0.013731244973155747\n",
            "Loss:  0.013721105221877907\n",
            "Loss:  0.01371096299945882\n",
            "Loss:  0.013700817276980871\n",
            "Loss:  0.013690667042012342\n",
            "Loss:  0.013680511298835019\n",
            "Loss:  0.013670349068675204\n",
            "Loss:  0.013660179389938484\n",
            "Loss:  0.013650001318447368\n",
            "Loss:  0.013639813927681528\n",
            "Loss:  0.013629616309020343\n",
            "Loss:  0.013619407571987324\n",
            "Loss:  0.013609186844495821\n",
            "Loss:  0.013598953273095708\n",
            "Loss:  0.01358870602322046\n",
            "Loss:  0.013578444279434438\n",
            "Loss:  0.013568167245679039\n",
            "Loss:  0.013557874145518243\n",
            "Loss:  0.0135475642223822\n",
            "Loss:  0.013537236739808596\n",
            "Loss:  0.013526890981681238\n",
            "Loss:  0.013516526252465147\n",
            "Loss:  0.013506141877437516\n",
            "Loss:  0.013495737202914254\n",
            "Loss:  0.013485311596470812\n",
            "Loss:  0.01347486444715723\n",
            "Loss:  0.013464395165706536\n",
            "Loss:  0.01345390318473605\n",
            "Loss:  0.0134433879589405\n",
            "Loss:  0.013432848965276403\n",
            "Loss:  0.01342228570313756\n",
            "Loss:  0.013411697694520419\n",
            "Loss:  0.013401084484178842\n",
            "Loss:  0.013390445639767642\n",
            "Loss:  0.01337978075197428\n",
            "Loss:  0.013369089434638048\n",
            "Loss:  0.01335837132485606\n",
            "Loss:  0.01334762608307579\n",
            "Loss:  0.013336853393172712\n",
            "Loss:  0.01332605296251387\n",
            "Loss:  0.013315224522005356\n",
            "Loss:  0.013304367826124113\n",
            "Loss:  0.01329348265293339\n",
            "Loss:  0.013282568804081367\n",
            "Loss:  0.013271626104782044\n",
            "Loss:  0.013260654403778912\n",
            "Loss:  0.013249653573290193\n",
            "Loss:  0.013238623508935942\n",
            "Loss:  0.013227564129646105\n",
            "Loss:  0.013216475377550002\n",
            "Loss:  0.01320535721784633\n",
            "Loss:  0.013194209638654008\n",
            "Loss:  0.013183032650843406\n",
            "Loss:  0.013171826287848127\n",
            "Loss:  0.013160590605457323\n",
            "Loss:  0.013149325681588173\n",
            "Loss:  0.013138031616038952\n",
            "Loss:  0.013126708530222806\n",
            "Loss:  0.013115356566882126\n",
            "Loss:  0.013103975889783724\n",
            "Loss:  0.013092566683395424\n",
            "Loss:  0.013081129152544176\n",
            "Loss:  0.013069663522055422\n",
            "Loss:  0.013058170036375267\n",
            "Loss:  0.013046648959174812\n",
            "Loss:  0.013035100572937698\n",
            "Loss:  0.01302352517853106\n",
            "Loss:  0.013011923094760717\n",
            "Loss:  0.013000294657910895\n",
            "Loss:  0.012988640221269339\n",
            "Loss:  0.012976960154638189\n",
            "Loss:  0.0129652548438318\n",
            "Loss:  0.01295352469016125\n",
            "Loss:  0.012941770109907408\n",
            "Loss:  0.012929991533782728\n",
            "Loss:  0.012918189406382325\n",
            "Loss:  0.012906364185625912\n",
            "Loss:  0.012894516342190286\n",
            "Loss:  0.012882646358934435\n",
            "Loss:  0.012870754730317208\n",
            "Loss:  0.012858841961808679\n",
            "Loss:  0.012846908569296116\n",
            "Loss:  0.012834955078485507\n",
            "Loss:  0.012822982024298974\n",
            "Loss:  0.012810989950269705\n",
            "Loss:  0.012798979407934763\n",
            "Loss:  0.012786950956226473\n",
            "Loss:  0.012774905160863798\n",
            "Loss:  0.012762842593743752\n",
            "Loss:  0.012750763832334623\n",
            "Loss:  0.012738669459070692\n",
            "Loss:  0.012726560060750092\n",
            "Loss:  0.012714436227936063\n",
            "Loss:  0.01270229855436264\n",
            "Loss:  0.012690147636345087\n",
            "Loss:  0.012677984072196271\n",
            "Loss:  0.012665808461648963\n",
            "Loss:  0.012653621405285337\n",
            "Loss:  0.012641423503973695\n",
            "Loss:  0.012629215358313685\n",
            "Loss:  0.012616997568089358\n",
            "Loss:  0.012604770731731827\n",
            "Loss:  0.012592535445791322\n",
            "Loss:  0.012580292304418981\n",
            "Loss:  0.012568041898859102\n",
            "Loss:  0.012555784816952027\n",
            "Loss:  0.01254352164264784\n",
            "Loss:  0.012531252955531718\n",
            "Loss:  0.012518979330360294\n",
            "Loss:  0.012506701336610087\n",
            "Loss:  0.012494419538037793\n",
            "Loss:  0.012482134492252593\n",
            "Loss:  0.012469846750300549\n",
            "Loss:  0.012457556856261645\n",
            "Loss:  0.012445265346858659\n",
            "Loss:  0.0124329727510789\n",
            "Loss:  0.012420679589807864\n",
            "Loss:  0.012408386375475804\n",
            "Loss:  0.012396093611716135\n",
            "Loss:  0.012383801793036239\n",
            "Loss:  0.012371511404500471\n",
            "Loss:  0.012359222921425219\n",
            "Loss:  0.012346936809085786\n",
            "Loss:  0.012334653522435075\n",
            "Loss:  0.012322373505833886\n",
            "Loss:  0.012310097192792654\n",
            "Loss:  0.012297825005724156\n",
            "Loss:  0.012285557355707714\n",
            "Loss:  0.012273294642263713\n",
            "Loss:  0.012261037253139094\n",
            "Loss:  0.012248785564102799\n",
            "Loss:  0.012236539938751753\n",
            "Loss:  0.012224300728326582\n",
            "Loss:  0.01221206827153698\n",
            "Loss:  0.012199842894396636\n",
            "Loss:  0.012187624910067458\n",
            "Loss:  0.01217541461871271\n",
            "Loss:  0.012163212307359214\n",
            "Loss:  0.012151018249768164\n",
            "Loss:  0.012138832706314573\n",
            "Loss:  0.012126655923874985\n",
            "Loss:  0.012114488135723888\n",
            "Loss:  0.012102329561438004\n",
            "Loss:  0.012090180406808928\n",
            "Loss:  0.012078040863764287\n",
            "Loss:  0.01206591111029672\n",
            "Loss:  0.012053791310401449\n",
            "Loss:  0.01204168161402249\n",
            "Loss:  0.012029582157007267\n",
            "Loss:  0.012017493061070299\n",
            "Loss:  0.012005414433766208\n",
            "Loss:  0.011993346368472338\n",
            "Loss:  0.011981288944381399\n",
            "Loss:  0.011969242226504995\n",
            "Loss:  0.011957206265688072\n",
            "Loss:  0.01194518109863578\n",
            "Loss:  0.011933166747952868\n",
            "Loss:  0.011921163222196869\n",
            "Loss:  0.011909170515946173\n",
            "Loss:  0.011897188609883808\n",
            "Loss:  0.01188521747089835\n",
            "Loss:  0.011873257052203476\n",
            "Loss:  0.011861307293477145\n",
            "Loss:  0.011849368121022552\n",
            "Loss:  0.011837439447952116\n",
            "Loss:  0.011825521174396378\n",
            "Loss:  0.011813613187739788\n",
            "Loss:  0.011801715362885455\n",
            "Loss:  0.01178982756255044\n",
            "Loss:  0.011777949637594293\n",
            "Loss:  0.011766081427382285\n",
            "Loss:  0.011754222760186367\n",
            "Loss:  0.011742373453624869\n",
            "Loss:  0.011730533315143815\n",
            "Loss:  0.011718702142541495\n",
            "Loss:  0.011706879724538063\n",
            "Loss:  0.011695065841391619\n",
            "Loss:  0.011683260265562401\n",
            "Loss:  0.011671462762425358\n",
            "Loss:  0.011659673091032587\n",
            "Loss:  0.0116478910049247\n",
            "Loss:  0.011636116252991\n",
            "Loss:  0.011624348580377128\n",
            "Loss:  0.011612587729438543\n",
            "Loss:  0.01160083344073623\n",
            "Loss:  0.011589085454071642\n",
            "Loss:  0.011577343509555818\n",
            "Loss:  0.011565607348706833\n",
            "Loss:  0.011553876715569122\n",
            "Loss:  0.011542151357846482\n",
            "Loss:  0.011530431028039773\n",
            "Loss:  0.011518715484579722\n",
            "Loss:  0.01150700449294341\n",
            "Loss:  0.011495297826742875\n",
            "Loss:  0.01148359526877383\n",
            "Loss:  0.011471896612010831\n",
            "Loss:  0.011460201660536504\n",
            "Loss:  0.011448510230391543\n",
            "Loss:  0.011436822150332723\n",
            "Loss:  0.011425137262487534\n",
            "Loss:  0.011413455422894028\n",
            "Loss:  0.01140177650191699\n",
            "Loss:  0.011390100384533211\n",
            "Loss:  0.011378426970480008\n",
            "Loss:  0.011366756174265065\n",
            "Loss:  0.011355087925037163\n",
            "Loss:  0.011343422166321573\n",
            "Loss:  0.011331758855625797\n",
            "Loss:  0.01132009796392551\n",
            "Loss:  0.0113084394750438\n",
            "Loss:  0.011296783384938515\n",
            "Loss:  0.01128512970091674\n",
            "Loss:  0.011273478440797078\n",
            "Loss:  0.01126182963204251\n",
            "Loss:  0.011250183310887821\n",
            "Loss:  0.011238539521486979\n",
            "Loss:  0.011226898315105495\n",
            "Loss:  0.011215259749382844\n",
            "Loss:  0.011203623887688287\n",
            "Loss:  0.011191990798592908\n",
            "Loss:  0.011180360555476568\n",
            "Loss:  0.0111687332362872\n",
            "Loss:  0.011157108923465506\n",
            "Loss:  0.011145487704044164\n",
            "Loss:  0.011133869669926707\n",
            "Loss:  0.011122254918346253\n",
            "Loss:  0.011110643552500106\n",
            "Loss:  0.011099035682350511\n",
            "Loss:  0.011087431425577743\n",
            "Loss:  0.011075830908666795\n",
            "Loss:  0.011064234268104213\n",
            "Loss:  0.01105264165165805\n",
            "Loss:  0.011041053219709981\n",
            "Loss:  0.011029469146606125\n",
            "Loss:  0.01101788962198994\n",
            "Loss:  0.011006314852080486\n",
            "Loss:  0.01099474506085799\n",
            "Loss:  0.01098318049111919\n",
            "Loss:  0.010971621405367052\n",
            "Loss:  0.01096006808650118\n",
            "Loss:  0.010948520838279916\n",
            "Loss:  0.010936979985528549\n",
            "Loss:  0.010925445874074124\n",
            "Loss:  0.010913918870393535\n",
            "Loss:  0.01088670234957477\n",
            "Loss:  0.010835155052628167\n",
            "Loss:  0.010780322883257485\n",
            "Loss:  0.010728972043940854\n",
            "Loss:  0.010678070220689012\n",
            "Loss:  0.010631380502913701\n",
            "Loss:  0.0105807565020788\n",
            "Loss:  0.010533219596248145\n",
            "Loss:  0.010486725271756673\n",
            "Loss:  0.010439240099277833\n",
            "Loss:  0.010393789951408931\n",
            "Loss:  0.010349430448267636\n",
            "Loss:  0.010303903560099218\n",
            "Loss:  0.010259804812330403\n",
            "Loss:  0.010215596348359236\n",
            "Loss:  0.010173377817193994\n",
            "Loss:  0.010128969550205503\n",
            "Loss:  0.010090594977270313\n",
            "Loss:  0.01004780462977718\n",
            "Loss:  0.01000723771952682\n",
            "Loss:  0.009966554187120816\n",
            "Loss:  0.009927114660104848\n",
            "Loss:  0.009887541011258468\n",
            "Loss:  0.009849161999952871\n",
            "Loss:  0.009810625553179431\n",
            "Loss:  0.009773290586180322\n",
            "Loss:  0.009735723915269131\n",
            "Loss:  0.009699418949624226\n",
            "Loss:  0.009662756863461045\n",
            "Loss:  0.009627469998057985\n",
            "Loss:  0.009591649413244037\n",
            "Loss:  0.009554178346998773\n",
            "Loss:  0.009523312678691142\n",
            "Loss:  0.009487911424233525\n",
            "Loss:  0.009451269214171849\n",
            "Loss:  0.00941919642132315\n",
            "Loss:  0.009384481974909606\n",
            "Loss:  0.00935326709980159\n",
            "Loss:  0.009319416885487016\n",
            "Loss:  0.009288933706066567\n",
            "Loss:  0.009255886623626917\n",
            "Loss:  0.009225434382219115\n",
            "Loss:  0.009194706598726912\n",
            "Loss:  0.009161462160839825\n",
            "Loss:  0.009133075018926034\n",
            "Loss:  0.009101823457676638\n",
            "Loss:  0.009073956610074497\n",
            "Loss:  0.009043423958626189\n",
            "Loss:  0.009015567457708444\n",
            "Loss:  0.008987194664748706\n",
            "Loss:  0.008956585423248198\n",
            "Loss:  0.008929608265152206\n",
            "Loss:  0.00890229760783346\n",
            "Loss:  0.008872679623460855\n",
            "Loss:  0.008847339299871816\n",
            "Loss:  0.008819363666606987\n",
            "Loss:  0.008790268604705727\n",
            "Loss:  0.0087661803269643\n",
            "Loss:  0.008739102551764856\n",
            "Loss:  0.008714702550741861\n",
            "Loss:  0.008689684025426114\n",
            "Loss:  0.0086623396821236\n",
            "Loss:  0.008635411671157368\n",
            "Loss:  0.008611474354948188\n",
            "Loss:  0.008585619514412404\n",
            "Loss:  0.008562252057263952\n",
            "Loss:  0.008536394260755758\n",
            "Loss:  0.00851511700789631\n",
            "Loss:  0.008490719714769666\n",
            "Loss:  0.008465163759347633\n",
            "Loss:  0.008444559471846039\n",
            "Loss:  0.008420893364721222\n",
            "Loss:  0.008396197550966445\n",
            "Loss:  0.00837616915372682\n",
            "Loss:  0.008353202183714417\n",
            "Loss:  0.008329278968529516\n",
            "Loss:  0.008309119479991594\n",
            "Loss:  0.008288424312975606\n",
            "Loss:  0.00826522060674685\n",
            "Loss:  0.008242131979174979\n",
            "Loss:  0.008222861396881907\n",
            "Loss:  0.008203024725374876\n",
            "Loss:  0.008180665685898867\n",
            "Loss:  0.008158408545761395\n",
            "Loss:  0.008140047677908694\n",
            "Loss:  0.008120943111090196\n",
            "Loss:  0.008099379241394788\n",
            "Loss:  0.008077905084421168\n",
            "Loss:  0.008056532725827655\n",
            "Loss:  0.008039939543900606\n",
            "Loss:  0.008020289550294444\n",
            "Loss:  0.007999612637422321\n",
            "Loss:  0.007978994844666727\n",
            "Loss:  0.00796312536584831\n",
            "Loss:  0.007944194372290122\n",
            "Loss:  0.007924221574776085\n",
            "Loss:  0.007904441723989962\n",
            "Loss:  0.007889122363561733\n",
            "Loss:  0.007870868503832078\n",
            "Loss:  0.007851559615192509\n",
            "Loss:  0.007832380508840732\n",
            "Loss:  0.00781711814504685\n",
            "Loss:  0.007801008513921777\n",
            "Loss:  0.007782392806706312\n",
            "Loss:  0.007763821699495661\n",
            "Loss:  0.00774541678180584\n",
            "Loss:  0.0077271239311626815\n",
            "Loss:  0.007713518299266894\n",
            "Loss:  0.007696825302389847\n",
            "Loss:  0.0076790460200503695\n",
            "Loss:  0.007661373392134216\n",
            "Loss:  0.0076437916199759036\n",
            "Loss:  0.007630914527692087\n",
            "Loss:  0.007614923651604819\n",
            "Loss:  0.0075978329373971795\n",
            "Loss:  0.007580838634902169\n",
            "Loss:  0.007563990022730079\n",
            "Loss:  0.00755092889538789\n",
            "Loss:  0.007537220847911165\n",
            "Loss:  0.007520837944468554\n",
            "Loss:  0.007504465807761975\n",
            "Loss:  0.007488227275944936\n",
            "Loss:  0.0074721253730164324\n",
            "Loss:  0.007455956483036516\n",
            "Loss:  0.007444764252115013\n",
            "Loss:  0.007430285005323775\n",
            "Loss:  0.007414677807492224\n",
            "Loss:  0.0073991449430700395\n",
            "Loss:  0.007383736298930692\n",
            "Loss:  0.007368282583730332\n",
            "Loss:  0.007357738053513925\n",
            "Loss:  0.00734392488414061\n",
            "Loss:  0.007328972205026331\n",
            "Loss:  0.007314084318481081\n",
            "Loss:  0.007299311341028579\n",
            "Loss:  0.007284653569459444\n",
            "Loss:  0.007273754723674679\n",
            "Loss:  0.0072621857483811045\n",
            "Loss:  0.007247901801403978\n",
            "Loss:  0.0072335977920995045\n",
            "Loss:  0.007219397264833591\n",
            "Loss:  0.0072053038132162996\n",
            "Loss:  0.0071913158998468405\n",
            "Loss:  0.007177472064433795\n",
            "Loss:  0.007168102388737951\n",
            "Loss:  0.007155652561873133\n",
            "Loss:  0.00714204195719664\n",
            "Loss:  0.00712847677286372\n",
            "Loss:  0.007115007501075211\n",
            "Loss:  0.0071016347907785085\n",
            "Loss:  0.007088357180698917\n",
            "Loss:  0.007078932466902373\n",
            "Loss:  0.0070685808242546486\n",
            "Loss:  0.007055620129313511\n",
            "Loss:  0.007042620650224811\n",
            "Loss:  0.0070297060253298954\n",
            "Loss:  0.007016880183740155\n",
            "Loss:  0.007004141928005604\n",
            "Loss:  0.006991489875430706\n",
            "Loss:  0.00697892266942347\n",
            "Loss:  0.006966373537721479\n",
            "Loss:  0.006958491612515507\n",
            "Loss:  0.006947404852387771\n",
            "Loss:  0.006935137323885696\n",
            "Loss:  0.006922896195449297\n",
            "Loss:  0.006910732292403808\n",
            "Loss:  0.006898646612409703\n",
            "Loss:  0.006886638035007439\n",
            "Loss:  0.006874705377173809\n",
            "Loss:  0.006862778002311742\n",
            "Loss:  0.006855514049056356\n",
            "Loss:  0.006845035987446637\n",
            "Loss:  0.006833369471775731\n",
            "Loss:  0.006821721168334796\n",
            "Loss:  0.006810142002000914\n",
            "Loss:  0.006798633116004726\n",
            "Loss:  0.006787193534345472\n",
            "Loss:  0.0067758222145855454\n",
            "Loss:  0.006764518135246137\n",
            "Loss:  0.006753171919989105\n",
            "Loss:  0.006746552565114524\n",
            "Loss:  0.006736677173765218\n",
            "Loss:  0.0067256066150804756\n",
            "Loss:  0.006714546348284713\n",
            "Loss:  0.006703547365005218\n",
            "Loss:  0.006692610949772416\n",
            "Loss:  0.006681736268199043\n",
            "Loss:  0.006670922415848472\n",
            "Loss:  0.006660168505634401\n",
            "Loss:  0.006649473670795043\n",
            "Loss:  0.006642569122800572\n",
            "Loss:  0.006634705006394972\n",
            "Loss:  0.006624224892891291\n",
            "Loss:  0.006613673271209993\n",
            "Loss:  0.0066031733036373596\n",
            "Loss:  0.006592729488569316\n",
            "Loss:  0.006582341234808521\n",
            "Loss:  0.006572007750942373\n",
            "Loss:  0.006561728255117115\n",
            "Loss:  0.006551501983379424\n",
            "Loss:  0.006541328189389783\n",
            "Loss:  0.0065312061438044825\n",
            "Loss:  0.006521135133680057\n",
            "Loss:  0.006511114461913011\n",
            "Loss:  0.0065047940973459615\n",
            "Loss:  0.006497639757438498\n",
            "Loss:  0.0064878087751764945\n",
            "Loss:  0.006477898680632771\n",
            "Loss:  0.006468032231578338\n",
            "Loss:  0.006458214055132788\n",
            "Loss:  0.00644844371021083\n",
            "Loss:  0.0064387205527640596\n",
            "Loss:  0.006429043944615465\n",
            "Loss:  0.006419413261937647\n",
            "Loss:  0.0064098278951386575\n",
            "Loss:  0.006400287248404863\n",
            "Loss:  0.006390790739254623\n",
            "Loss:  0.006381337798115744\n",
            "Loss:  0.006371927867925067\n",
            "Loss:  0.0063625605030679265\n",
            "Loss:  0.006357625022134781\n",
            "Loss:  0.0063495617295128625\n"
          ]
        }
      ],
      "source": [
        "# Despus de varia pruebas notamos que con un learning_rate de 0.0001 y 1200 epoch la diferencia de resultados\n",
        "# entre entrenamientos se reduce considerablemente.\n",
        "embedding.train(encoder_input, 1200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faSpTmrsH0vm",
        "outputId": "fd785314-183c-45f3-f5fc-3158fb6b5a5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13\n"
          ]
        }
      ],
      "source": [
        "embedding_dic={}\n",
        "for i in range(len(source_token_dict)):\n",
        "  response = embedding.getEmbedding([i])\n",
        "  embedding_dic[i]=response\n",
        "print(len(embedding_dic))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open(\"embeddings.pkl\", \"wb\") as tf:\n",
        "    pickle.dump(embedding_dic, tf)"
      ],
      "metadata": {
        "id": "ZPZhimdGF86R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Notamos que los resultados son muy variados, por lo que cargamos uno por defecto\n",
        "with open(\"embeddings.pkl\", \"rb\") as tf:\n",
        "    embedding_dic = pickle.load(tf)"
      ],
      "metadata": {
        "id": "_yGtFw21Psw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "x=[]\n",
        "y=[]\n",
        "for key, value in embedding_dic.items():\n",
        "  x.append(embedding_dic[key][0][0][0])\n",
        "  y.append(embedding_dic[key][0][0][1])\n",
        "  #words.append(source_token_dict_inv[key])\n",
        "plt.scatter(x, y)\n",
        "\n",
        "for key, value in embedding_dic.items():\n",
        "  plt.annotate(source_token_dict_inv[key], (embedding_dic[key][0][0][0], embedding_dic[key][0][0][1]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "oj8jDxPiHb3w",
        "outputId": "6236b79a-346a-4db3-cc81-b53103a37bf4"
      },
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 205,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLlcUFV1ILch",
        "outputId": "b8ee87e3-e212-46b8-ceca-bd6cc53bdc43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ],
      "source": [
        "key=source_token_dict['rey']\n",
        "print(key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 226,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqkwovI8Vey-",
        "outputId": "eea8c87e-7f61-4aee-c218-f6709e45bc63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[1.46014078, 0.9449566 ]])]\n"
          ]
        }
      ],
      "source": [
        "embedding_of_rey = embedding_dic[source_token_dict['rey']]\n",
        "print(embedding_of_rey)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 227,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baYuk5cRV4EV",
        "outputId": "2424541b-1cc4-44ea-b2d5-8e293dfdfb6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[1.80458412, 1.69887448]])]\n"
          ]
        }
      ],
      "source": [
        "embedding_of_hombre = embedding_dic[source_token_dict['hombre']]\n",
        "print(embedding_of_hombre)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 228,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMXE_23iWH2E",
        "outputId": "cc63f5e7-f95d-4c22-bf2e-6407566e24a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[1.89748671, 1.76297155]])]\n"
          ]
        }
      ],
      "source": [
        "embedding_of_mujer = embedding_dic[source_token_dict['mujer']]\n",
        "print(embedding_of_mujer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 229,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21MF3a8KWSHv",
        "outputId": "9912385c-552c-4c50-a69b-9624268dc0ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.55304337 1.00905367]]\n",
            "('reina', 0.07331122787055243)\n",
            "[array([[1.58570251, 0.94341895]])]\n",
            "[array([[1.58570251, 0.94341895]])]\n"
          ]
        }
      ],
      "source": [
        "analogia = embedding_of_rey[0]-embedding_of_hombre[0]+embedding_of_mujer[0]\n",
        "print(analogia)\n",
        "distances={}\n",
        "for key, value in embedding_dic.items():\n",
        "  distance=cosine_similarity(analogia, value)\n",
        "  distances[source_token_dict_inv[key]]=distance\n",
        "sorted_similarities = sorted(distances.items(), key=lambda x: x[1])\n",
        "print(sorted_similarities[0])\n",
        "print(embedding_dic[source_token_dict[sorted_similarities[0][0]]])\n",
        "print(embedding_dic[8])#source_token_dict[sorted_similarities[0][0]]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "erdqVc5iYT3i"
      },
      "outputs": [],
      "source": [
        "def cosine_similarity(v1, v2):\n",
        "  #return (np.array(v1)@np.array(v2))/(np.linalg.norm(v1)*np.linalg.norm(v2))\n",
        "  return np.sqrt(np.sum((np.power((v1-v2),2))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "LvgKb5MRzuwV"
      },
      "outputs": [],
      "source": [
        "def most_similar(word, word_dict, top_k=4):\n",
        "  if word not in word_dict:\n",
        "    raise ValueError(f\"{word} not found in the dictionary\")\n",
        "  else:\n",
        "    key = word_dict[word]\n",
        "\n",
        "    queryVector = embedding_dic[key]\n",
        "\n",
        "    similarities = {}\n",
        "    for key2, value in embedding_dic.items():\n",
        "      if key!=key2:\n",
        "        similarity = cosine_similarity(queryVector[0][0], value[0][0])\n",
        "        similarities[source_token_dict_inv[key2]]=similarity\n",
        "  sorted_similarities = sorted(similarities.items(), key=lambda x: x[1])\n",
        "  return sorted_similarities[:top_k]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jTlJoK92GOm",
        "outputId": "794bde27-d803-4bac-a288-b3b139f705b3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('hombre', 0.17498751755596204),\n",
              " ('Andrea', 0.18590348807721163),\n",
              " ('mujer', 0.2943504966882905),\n",
              " ('una', 0.5207554485750954)]"
            ]
          },
          "metadata": {},
          "execution_count": 178
        }
      ],
      "source": [
        "most_similar('Carlos', source_token_dict, 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U74LNNu1Io2U"
      },
      "source": [
        "**Ahora veremos el trmino \"Positional Encoding\"**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcA79xihIwbb"
      },
      "outputs": [],
      "source": [
        "def get_positionalEncoding(len_Sentence, dim_embedding):\n",
        "  pos = np.arange(len_Sentence)[:, np.newaxis]\n",
        "  angle_rads = pos / np.power(10000, (2 * (np.arange(dim_embedding) // 2)) / np.float32(dim_embedding))\n",
        "  # Aplicar seno a los ndices pares en el arreglo\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "  # Aplicar coseno a los ndices impares en el arreglo\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "  return angle_rads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dWkFjGvqi7z",
        "outputId": "9ee3d30c-52f5-4625-bf6d-a0877cf655c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 1.]\n",
            "[0.84147098 0.54030231]\n",
            "[ 0.90929743 -0.41614684]\n",
            "[ 0.14112001 -0.9899925 ]\n",
            "[-0.7568025  -0.65364362]\n",
            "[-0.95892427  0.28366219]\n",
            "[-0.2794155   0.96017029]\n"
          ]
        }
      ],
      "source": [
        "positional_encoding = get_positionalEncoding(7, 2)\n",
        "for vector in positional_encoding:\n",
        "  print(vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        },
        "id": "O0as2s2T0Y0Y",
        "outputId": "1acca3c7-8b87-48a4-ae49-d2583a8419a7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.colorbar.Colorbar at 0x79e828bd0850>"
            ]
          },
          "metadata": {},
          "execution_count": 110
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x933.333 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "P = get_positionalEncoding(7, 3)\n",
        "cax = plt.matshow(P)\n",
        "plt.gcf().colorbar(cax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsTckgK6Hrj4"
      },
      "source": [
        "Sumamos el embedding + la codificacin posicional para la frase **la reina de inglarterra es una mujer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oapwTAJmH4v3",
        "outputId": "2fe85425-65d0-48c5-e198-fcbd5f4d1957"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding\n",
            "[4.36784609 0.03817434]\n",
            "[5.66969563 0.        ]\n",
            "[0. 0.]\n",
            "[1.26963357 0.        ]\n",
            "[7.23264316 0.        ]\n",
            "[0. 0.]\n",
            "[5.52944602 0.12864978]\n",
            "\n",
            "Positional Encoding\n",
            "[0. 1.]\n",
            "[0.84147098 0.54030231]\n",
            "[ 0.90929743 -0.41614684]\n",
            "[ 0.14112001 -0.9899925 ]\n",
            "[-0.7568025  -0.65364362]\n",
            "[-0.95892427  0.28366219]\n",
            "[-0.2794155   0.96017029]\n",
            "\n",
            "Input for Encoder\n",
            "[4.36784609 1.03817434]\n",
            "[6.51116662 0.54030231]\n",
            "[ 0.90929743 -0.41614684]\n",
            "[ 1.41075358 -0.9899925 ]\n",
            "[ 6.47584067 -0.65364362]\n",
            "[-0.95892427  0.28366219]\n",
            "[5.25003052 1.08882006]\n"
          ]
        }
      ],
      "source": [
        "embedding_of_sentence = embedding.getEmbedding(encoder_input[1])\n",
        "print(\"Embedding\")\n",
        "for value in embedding_of_sentence:\n",
        "  print(value[0])\n",
        "print(\"\\nPositional Encoding\")\n",
        "for value in np.array(positional_encoding):\n",
        "  print(value)\n",
        "print(\"\\nInput for Encoder\")\n",
        "for i in range(len(embedding_of_sentence)):\n",
        "  print(embedding_of_sentence[i][0]+np.array(positional_encoding)[i])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASM0zjgWcXC8"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xb76eFqcRLR",
        "outputId": "62963558-c027-4232-e390-13ef1bfa5b5c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3.1622776601683795"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.sqrt(np.sum(np.power(np.array([3, 2, 5])-np.array([2, 2, 2]), 2)))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyObJMgNwReCRQDlbkkvBHGo",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}